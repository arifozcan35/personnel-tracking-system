2025-06-20 05:29:37 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - Starting PersonelTrackingSystemApplication using Java 21.0.6 with PID 16944 (C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes started by Lenovo in C:\Users\Lenovo\Desktop\personnel-tracking-system)
2025-06-20 05:29:37 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - No active profile set, falling back to 1 default profile: "default"
2025-06-20 05:29:39 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-20 05:29:39 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-20 05:29:39 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.30]
2025-06-20 05:29:40 [restartedMain] INFO  o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-20 05:29:40 [restartedMain] WARN  c.h.i.impl.HazelcastInstanceFactory - Hazelcast is starting in a Java modular environment (Java 9 and newer) but without proper access to required Java packages. Use additional Java arguments to provide Hazelcast access to Java internal API. The internal API access is used to get the best performance results. Arguments to be used:
 --add-modules java.se --add-exports java.base/jdk.internal.ref=ALL-UNNAMED --add-opens java.base/java.lang=ALL-UNNAMED --add-opens java.base/sun.nio.ch=ALL-UNNAMED --add-opens java.management/sun.management=ALL-UNNAMED --add-opens jdk.management/com.sun.management.internal=ALL-UNNAMED
2025-06-20 05:29:40 [restartedMain] INFO  com.hazelcast.system.logo - [192.168.1.36]:5701 [dev] [5.4.0] 
	o    o     o     o---o   o--o o      o---o     o     o----o o--o--o
	|    |    / \       /         |     /         / \    |         |   
	o----o       o     o   o----o |    o             o   o----o    |   
	|    |  *     \   /           |     \       *     \       |    |   
	o    o *       o o---o   o--o o----o o---o *       o o----o    o   
2025-06-20 05:29:40 [restartedMain] INFO  com.hazelcast.system - [192.168.1.36]:5701 [dev] [5.4.0] Copyright (c) 2008-2024, Hazelcast, Inc. All Rights Reserved.
2025-06-20 05:29:40 [restartedMain] INFO  com.hazelcast.system - [192.168.1.36]:5701 [dev] [5.4.0] Hazelcast Platform 5.4.0 (20240415) starting at [192.168.1.36]:5701
2025-06-20 05:29:40 [restartedMain] INFO  com.hazelcast.system - [192.168.1.36]:5701 [dev] [5.4.0] Cluster name: dev
2025-06-20 05:29:40 [restartedMain] INFO  com.hazelcast.system - [192.168.1.36]:5701 [dev] [5.4.0] Integrity Checker is disabled. Fail-fast on corrupted executables will not be performed. For more information, see the documentation for Integrity Checker.
2025-06-20 05:29:40 [restartedMain] INFO  com.hazelcast.system - [192.168.1.36]:5701 [dev] [5.4.0] The Jet engine is disabled.
To enable the Jet engine on the members, do one of the following:
  - Change member config using Java API: config.getJetConfig().setEnabled(true)
  - Change XML/YAML configuration property: Set hazelcast.jet.enabled to true
  - Add system property: -Dhz.jet.enabled=true (for Hazelcast embedded, works only when loading config via Config.load)
  - Add environment variable: HZ_JET_ENABLED=true (recommended when running container image. For Hazelcast embedded, works only when loading config via Config.load)
2025-06-20 05:29:41 [restartedMain] INFO  com.hazelcast.system.security - [192.168.1.36]:5701 [dev] [5.4.0] Enable DEBUG/FINE log level for log category com.hazelcast.system.security  or use -Dhazelcast.security.recommendations system property to see security recommendations and the status of current config.
2025-06-20 05:29:41 [restartedMain] WARN  com.hazelcast.instance.impl.Node - [192.168.1.36]:5701 [dev] [5.4.0] Invalid argument: setsockopt
java.net.SocketException: Invalid argument: setsockopt
	at java.base/sun.nio.ch.Net.setInterface6(Native Method)
	at java.base/sun.nio.ch.DatagramChannelImpl.setOption(DatagramChannelImpl.java:381)
	at java.base/sun.nio.ch.DatagramSocketAdaptor.setOption(DatagramSocketAdaptor.java:379)
	at java.base/sun.nio.ch.DatagramSocketAdaptor.setNetworkInterface(DatagramSocketAdaptor.java:564)
	at java.base/sun.nio.ch.DatagramSocketAdaptor.setInterface(DatagramSocketAdaptor.java:528)
	at java.base/java.net.MulticastSocket.setInterface(MulticastSocket.java:404)
	at com.hazelcast.internal.cluster.impl.MulticastService.configureMulticastSocket(MulticastService.java:162)
	at com.hazelcast.internal.cluster.impl.MulticastService.createMulticastService(MulticastService.java:115)
	at com.hazelcast.instance.impl.Node.<init>(Node.java:301)
	at com.hazelcast.instance.impl.HazelcastInstanceImpl.createNode(HazelcastInstanceImpl.java:150)
	at com.hazelcast.instance.impl.HazelcastInstanceImpl.<init>(HazelcastInstanceImpl.java:118)
	at com.hazelcast.instance.impl.HazelcastInstanceFactory.constructHazelcastInstance(HazelcastInstanceFactory.java:218)
	at com.hazelcast.instance.impl.HazelcastInstanceFactory.newHazelcastInstance(HazelcastInstanceFactory.java:197)
	at com.hazelcast.instance.impl.HazelcastInstanceFactory.newHazelcastInstance(HazelcastInstanceFactory.java:135)
	at com.hazelcast.core.Hazelcast.newHazelcastInstance(Hazelcast.java:61)
	at com.personneltrackingsystem.config.HazelcastConfig.hazelcastInstance(HazelcastConfig.java:24)
	at com.personneltrackingsystem.config.HazelcastConfig$$SpringCGLIB$$0.CGLIB$hazelcastInstance$0(<generated>)
	at com.personneltrackingsystem.config.HazelcastConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:348)
	at com.personneltrackingsystem.config.HazelcastConfig$$SpringCGLIB$$0.hazelcastInstance(<generated>)
	at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)
	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:146)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:644)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:485)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1355)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1185)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:313)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:365)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:135)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveConstructorArguments(ConstructorResolver.java:682)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:509)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1355)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1185)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:365)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:135)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyPropertyValues(AbstractAutowireCapableBeanFactory.java:1705)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1454)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:599)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1443)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1353)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:904)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:782)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:237)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1375)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1212)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1443)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1353)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:904)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:782)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:237)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1375)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1212)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:205)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.getOrderedBeansOfType(ServletContextInitializerBeans.java:211)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.addAsRegistrationBean(ServletContextInitializerBeans.java:174)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.addAsRegistrationBean(ServletContextInitializerBeans.java:169)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.addAdaptableBeans(ServletContextInitializerBeans.java:154)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.<init>(ServletContextInitializerBeans.java:87)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.getServletContextInitializerBeans(ServletWebServerApplicationContext.java:266)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.selfInitialize(ServletWebServerApplicationContext.java:240)
	at org.springframework.boot.web.embedded.tomcat.TomcatStarter.onStartup(TomcatStarter.java:52)
	at org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:4412)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1203)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1193)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at org.apache.tomcat.util.threads.InlineExecutorService.execute(InlineExecutorService.java:75)
	at java.base/java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:145)
	at org.apache.catalina.core.ContainerBase.startInternal(ContainerBase.java:749)
	at org.apache.catalina.core.StandardHost.startInternal(StandardHost.java:772)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1203)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1193)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at org.apache.tomcat.util.threads.InlineExecutorService.execute(InlineExecutorService.java:75)
	at java.base/java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:145)
	at org.apache.catalina.core.ContainerBase.startInternal(ContainerBase.java:749)
	at org.apache.catalina.core.StandardEngine.startInternal(StandardEngine.java:203)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.core.StandardService.startInternal(StandardService.java:415)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.core.StandardServer.startInternal(StandardServer.java:870)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.startup.Tomcat.start(Tomcat.java:437)
	at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.initialize(TomcatWebServer.java:128)
	at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.<init>(TomcatWebServer.java:107)
	at org.springframework.boot.web.embedded.tomcat.TomcatServletWebServerFactory.getTomcatWebServer(TomcatServletWebServerFactory.java:516)
	at org.springframework.boot.web.embedded.tomcat.TomcatServletWebServerFactory.getWebServer(TomcatServletWebServerFactory.java:222)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.createWebServer(ServletWebServerApplicationContext.java:188)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.onRefresh(ServletWebServerApplicationContext.java:162)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:619)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:754)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:456)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:335)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1363)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1352)
	at com.personneltrackingsystem.PersonelTrackingSystemApplication.main(PersonelTrackingSystemApplication.java:12)
	at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)
	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
2025-06-20 05:29:41 [restartedMain] INFO  com.hazelcast.instance.impl.Node - [192.168.1.36]:5701 [dev] [5.4.0] Using Multicast discovery
2025-06-20 05:29:41 [restartedMain] WARN  com.hazelcast.cp.CPSubsystem - [192.168.1.36]:5701 [dev] [5.4.0] CP Subsystem is not enabled. CP data structures will operate in UNSAFE mode! Please note that UNSAFE mode will not provide strong consistency guarantees.
2025-06-20 05:29:41 [restartedMain] INFO  c.h.internal.diagnostics.Diagnostics - [192.168.1.36]:5701 [dev] [5.4.0] Diagnostics disabled. To enable add -Dhazelcast.diagnostics.enabled=true to the JVM arguments.
2025-06-20 05:29:41 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [192.168.1.36]:5701 [dev] [5.4.0] [192.168.1.36]:5701 is STARTING
2025-06-20 05:29:44 [restartedMain] INFO  c.h.internal.cluster.ClusterService - [192.168.1.36]:5701 [dev] [5.4.0] 

Members {size:1, ver:1} [
	Member [192.168.1.36]:5701 - 542e00aa-920a-4895-a4b7-9562132a543d this
]

2025-06-20 05:29:44 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [192.168.1.36]:5701 [dev] [5.4.0] [192.168.1.36]:5701 is STARTED
2025-06-20 05:29:44 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-1 - Starting...
2025-06-20 05:29:45 [restartedMain] ERROR o.s.b.w.e.tomcat.TomcatStarter - Error starting Tomcat context. Exception: org.springframework.beans.factory.UnsatisfiedDependencyException. Message: Error creating bean with name 'jwtAuthenticationFilter' defined in file [C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes\com\personneltrackingsystem\filter\JwtAuthenticationFilter.class]: Unsatisfied dependency expressed through constructor parameter 1: Error creating bean with name 'customUserDetailsServiceImpl' defined in file [C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes\com\personneltrackingsystem\service\impl\CustomUserDetailsServiceImpl.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'userRepository' defined in com.personneltrackingsystem.repository.UserRepository defined in @EnableJpaRepositories declared on JpaRepositoriesRegistrar.EnableJpaRepositoriesConfiguration: Cannot resolve reference to bean 'jpaSharedEM_entityManagerFactory' while setting bean property 'entityManager'
2025-06-20 05:29:45 [restartedMain] INFO  o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-06-20 05:29:45 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.event-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:435)
 com.hazelcast.internal.util.executor.StripedExecutor$Worker.run(StripedExecutor.java:227)
2025-06-20 05:29:45 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.event-2] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:435)
 com.hazelcast.internal.util.executor.StripedExecutor$Worker.run(StripedExecutor.java:227)
2025-06-20 05:29:45 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.event-3] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:435)
 com.hazelcast.internal.util.executor.StripedExecutor$Worker.run(StripedExecutor.java:227)
2025-06-20 05:29:45 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.event-4] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:435)
 com.hazelcast.internal.util.executor.StripedExecutor$Worker.run(StripedExecutor.java:227)
2025-06-20 05:29:45 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.event-5] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:435)
 com.hazelcast.internal.util.executor.StripedExecutor$Worker.run(StripedExecutor.java:227)
2025-06-20 05:29:45 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.MetricsRegistry.thread-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1177)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 05:29:45 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.MetricsRegistry.thread-2] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:269)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1763)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 05:29:45 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.migration] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:269)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1763)
 java.base/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:460)
 com.hazelcast.internal.partition.impl.MigrationQueue.poll(MigrationQueue.java:48)
 com.hazelcast.internal.partition.impl.MigrationThread.doRun(MigrationThread.java:91)
 com.hazelcast.internal.partition.impl.MigrationThread.run(MigrationThread.java:66)
2025-06-20 05:29:45 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.InvocationMonitorThread] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:269)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1763)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 05:29:45 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.SlowOperationDetectorThread] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/java.lang.Thread.sleep0(Native Method)
 java.base/java.lang.Thread.sleep(Thread.java:558)
 java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
 com.hazelcast.spi.impl.operationexecutor.slowoperationdetector.SlowOperationDetector$DetectorThread.sleepInterval(SlowOperationDetector.java:280)
 com.hazelcast.spi.impl.operationexecutor.slowoperationdetector.SlowOperationDetector$DetectorThread.run(SlowOperationDetector.java:153)
2025-06-20 05:29:45 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-in-0] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:142)
 com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:292)
 com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249)
 com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111)
2025-06-20 05:29:45 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-in-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:142)
 com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:292)
 com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249)
 com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111)
2025-06-20 05:29:45 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-in-2] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:142)
 com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:292)
 com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249)
 com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111)
2025-06-20 05:29:45 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-out-0] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:142)
 com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:292)
 com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249)
 com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111)
2025-06-20 05:29:45 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-out-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:142)
 com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:292)
 com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249)
 com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111)
2025-06-20 05:29:45 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-out-2] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:142)
 com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:292)
 com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249)
 com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111)
2025-06-20 05:29:45 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.BalancerThread] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:269)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1763)
 java.base/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:460)
 com.hazelcast.internal.networking.nio.iobalancer.IOBalancerThread.run(IOBalancerThread.java:65)
2025-06-20 05:29:45 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-Acceptor] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:147)
 com.hazelcast.internal.server.tcp.TcpServerAcceptor$AcceptorIOThread.acceptLoop(TcpServerAcceptor.java:186)
 com.hazelcast.internal.server.tcp.TcpServerAcceptor$AcceptorIOThread.run(TcpServerAcceptor.java:172)
2025-06-20 05:29:45 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.MulticastThread] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.Net.poll(Native Method)
 java.base/sun.nio.ch.DatagramChannelImpl.park(DatagramChannelImpl.java:511)
 java.base/sun.nio.ch.DatagramChannelImpl.tryBlockingReceive(DatagramChannelImpl.java:762)
 java.base/sun.nio.ch.DatagramChannelImpl.blockingReceive(DatagramChannelImpl.java:692)
 java.base/sun.nio.ch.DatagramSocketAdaptor.receive(DatagramSocketAdaptor.java:204)
 java.base/java.net.DatagramSocket.receive(DatagramSocket.java:714)
 com.hazelcast.internal.cluster.impl.MulticastService.receive(MulticastService.java:247)
 com.hazelcast.internal.cluster.impl.MulticastService.run(MulticastService.java:223)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 05:29:45 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.HealthMonitor] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/java.lang.Thread.sleep0(Native Method)
 java.base/java.lang.Thread.sleep(Thread.java:558)
 java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
 com.hazelcast.internal.diagnostics.HealthMonitor$HealthMonitorThread.run(HealthMonitor.java:164)
2025-06-20 05:29:45 [restartedMain] WARN  o.s.b.w.s.c.AnnotationConfigServletWebServerApplicationContext - Exception encountered during context initialization - cancelling refresh attempt: org.springframework.context.ApplicationContextException: Unable to start web server
2025-06-20 05:29:45 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [192.168.1.36]:5701 [dev] [5.4.0] [192.168.1.36]:5701 is SHUTTING_DOWN
2025-06-20 05:29:45 [restartedMain] INFO  com.hazelcast.instance.impl.Node - [192.168.1.36]:5701 [dev] [5.4.0] Shutting down multicast service...
2025-06-20 05:29:45 [restartedMain] INFO  com.hazelcast.instance.impl.Node - [192.168.1.36]:5701 [dev] [5.4.0] Shutting down connection manager...
2025-06-20 05:29:45 [restartedMain] INFO  com.hazelcast.instance.impl.Node - [192.168.1.36]:5701 [dev] [5.4.0] Shutting down node engine...
2025-06-20 05:29:45 [restartedMain] INFO  c.h.instance.impl.NodeExtension - [192.168.1.36]:5701 [dev] [5.4.0] Destroying node NodeExtension.
2025-06-20 05:29:45 [restartedMain] INFO  com.hazelcast.instance.impl.Node - [192.168.1.36]:5701 [dev] [5.4.0] Hazelcast Shutdown is completed in 22 ms.
2025-06-20 05:29:45 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [192.168.1.36]:5701 [dev] [5.4.0] [192.168.1.36]:5701 is SHUTDOWN
2025-06-20 05:29:45 [restartedMain] ERROR o.s.boot.SpringApplication - Application run failed
org.springframework.context.ApplicationContextException: Unable to start web server
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.onRefresh(ServletWebServerApplicationContext.java:165)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:619)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:754)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:456)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:335)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1363)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1352)
	at com.personneltrackingsystem.PersonelTrackingSystemApplication.main(PersonelTrackingSystemApplication.java:12)
	at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)
	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
Caused by: org.springframework.boot.web.server.WebServerException: Unable to start embedded Tomcat
	at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.initialize(TomcatWebServer.java:147)
	at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.<init>(TomcatWebServer.java:107)
	at org.springframework.boot.web.embedded.tomcat.TomcatServletWebServerFactory.getTomcatWebServer(TomcatServletWebServerFactory.java:516)
	at org.springframework.boot.web.embedded.tomcat.TomcatServletWebServerFactory.getWebServer(TomcatServletWebServerFactory.java:222)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.createWebServer(ServletWebServerApplicationContext.java:188)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.onRefresh(ServletWebServerApplicationContext.java:162)
	... 11 common frames omitted
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'jwtAuthenticationFilter' defined in file [C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes\com\personneltrackingsystem\filter\JwtAuthenticationFilter.class]: Unsatisfied dependency expressed through constructor parameter 1: Error creating bean with name 'customUserDetailsServiceImpl' defined in file [C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes\com\personneltrackingsystem\service\impl\CustomUserDetailsServiceImpl.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'userRepository' defined in com.personneltrackingsystem.repository.UserRepository defined in @EnableJpaRepositories declared on JpaRepositoriesRegistrar.EnableJpaRepositoriesConfiguration: Cannot resolve reference to bean 'jpaSharedEM_entityManagerFactory' while setting bean property 'entityManager'
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:795)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:237)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1375)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1212)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:205)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.getOrderedBeansOfType(ServletContextInitializerBeans.java:211)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.addAsRegistrationBean(ServletContextInitializerBeans.java:174)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.addAsRegistrationBean(ServletContextInitializerBeans.java:169)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.addAdaptableBeans(ServletContextInitializerBeans.java:154)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.<init>(ServletContextInitializerBeans.java:87)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.getServletContextInitializerBeans(ServletWebServerApplicationContext.java:266)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.selfInitialize(ServletWebServerApplicationContext.java:240)
	at org.springframework.boot.web.embedded.tomcat.TomcatStarter.onStartup(TomcatStarter.java:52)
	at org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:4412)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1203)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1193)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at org.apache.tomcat.util.threads.InlineExecutorService.execute(InlineExecutorService.java:75)
	at java.base/java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:145)
	at org.apache.catalina.core.ContainerBase.startInternal(ContainerBase.java:749)
	at org.apache.catalina.core.StandardHost.startInternal(StandardHost.java:772)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1203)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1193)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at org.apache.tomcat.util.threads.InlineExecutorService.execute(InlineExecutorService.java:75)
	at java.base/java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:145)
	at org.apache.catalina.core.ContainerBase.startInternal(ContainerBase.java:749)
	at org.apache.catalina.core.StandardEngine.startInternal(StandardEngine.java:203)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.core.StandardService.startInternal(StandardService.java:415)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.core.StandardServer.startInternal(StandardServer.java:870)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.startup.Tomcat.start(Tomcat.java:437)
	at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.initialize(TomcatWebServer.java:128)
	... 16 common frames omitted
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'customUserDetailsServiceImpl' defined in file [C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes\com\personneltrackingsystem\service\impl\CustomUserDetailsServiceImpl.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'userRepository' defined in com.personneltrackingsystem.repository.UserRepository defined in @EnableJpaRepositories declared on JpaRepositoriesRegistrar.EnableJpaRepositoriesConfiguration: Cannot resolve reference to bean 'jpaSharedEM_entityManagerFactory' while setting bean property 'entityManager'
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:795)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:237)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1375)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1212)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1443)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1353)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:904)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:782)
	... 57 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'userRepository' defined in com.personneltrackingsystem.repository.UserRepository defined in @EnableJpaRepositories declared on JpaRepositoriesRegistrar.EnableJpaRepositoriesConfiguration: Cannot resolve reference to bean 'jpaSharedEM_entityManagerFactory' while setting bean property 'entityManager'
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:377)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:135)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyPropertyValues(AbstractAutowireCapableBeanFactory.java:1705)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1454)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:599)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1443)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1353)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:904)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:782)
	... 71 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'jpaSharedEM_entityManagerFactory': Cannot resolve reference to bean 'entityManagerFactory' while setting constructor argument
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:377)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:135)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveConstructorArguments(ConstructorResolver.java:682)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:509)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1355)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1185)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:365)
	... 85 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'liquibase' defined in class path resource [org/springframework/boot/autoconfigure/liquibase/LiquibaseAutoConfiguration$LiquibaseConfiguration.class]: liquibase.exception.DatabaseException: org.postgresql.util.PSQLException: LMCL (FATAL): "dbpts" veritaban mevcut deil
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1806)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:313)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:365)
	... 97 common frames omitted
Caused by: liquibase.exception.UnexpectedLiquibaseException: liquibase.exception.DatabaseException: org.postgresql.util.PSQLException: LMCL (FATAL): "dbpts" veritaban mevcut deil
	at liquibase.integration.spring.SpringLiquibase.afterPropertiesSet(SpringLiquibase.java:267)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1853)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1802)
	... 106 common frames omitted
Caused by: liquibase.exception.DatabaseException: org.postgresql.util.PSQLException: LMCL (FATAL): "dbpts" veritaban mevcut deil
	at liquibase.integration.spring.SpringLiquibase.lambda$afterPropertiesSet$0(SpringLiquibase.java:259)
	at liquibase.Scope.lambda$child$0(Scope.java:191)
	at liquibase.Scope.child(Scope.java:200)
	at liquibase.Scope.child(Scope.java:190)
	at liquibase.Scope.child(Scope.java:169)
	at liquibase.Scope.child(Scope.java:257)
	at liquibase.integration.spring.SpringLiquibase.afterPropertiesSet(SpringLiquibase.java:250)
	... 108 common frames omitted
Caused by: org.postgresql.util.PSQLException: LMCL (FATAL): "dbpts" veritaban mevcut deil
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2725)
	at org.postgresql.core.v3.QueryExecutorImpl.readStartupMessages(QueryExecutorImpl.java:2837)
	at org.postgresql.core.v3.QueryExecutorImpl.<init>(QueryExecutorImpl.java:175)
	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:317)
	at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)
	at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:273)
	at org.postgresql.Driver.makeConnection(Driver.java:446)
	at org.postgresql.Driver.connect(Driver.java:298)
	at com.zaxxer.hikari.util.DriverDataSource.getConnection(DriverDataSource.java:137)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:360)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:202)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:461)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:550)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:98)
	at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:111)
	at liquibase.integration.spring.SpringLiquibase.lambda$afterPropertiesSet$0(SpringLiquibase.java:254)
	... 114 common frames omitted
2025-06-20 05:59:49 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - Starting PersonelTrackingSystemApplication using Java 21.0.7 with PID 6728 (C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes started by Lenovo in C:\Users\Lenovo\Desktop\personnel-tracking-system)
2025-06-20 05:59:49 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - No active profile set, falling back to 1 default profile: "default"
2025-06-20 05:59:52 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-20 05:59:52 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-20 05:59:52 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.30]
2025-06-20 05:59:52 [restartedMain] INFO  o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-20 05:59:52 [restartedMain] WARN  c.h.i.impl.HazelcastInstanceFactory - Hazelcast is starting in a Java modular environment (Java 9 and newer) but without proper access to required Java packages. Use additional Java arguments to provide Hazelcast access to Java internal API. The internal API access is used to get the best performance results. Arguments to be used:
 --add-modules java.se --add-exports java.base/jdk.internal.ref=ALL-UNNAMED --add-opens java.base/java.lang=ALL-UNNAMED --add-opens java.base/sun.nio.ch=ALL-UNNAMED --add-opens java.management/sun.management=ALL-UNNAMED --add-opens jdk.management/com.sun.management.internal=ALL-UNNAMED
2025-06-20 05:59:52 [restartedMain] INFO  com.hazelcast.system.logo - [192.168.1.36]:5701 [dev] [5.4.0] 
	o    o     o     o---o   o--o o      o---o     o     o----o o--o--o
	|    |    / \       /         |     /         / \    |         |   
	o----o       o     o   o----o |    o             o   o----o    |   
	|    |  *     \   /           |     \       *     \       |    |   
	o    o *       o o---o   o--o o----o o---o *       o o----o    o   
2025-06-20 05:59:52 [restartedMain] INFO  com.hazelcast.system - [192.168.1.36]:5701 [dev] [5.4.0] Copyright (c) 2008-2024, Hazelcast, Inc. All Rights Reserved.
2025-06-20 05:59:52 [restartedMain] INFO  com.hazelcast.system - [192.168.1.36]:5701 [dev] [5.4.0] Hazelcast Platform 5.4.0 (20240415) starting at [192.168.1.36]:5701
2025-06-20 05:59:52 [restartedMain] INFO  com.hazelcast.system - [192.168.1.36]:5701 [dev] [5.4.0] Cluster name: dev
2025-06-20 05:59:52 [restartedMain] INFO  com.hazelcast.system - [192.168.1.36]:5701 [dev] [5.4.0] Integrity Checker is disabled. Fail-fast on corrupted executables will not be performed. For more information, see the documentation for Integrity Checker.
2025-06-20 05:59:52 [restartedMain] INFO  com.hazelcast.system - [192.168.1.36]:5701 [dev] [5.4.0] The Jet engine is disabled.
To enable the Jet engine on the members, do one of the following:
  - Change member config using Java API: config.getJetConfig().setEnabled(true)
  - Change XML/YAML configuration property: Set hazelcast.jet.enabled to true
  - Add system property: -Dhz.jet.enabled=true (for Hazelcast embedded, works only when loading config via Config.load)
  - Add environment variable: HZ_JET_ENABLED=true (recommended when running container image. For Hazelcast embedded, works only when loading config via Config.load)
2025-06-20 05:59:53 [restartedMain] INFO  com.hazelcast.system.security - [192.168.1.36]:5701 [dev] [5.4.0] Enable DEBUG/FINE log level for log category com.hazelcast.system.security  or use -Dhazelcast.security.recommendations system property to see security recommendations and the status of current config.
2025-06-20 05:59:53 [restartedMain] WARN  com.hazelcast.instance.impl.Node - [192.168.1.36]:5701 [dev] [5.4.0] Invalid argument: setsockopt
java.net.SocketException: Invalid argument: setsockopt
	at java.base/sun.nio.ch.Net.setInterface6(Native Method)
	at java.base/sun.nio.ch.DatagramChannelImpl.setOption(DatagramChannelImpl.java:381)
	at java.base/sun.nio.ch.DatagramSocketAdaptor.setOption(DatagramSocketAdaptor.java:379)
	at java.base/sun.nio.ch.DatagramSocketAdaptor.setNetworkInterface(DatagramSocketAdaptor.java:564)
	at java.base/sun.nio.ch.DatagramSocketAdaptor.setInterface(DatagramSocketAdaptor.java:528)
	at java.base/java.net.MulticastSocket.setInterface(MulticastSocket.java:404)
	at com.hazelcast.internal.cluster.impl.MulticastService.configureMulticastSocket(MulticastService.java:162)
	at com.hazelcast.internal.cluster.impl.MulticastService.createMulticastService(MulticastService.java:115)
	at com.hazelcast.instance.impl.Node.<init>(Node.java:301)
	at com.hazelcast.instance.impl.HazelcastInstanceImpl.createNode(HazelcastInstanceImpl.java:150)
	at com.hazelcast.instance.impl.HazelcastInstanceImpl.<init>(HazelcastInstanceImpl.java:118)
	at com.hazelcast.instance.impl.HazelcastInstanceFactory.constructHazelcastInstance(HazelcastInstanceFactory.java:218)
	at com.hazelcast.instance.impl.HazelcastInstanceFactory.newHazelcastInstance(HazelcastInstanceFactory.java:197)
	at com.hazelcast.instance.impl.HazelcastInstanceFactory.newHazelcastInstance(HazelcastInstanceFactory.java:135)
	at com.hazelcast.core.Hazelcast.newHazelcastInstance(Hazelcast.java:61)
	at com.personneltrackingsystem.config.HazelcastConfig.hazelcastInstance(HazelcastConfig.java:24)
	at com.personneltrackingsystem.config.HazelcastConfig$$SpringCGLIB$$0.CGLIB$hazelcastInstance$0(<generated>)
	at com.personneltrackingsystem.config.HazelcastConfig$$SpringCGLIB$$FastClass$$1.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:258)
	at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:348)
	at com.personneltrackingsystem.config.HazelcastConfig$$SpringCGLIB$$0.hazelcastInstance(<generated>)
	at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)
	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:146)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:644)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:485)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1355)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1185)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:313)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:365)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:135)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveConstructorArguments(ConstructorResolver.java:682)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:509)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1355)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1185)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:365)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:135)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyPropertyValues(AbstractAutowireCapableBeanFactory.java:1705)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1454)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:599)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1443)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1353)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:904)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:782)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:237)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1375)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1212)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1443)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1353)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:904)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:782)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:237)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1375)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1212)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:205)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.getOrderedBeansOfType(ServletContextInitializerBeans.java:211)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.addAsRegistrationBean(ServletContextInitializerBeans.java:174)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.addAsRegistrationBean(ServletContextInitializerBeans.java:169)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.addAdaptableBeans(ServletContextInitializerBeans.java:154)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.<init>(ServletContextInitializerBeans.java:87)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.getServletContextInitializerBeans(ServletWebServerApplicationContext.java:266)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.selfInitialize(ServletWebServerApplicationContext.java:240)
	at org.springframework.boot.web.embedded.tomcat.TomcatStarter.onStartup(TomcatStarter.java:52)
	at org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:4412)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1203)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1193)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at org.apache.tomcat.util.threads.InlineExecutorService.execute(InlineExecutorService.java:75)
	at java.base/java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:145)
	at org.apache.catalina.core.ContainerBase.startInternal(ContainerBase.java:749)
	at org.apache.catalina.core.StandardHost.startInternal(StandardHost.java:772)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1203)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1193)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at org.apache.tomcat.util.threads.InlineExecutorService.execute(InlineExecutorService.java:75)
	at java.base/java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:145)
	at org.apache.catalina.core.ContainerBase.startInternal(ContainerBase.java:749)
	at org.apache.catalina.core.StandardEngine.startInternal(StandardEngine.java:203)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.core.StandardService.startInternal(StandardService.java:415)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.core.StandardServer.startInternal(StandardServer.java:870)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.startup.Tomcat.start(Tomcat.java:437)
	at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.initialize(TomcatWebServer.java:128)
	at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.<init>(TomcatWebServer.java:107)
	at org.springframework.boot.web.embedded.tomcat.TomcatServletWebServerFactory.getTomcatWebServer(TomcatServletWebServerFactory.java:516)
	at org.springframework.boot.web.embedded.tomcat.TomcatServletWebServerFactory.getWebServer(TomcatServletWebServerFactory.java:222)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.createWebServer(ServletWebServerApplicationContext.java:188)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.onRefresh(ServletWebServerApplicationContext.java:162)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:619)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:754)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:456)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:335)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1363)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1352)
	at com.personneltrackingsystem.PersonelTrackingSystemApplication.main(PersonelTrackingSystemApplication.java:12)
	at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)
	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
2025-06-20 05:59:53 [restartedMain] INFO  com.hazelcast.instance.impl.Node - [192.168.1.36]:5701 [dev] [5.4.0] Using Multicast discovery
2025-06-20 05:59:53 [restartedMain] WARN  com.hazelcast.cp.CPSubsystem - [192.168.1.36]:5701 [dev] [5.4.0] CP Subsystem is not enabled. CP data structures will operate in UNSAFE mode! Please note that UNSAFE mode will not provide strong consistency guarantees.
2025-06-20 05:59:53 [restartedMain] INFO  c.h.internal.diagnostics.Diagnostics - [192.168.1.36]:5701 [dev] [5.4.0] Diagnostics disabled. To enable add -Dhazelcast.diagnostics.enabled=true to the JVM arguments.
2025-06-20 05:59:53 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [192.168.1.36]:5701 [dev] [5.4.0] [192.168.1.36]:5701 is STARTING
2025-06-20 05:59:55 [restartedMain] INFO  c.h.internal.cluster.ClusterService - [192.168.1.36]:5701 [dev] [5.4.0] 

Members {size:1, ver:1} [
	Member [192.168.1.36]:5701 - b029a09b-1ca6-4d5a-a57b-d960a9e5e916 this
]

2025-06-20 05:59:55 [hz.personnel-tracking-system.HealthMonitor] INFO  c.h.i.diagnostics.HealthMonitor - [192.168.1.36]:5701 [dev] [5.4.0] The HealthMonitor has detected a high load on the system. For more detailed information,
enable Diagnostics by adding the property -Dhazelcast.diagnostics.enabled=true
2025-06-20 05:59:55 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [192.168.1.36]:5701 [dev] [5.4.0] [192.168.1.36]:5701 is STARTED
2025-06-20 05:59:55 [hz.personnel-tracking-system.HealthMonitor] INFO  c.h.i.diagnostics.HealthMonitor - [192.168.1.36]:5701 [dev] [5.4.0] processors=8, physical.memory.total=7,8G, physical.memory.free=1,1G, swap.space.total=0, swap.space.free=0, heap.memory.used=53,0M, heap.memory.free=16,7M, heap.memory.total=70,0M, heap.memory.max=2,0G, heap.memory.used/total=75,72%, heap.memory.used/max=2,64%, minor.gc.count=10, minor.gc.time=38ms, major.gc.count=0, major.gc.time=0ms, unknown.gc.count=4, unknown.gc.time=3ms, load.process=0,00%, load.system=100,00%, load.systemAverage=n/a thread.count=49, thread.peakCount=49, cluster.timeDiff=0, event.q.size=0, executor.q.async.size=0, executor.q.client.size=0, executor.q.client.query.size=0, executor.q.client.blocking.size=0, executor.q.query.size=0, executor.q.scheduled.size=0, executor.q.io.size=0, executor.q.system.size=0, executor.q.operations.size=0, executor.q.priorityOperation.size=0, operations.completed.count=0, executor.q.mapLoad.size=0, executor.q.mapLoadAllKeys.size=0, executor.q.cluster.size=0, executor.q.response.size=0, operations.running.count=0, operations.pending.invocations.percentage=0,00%, operations.pending.invocations.count=0, proxy.count=0, clientEndpoint.count=0, connection.active.count=0, client.connection.count=0, connection.count=0
2025-06-20 05:59:56 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-1 - Starting...
2025-06-20 05:59:57 [restartedMain] ERROR o.s.b.w.e.tomcat.TomcatStarter - Error starting Tomcat context. Exception: org.springframework.beans.factory.UnsatisfiedDependencyException. Message: Error creating bean with name 'jwtAuthenticationFilter' defined in file [C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes\com\personneltrackingsystem\filter\JwtAuthenticationFilter.class]: Unsatisfied dependency expressed through constructor parameter 1: Error creating bean with name 'customUserDetailsServiceImpl' defined in file [C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes\com\personneltrackingsystem\service\impl\CustomUserDetailsServiceImpl.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'userRepository' defined in com.personneltrackingsystem.repository.UserRepository defined in @EnableJpaRepositories declared on JpaRepositoriesRegistrar.EnableJpaRepositoriesConfiguration: Cannot resolve reference to bean 'jpaSharedEM_entityManagerFactory' while setting bean property 'entityManager'
2025-06-20 05:59:57 [restartedMain] INFO  o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-06-20 05:59:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.event-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:435)
 com.hazelcast.internal.util.executor.StripedExecutor$Worker.run(StripedExecutor.java:227)
2025-06-20 05:59:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.event-2] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:435)
 com.hazelcast.internal.util.executor.StripedExecutor$Worker.run(StripedExecutor.java:227)
2025-06-20 05:59:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.event-3] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:435)
 com.hazelcast.internal.util.executor.StripedExecutor$Worker.run(StripedExecutor.java:227)
2025-06-20 05:59:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.event-4] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:435)
 com.hazelcast.internal.util.executor.StripedExecutor$Worker.run(StripedExecutor.java:227)
2025-06-20 05:59:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.event-5] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:435)
 com.hazelcast.internal.util.executor.StripedExecutor$Worker.run(StripedExecutor.java:227)
2025-06-20 05:59:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.MetricsRegistry.thread-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:269)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1763)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 05:59:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.MetricsRegistry.thread-2] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1177)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 05:59:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.migration] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:269)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1763)
 java.base/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:460)
 com.hazelcast.internal.partition.impl.MigrationQueue.poll(MigrationQueue.java:48)
 com.hazelcast.internal.partition.impl.MigrationThread.doRun(MigrationThread.java:91)
 com.hazelcast.internal.partition.impl.MigrationThread.run(MigrationThread.java:66)
2025-06-20 05:59:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.InvocationMonitorThread] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:269)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1763)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 05:59:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.SlowOperationDetectorThread] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/java.lang.Thread.sleep0(Native Method)
 java.base/java.lang.Thread.sleep(Thread.java:558)
 java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
 com.hazelcast.spi.impl.operationexecutor.slowoperationdetector.SlowOperationDetector$DetectorThread.sleepInterval(SlowOperationDetector.java:280)
 com.hazelcast.spi.impl.operationexecutor.slowoperationdetector.SlowOperationDetector$DetectorThread.run(SlowOperationDetector.java:153)
2025-06-20 05:59:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-in-0] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:142)
 com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:292)
 com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249)
 com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111)
2025-06-20 05:59:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-in-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:142)
 com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:292)
 com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249)
 com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111)
2025-06-20 05:59:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-in-2] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:142)
 com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:292)
 com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249)
 com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111)
2025-06-20 05:59:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-out-0] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:142)
 com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:292)
 com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249)
 com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111)
2025-06-20 05:59:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-out-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:142)
 com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:292)
 com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249)
 com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111)
2025-06-20 05:59:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-out-2] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:142)
 com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:292)
 com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249)
 com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111)
2025-06-20 05:59:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.BalancerThread] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:269)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1763)
 java.base/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:460)
 com.hazelcast.internal.networking.nio.iobalancer.IOBalancerThread.run(IOBalancerThread.java:65)
2025-06-20 05:59:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-Acceptor] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:147)
 com.hazelcast.internal.server.tcp.TcpServerAcceptor$AcceptorIOThread.acceptLoop(TcpServerAcceptor.java:186)
 com.hazelcast.internal.server.tcp.TcpServerAcceptor$AcceptorIOThread.run(TcpServerAcceptor.java:172)
2025-06-20 05:59:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.MulticastThread] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.Net.poll(Native Method)
 java.base/sun.nio.ch.DatagramChannelImpl.park(DatagramChannelImpl.java:511)
 java.base/sun.nio.ch.DatagramChannelImpl.tryBlockingReceive(DatagramChannelImpl.java:762)
 java.base/sun.nio.ch.DatagramChannelImpl.blockingReceive(DatagramChannelImpl.java:692)
 java.base/sun.nio.ch.DatagramSocketAdaptor.receive(DatagramSocketAdaptor.java:204)
 java.base/java.net.DatagramSocket.receive(DatagramSocket.java:714)
 com.hazelcast.internal.cluster.impl.MulticastService.receive(MulticastService.java:247)
 com.hazelcast.internal.cluster.impl.MulticastService.run(MulticastService.java:223)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 05:59:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.HealthMonitor] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/java.lang.Thread.sleep0(Native Method)
 java.base/java.lang.Thread.sleep(Thread.java:558)
 java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
 com.hazelcast.internal.diagnostics.HealthMonitor$HealthMonitorThread.run(HealthMonitor.java:164)
2025-06-20 05:59:57 [restartedMain] WARN  o.s.b.w.s.c.AnnotationConfigServletWebServerApplicationContext - Exception encountered during context initialization - cancelling refresh attempt: org.springframework.context.ApplicationContextException: Unable to start web server
2025-06-20 05:59:57 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [192.168.1.36]:5701 [dev] [5.4.0] [192.168.1.36]:5701 is SHUTTING_DOWN
2025-06-20 05:59:57 [restartedMain] INFO  com.hazelcast.instance.impl.Node - [192.168.1.36]:5701 [dev] [5.4.0] Shutting down multicast service...
2025-06-20 05:59:57 [restartedMain] INFO  com.hazelcast.instance.impl.Node - [192.168.1.36]:5701 [dev] [5.4.0] Shutting down connection manager...
2025-06-20 05:59:57 [restartedMain] INFO  com.hazelcast.instance.impl.Node - [192.168.1.36]:5701 [dev] [5.4.0] Shutting down node engine...
2025-06-20 05:59:57 [restartedMain] INFO  c.h.instance.impl.NodeExtension - [192.168.1.36]:5701 [dev] [5.4.0] Destroying node NodeExtension.
2025-06-20 05:59:57 [restartedMain] INFO  com.hazelcast.instance.impl.Node - [192.168.1.36]:5701 [dev] [5.4.0] Hazelcast Shutdown is completed in 37 ms.
2025-06-20 05:59:57 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [192.168.1.36]:5701 [dev] [5.4.0] [192.168.1.36]:5701 is SHUTDOWN
2025-06-20 05:59:57 [restartedMain] ERROR o.s.boot.SpringApplication - Application run failed
org.springframework.context.ApplicationContextException: Unable to start web server
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.onRefresh(ServletWebServerApplicationContext.java:165)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:619)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:754)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:456)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:335)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1363)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1352)
	at com.personneltrackingsystem.PersonelTrackingSystemApplication.main(PersonelTrackingSystemApplication.java:12)
	at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)
	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
Caused by: org.springframework.boot.web.server.WebServerException: Unable to start embedded Tomcat
	at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.initialize(TomcatWebServer.java:147)
	at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.<init>(TomcatWebServer.java:107)
	at org.springframework.boot.web.embedded.tomcat.TomcatServletWebServerFactory.getTomcatWebServer(TomcatServletWebServerFactory.java:516)
	at org.springframework.boot.web.embedded.tomcat.TomcatServletWebServerFactory.getWebServer(TomcatServletWebServerFactory.java:222)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.createWebServer(ServletWebServerApplicationContext.java:188)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.onRefresh(ServletWebServerApplicationContext.java:162)
	... 11 common frames omitted
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'jwtAuthenticationFilter' defined in file [C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes\com\personneltrackingsystem\filter\JwtAuthenticationFilter.class]: Unsatisfied dependency expressed through constructor parameter 1: Error creating bean with name 'customUserDetailsServiceImpl' defined in file [C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes\com\personneltrackingsystem\service\impl\CustomUserDetailsServiceImpl.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'userRepository' defined in com.personneltrackingsystem.repository.UserRepository defined in @EnableJpaRepositories declared on JpaRepositoriesRegistrar.EnableJpaRepositoriesConfiguration: Cannot resolve reference to bean 'jpaSharedEM_entityManagerFactory' while setting bean property 'entityManager'
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:795)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:237)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1375)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1212)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:205)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.getOrderedBeansOfType(ServletContextInitializerBeans.java:211)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.addAsRegistrationBean(ServletContextInitializerBeans.java:174)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.addAsRegistrationBean(ServletContextInitializerBeans.java:169)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.addAdaptableBeans(ServletContextInitializerBeans.java:154)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.<init>(ServletContextInitializerBeans.java:87)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.getServletContextInitializerBeans(ServletWebServerApplicationContext.java:266)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.selfInitialize(ServletWebServerApplicationContext.java:240)
	at org.springframework.boot.web.embedded.tomcat.TomcatStarter.onStartup(TomcatStarter.java:52)
	at org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:4412)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1203)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1193)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at org.apache.tomcat.util.threads.InlineExecutorService.execute(InlineExecutorService.java:75)
	at java.base/java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:145)
	at org.apache.catalina.core.ContainerBase.startInternal(ContainerBase.java:749)
	at org.apache.catalina.core.StandardHost.startInternal(StandardHost.java:772)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1203)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1193)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at org.apache.tomcat.util.threads.InlineExecutorService.execute(InlineExecutorService.java:75)
	at java.base/java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:145)
	at org.apache.catalina.core.ContainerBase.startInternal(ContainerBase.java:749)
	at org.apache.catalina.core.StandardEngine.startInternal(StandardEngine.java:203)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.core.StandardService.startInternal(StandardService.java:415)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.core.StandardServer.startInternal(StandardServer.java:870)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.startup.Tomcat.start(Tomcat.java:437)
	at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.initialize(TomcatWebServer.java:128)
	... 16 common frames omitted
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'customUserDetailsServiceImpl' defined in file [C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes\com\personneltrackingsystem\service\impl\CustomUserDetailsServiceImpl.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'userRepository' defined in com.personneltrackingsystem.repository.UserRepository defined in @EnableJpaRepositories declared on JpaRepositoriesRegistrar.EnableJpaRepositoriesConfiguration: Cannot resolve reference to bean 'jpaSharedEM_entityManagerFactory' while setting bean property 'entityManager'
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:795)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:237)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1375)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1212)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1443)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1353)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:904)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:782)
	... 57 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'userRepository' defined in com.personneltrackingsystem.repository.UserRepository defined in @EnableJpaRepositories declared on JpaRepositoriesRegistrar.EnableJpaRepositoriesConfiguration: Cannot resolve reference to bean 'jpaSharedEM_entityManagerFactory' while setting bean property 'entityManager'
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:377)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:135)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyPropertyValues(AbstractAutowireCapableBeanFactory.java:1705)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1454)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:599)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1443)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1353)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:904)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:782)
	... 71 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'jpaSharedEM_entityManagerFactory': Cannot resolve reference to bean 'entityManagerFactory' while setting constructor argument
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:377)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:135)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveConstructorArguments(ConstructorResolver.java:682)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:509)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1355)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1185)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:365)
	... 85 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'liquibase' defined in class path resource [org/springframework/boot/autoconfigure/liquibase/LiquibaseAutoConfiguration$LiquibaseConfiguration.class]: liquibase.exception.DatabaseException: org.postgresql.util.PSQLException: LMCL (FATAL): "dbpts" veritaban mevcut deil
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1806)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:313)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:365)
	... 97 common frames omitted
Caused by: liquibase.exception.UnexpectedLiquibaseException: liquibase.exception.DatabaseException: org.postgresql.util.PSQLException: LMCL (FATAL): "dbpts" veritaban mevcut deil
	at liquibase.integration.spring.SpringLiquibase.afterPropertiesSet(SpringLiquibase.java:267)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1853)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1802)
	... 106 common frames omitted
Caused by: liquibase.exception.DatabaseException: org.postgresql.util.PSQLException: LMCL (FATAL): "dbpts" veritaban mevcut deil
	at liquibase.integration.spring.SpringLiquibase.lambda$afterPropertiesSet$0(SpringLiquibase.java:259)
	at liquibase.Scope.lambda$child$0(Scope.java:191)
	at liquibase.Scope.child(Scope.java:200)
	at liquibase.Scope.child(Scope.java:190)
	at liquibase.Scope.child(Scope.java:169)
	at liquibase.Scope.child(Scope.java:257)
	at liquibase.integration.spring.SpringLiquibase.afterPropertiesSet(SpringLiquibase.java:250)
	... 108 common frames omitted
Caused by: org.postgresql.util.PSQLException: LMCL (FATAL): "dbpts" veritaban mevcut deil
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2725)
	at org.postgresql.core.v3.QueryExecutorImpl.readStartupMessages(QueryExecutorImpl.java:2837)
	at org.postgresql.core.v3.QueryExecutorImpl.<init>(QueryExecutorImpl.java:175)
	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:317)
	at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)
	at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:273)
	at org.postgresql.Driver.makeConnection(Driver.java:446)
	at org.postgresql.Driver.connect(Driver.java:298)
	at com.zaxxer.hikari.util.DriverDataSource.getConnection(DriverDataSource.java:137)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:360)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:202)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:461)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:550)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:98)
	at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:111)
	at liquibase.integration.spring.SpringLiquibase.lambda$afterPropertiesSet$0(SpringLiquibase.java:254)
	... 114 common frames omitted
2025-06-20 06:02:57 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - Starting PersonelTrackingSystemApplication using Java 21.0.7 with PID 19240 (C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes started by Lenovo in C:\Users\Lenovo\Desktop\personnel-tracking-system)
2025-06-20 06:02:57 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - No active profile set, falling back to 1 default profile: "default"
2025-06-20 06:03:00 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-20 06:03:00 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-20 06:03:00 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.30]
2025-06-20 06:03:01 [restartedMain] INFO  o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-20 06:03:01 [restartedMain] WARN  c.h.i.impl.HazelcastInstanceFactory - Hazelcast is starting in a Java modular environment (Java 9 and newer) but without proper access to required Java packages. Use additional Java arguments to provide Hazelcast access to Java internal API. The internal API access is used to get the best performance results. Arguments to be used:
 --add-modules java.se --add-exports java.base/jdk.internal.ref=ALL-UNNAMED --add-opens java.base/java.lang=ALL-UNNAMED --add-opens java.base/sun.nio.ch=ALL-UNNAMED --add-opens java.management/sun.management=ALL-UNNAMED --add-opens jdk.management/com.sun.management.internal=ALL-UNNAMED
2025-06-20 06:03:01 [restartedMain] INFO  c.hazelcast.instance.AddressPicker - [LOCAL] [pts-cluster] [5.4.0] Interfaces is disabled, trying to pick one address from TCP-IP config addresses: [127.0.0.1]
2025-06-20 06:03:01 [restartedMain] INFO  com.hazelcast.system.logo - [127.0.0.1]:5701 [pts-cluster] [5.4.0] 
	o    o     o     o---o   o--o o      o---o     o     o----o o--o--o
	|    |    / \       /         |     /         / \    |         |   
	o----o       o     o   o----o |    o             o   o----o    |   
	|    |  *     \   /           |     \       *     \       |    |   
	o    o *       o o---o   o--o o----o o---o *       o o----o    o   
2025-06-20 06:03:01 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Copyright (c) 2008-2024, Hazelcast, Inc. All Rights Reserved.
2025-06-20 06:03:01 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Hazelcast Platform 5.4.0 (20240415) starting at [127.0.0.1]:5701
2025-06-20 06:03:01 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Cluster name: pts-cluster
2025-06-20 06:03:01 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Integrity Checker is disabled. Fail-fast on corrupted executables will not be performed. For more information, see the documentation for Integrity Checker.
2025-06-20 06:03:01 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] The Jet engine is disabled.
To enable the Jet engine on the members, do one of the following:
  - Change member config using Java API: config.getJetConfig().setEnabled(true)
  - Change XML/YAML configuration property: Set hazelcast.jet.enabled to true
  - Add system property: -Dhz.jet.enabled=true (for Hazelcast embedded, works only when loading config via Config.load)
  - Add environment variable: HZ_JET_ENABLED=true (recommended when running container image. For Hazelcast embedded, works only when loading config via Config.load)
2025-06-20 06:03:01 [restartedMain] INFO  com.hazelcast.system.security - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Enable DEBUG/FINE log level for log category com.hazelcast.system.security  or use -Dhazelcast.security.recommendations system property to see security recommendations and the status of current config.
2025-06-20 06:03:01 [restartedMain] INFO  com.hazelcast.instance.impl.Node - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Using TCP/IP discovery
2025-06-20 06:03:01 [restartedMain] WARN  com.hazelcast.cp.CPSubsystem - [127.0.0.1]:5701 [pts-cluster] [5.4.0] CP Subsystem is not enabled. CP data structures will operate in UNSAFE mode! Please note that UNSAFE mode will not provide strong consistency guarantees.
2025-06-20 06:03:02 [restartedMain] INFO  c.h.internal.diagnostics.Diagnostics - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Diagnostics disabled. To enable add -Dhazelcast.diagnostics.enabled=true to the JVM arguments.
2025-06-20 06:03:02 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is STARTING
2025-06-20 06:03:02 [hz.personnel-tracking-system.cached.thread-3] INFO  c.h.i.cluster.impl.TcpIpJoiner - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5703 is added to the blacklist.
2025-06-20 06:03:02 [hz.personnel-tracking-system.cached.thread-2] INFO  c.h.i.cluster.impl.TcpIpJoiner - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5702 is added to the blacklist.
2025-06-20 06:03:03 [restartedMain] INFO  c.h.internal.cluster.ClusterService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] 

Members {size:1, ver:1} [
	Member [127.0.0.1]:5701 - c4a58dd7-377a-40b0-b65b-fd8b530ee2d2 this
]

2025-06-20 06:03:03 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is STARTED
2025-06-20 06:03:03 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-1 - Starting...
2025-06-20 06:03:04 [restartedMain] ERROR o.s.b.w.e.tomcat.TomcatStarter - Error starting Tomcat context. Exception: org.springframework.beans.factory.UnsatisfiedDependencyException. Message: Error creating bean with name 'jwtAuthenticationFilter' defined in file [C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes\com\personneltrackingsystem\filter\JwtAuthenticationFilter.class]: Unsatisfied dependency expressed through constructor parameter 1: Error creating bean with name 'customUserDetailsServiceImpl' defined in file [C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes\com\personneltrackingsystem\service\impl\CustomUserDetailsServiceImpl.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'userRepository' defined in com.personneltrackingsystem.repository.UserRepository defined in @EnableJpaRepositories declared on JpaRepositoriesRegistrar.EnableJpaRepositoriesConfiguration: Cannot resolve reference to bean 'jpaSharedEM_entityManagerFactory' while setting bean property 'entityManager'
2025-06-20 06:03:04 [restartedMain] INFO  o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-06-20 06:03:04 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.event-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:435)
 com.hazelcast.internal.util.executor.StripedExecutor$Worker.run(StripedExecutor.java:227)
2025-06-20 06:03:04 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.event-2] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:435)
 com.hazelcast.internal.util.executor.StripedExecutor$Worker.run(StripedExecutor.java:227)
2025-06-20 06:03:04 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.event-3] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:435)
 com.hazelcast.internal.util.executor.StripedExecutor$Worker.run(StripedExecutor.java:227)
2025-06-20 06:03:04 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.event-4] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:435)
 com.hazelcast.internal.util.executor.StripedExecutor$Worker.run(StripedExecutor.java:227)
2025-06-20 06:03:04 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.event-5] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:435)
 com.hazelcast.internal.util.executor.StripedExecutor$Worker.run(StripedExecutor.java:227)
2025-06-20 06:03:04 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.MetricsRegistry.thread-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:269)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1763)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 06:03:04 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.MetricsRegistry.thread-2] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1177)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 06:03:04 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.migration] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:269)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1763)
 java.base/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:460)
 com.hazelcast.internal.partition.impl.MigrationQueue.poll(MigrationQueue.java:48)
 com.hazelcast.internal.partition.impl.MigrationThread.doRun(MigrationThread.java:91)
 com.hazelcast.internal.partition.impl.MigrationThread.run(MigrationThread.java:66)
2025-06-20 06:03:04 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.InvocationMonitorThread] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:269)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1763)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 06:03:04 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.SlowOperationDetectorThread] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/java.lang.Thread.sleep0(Native Method)
 java.base/java.lang.Thread.sleep(Thread.java:558)
 java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
 com.hazelcast.spi.impl.operationexecutor.slowoperationdetector.SlowOperationDetector$DetectorThread.sleepInterval(SlowOperationDetector.java:280)
 com.hazelcast.spi.impl.operationexecutor.slowoperationdetector.SlowOperationDetector$DetectorThread.run(SlowOperationDetector.java:153)
2025-06-20 06:03:04 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-in-0] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:142)
 com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:292)
 com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249)
 com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111)
2025-06-20 06:03:04 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-in-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:142)
 com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:292)
 com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249)
 com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111)
2025-06-20 06:03:04 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-in-2] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:142)
 com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:292)
 com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249)
 com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111)
2025-06-20 06:03:04 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-out-0] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:142)
 com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:292)
 com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249)
 com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111)
2025-06-20 06:03:04 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-out-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:142)
 com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:292)
 com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249)
 com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111)
2025-06-20 06:03:04 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-out-2] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:142)
 com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:292)
 com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249)
 com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111)
2025-06-20 06:03:04 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.BalancerThread] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:269)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1763)
 java.base/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:460)
 com.hazelcast.internal.networking.nio.iobalancer.IOBalancerThread.run(IOBalancerThread.java:65)
2025-06-20 06:03:04 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-Acceptor] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:147)
 com.hazelcast.internal.server.tcp.TcpServerAcceptor$AcceptorIOThread.acceptLoop(TcpServerAcceptor.java:186)
 com.hazelcast.internal.server.tcp.TcpServerAcceptor$AcceptorIOThread.run(TcpServerAcceptor.java:172)
2025-06-20 06:03:04 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.TcpServer.thread-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1177)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 06:03:04 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.TcpServer.thread-2] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1177)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 06:03:04 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.TcpServer.thread-4] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1170)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 06:03:04 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.TcpServer.thread-3] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1170)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 06:03:04 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.HealthMonitor] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/java.lang.Thread.sleep0(Native Method)
 java.base/java.lang.Thread.sleep(Thread.java:558)
 java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
 com.hazelcast.internal.diagnostics.HealthMonitor$HealthMonitorThread.run(HealthMonitor.java:164)
2025-06-20 06:03:04 [restartedMain] WARN  o.s.b.w.s.c.AnnotationConfigServletWebServerApplicationContext - Exception encountered during context initialization - cancelling refresh attempt: org.springframework.context.ApplicationContextException: Unable to start web server
2025-06-20 06:03:04 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is SHUTTING_DOWN
2025-06-20 06:03:04 [restartedMain] INFO  com.hazelcast.instance.impl.Node - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Shutting down connection manager...
2025-06-20 06:03:04 [restartedMain] INFO  com.hazelcast.instance.impl.Node - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Shutting down node engine...
2025-06-20 06:03:04 [restartedMain] INFO  c.h.instance.impl.NodeExtension - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Destroying node NodeExtension.
2025-06-20 06:03:04 [restartedMain] INFO  com.hazelcast.instance.impl.Node - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Hazelcast Shutdown is completed in 22 ms.
2025-06-20 06:03:04 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is SHUTDOWN
2025-06-20 06:03:04 [restartedMain] ERROR o.s.boot.SpringApplication - Application run failed
org.springframework.context.ApplicationContextException: Unable to start web server
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.onRefresh(ServletWebServerApplicationContext.java:165)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:619)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:754)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:456)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:335)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1363)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1352)
	at com.personneltrackingsystem.PersonelTrackingSystemApplication.main(PersonelTrackingSystemApplication.java:12)
	at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)
	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
Caused by: org.springframework.boot.web.server.WebServerException: Unable to start embedded Tomcat
	at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.initialize(TomcatWebServer.java:147)
	at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.<init>(TomcatWebServer.java:107)
	at org.springframework.boot.web.embedded.tomcat.TomcatServletWebServerFactory.getTomcatWebServer(TomcatServletWebServerFactory.java:516)
	at org.springframework.boot.web.embedded.tomcat.TomcatServletWebServerFactory.getWebServer(TomcatServletWebServerFactory.java:222)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.createWebServer(ServletWebServerApplicationContext.java:188)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.onRefresh(ServletWebServerApplicationContext.java:162)
	... 11 common frames omitted
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'jwtAuthenticationFilter' defined in file [C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes\com\personneltrackingsystem\filter\JwtAuthenticationFilter.class]: Unsatisfied dependency expressed through constructor parameter 1: Error creating bean with name 'customUserDetailsServiceImpl' defined in file [C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes\com\personneltrackingsystem\service\impl\CustomUserDetailsServiceImpl.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'userRepository' defined in com.personneltrackingsystem.repository.UserRepository defined in @EnableJpaRepositories declared on JpaRepositoriesRegistrar.EnableJpaRepositoriesConfiguration: Cannot resolve reference to bean 'jpaSharedEM_entityManagerFactory' while setting bean property 'entityManager'
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:795)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:237)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1375)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1212)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:205)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.getOrderedBeansOfType(ServletContextInitializerBeans.java:211)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.addAsRegistrationBean(ServletContextInitializerBeans.java:174)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.addAsRegistrationBean(ServletContextInitializerBeans.java:169)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.addAdaptableBeans(ServletContextInitializerBeans.java:154)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.<init>(ServletContextInitializerBeans.java:87)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.getServletContextInitializerBeans(ServletWebServerApplicationContext.java:266)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.selfInitialize(ServletWebServerApplicationContext.java:240)
	at org.springframework.boot.web.embedded.tomcat.TomcatStarter.onStartup(TomcatStarter.java:52)
	at org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:4412)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1203)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1193)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at org.apache.tomcat.util.threads.InlineExecutorService.execute(InlineExecutorService.java:75)
	at java.base/java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:145)
	at org.apache.catalina.core.ContainerBase.startInternal(ContainerBase.java:749)
	at org.apache.catalina.core.StandardHost.startInternal(StandardHost.java:772)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1203)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1193)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at org.apache.tomcat.util.threads.InlineExecutorService.execute(InlineExecutorService.java:75)
	at java.base/java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:145)
	at org.apache.catalina.core.ContainerBase.startInternal(ContainerBase.java:749)
	at org.apache.catalina.core.StandardEngine.startInternal(StandardEngine.java:203)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.core.StandardService.startInternal(StandardService.java:415)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.core.StandardServer.startInternal(StandardServer.java:870)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.startup.Tomcat.start(Tomcat.java:437)
	at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.initialize(TomcatWebServer.java:128)
	... 16 common frames omitted
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'customUserDetailsServiceImpl' defined in file [C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes\com\personneltrackingsystem\service\impl\CustomUserDetailsServiceImpl.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'userRepository' defined in com.personneltrackingsystem.repository.UserRepository defined in @EnableJpaRepositories declared on JpaRepositoriesRegistrar.EnableJpaRepositoriesConfiguration: Cannot resolve reference to bean 'jpaSharedEM_entityManagerFactory' while setting bean property 'entityManager'
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:795)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:237)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1375)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1212)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1443)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1353)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:904)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:782)
	... 57 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'userRepository' defined in com.personneltrackingsystem.repository.UserRepository defined in @EnableJpaRepositories declared on JpaRepositoriesRegistrar.EnableJpaRepositoriesConfiguration: Cannot resolve reference to bean 'jpaSharedEM_entityManagerFactory' while setting bean property 'entityManager'
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:377)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:135)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyPropertyValues(AbstractAutowireCapableBeanFactory.java:1705)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1454)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:599)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1443)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1353)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:904)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:782)
	... 71 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'jpaSharedEM_entityManagerFactory': Cannot resolve reference to bean 'entityManagerFactory' while setting constructor argument
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:377)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:135)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveConstructorArguments(ConstructorResolver.java:682)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:509)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1355)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1185)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:365)
	... 85 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'liquibase' defined in class path resource [org/springframework/boot/autoconfigure/liquibase/LiquibaseAutoConfiguration$LiquibaseConfiguration.class]: liquibase.exception.DatabaseException: org.postgresql.util.PSQLException: LMCL (FATAL): "dbpts" veritaban mevcut deil
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1806)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:313)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:365)
	... 97 common frames omitted
Caused by: liquibase.exception.UnexpectedLiquibaseException: liquibase.exception.DatabaseException: org.postgresql.util.PSQLException: LMCL (FATAL): "dbpts" veritaban mevcut deil
	at liquibase.integration.spring.SpringLiquibase.afterPropertiesSet(SpringLiquibase.java:267)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1853)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1802)
	... 106 common frames omitted
Caused by: liquibase.exception.DatabaseException: org.postgresql.util.PSQLException: LMCL (FATAL): "dbpts" veritaban mevcut deil
	at liquibase.integration.spring.SpringLiquibase.lambda$afterPropertiesSet$0(SpringLiquibase.java:259)
	at liquibase.Scope.lambda$child$0(Scope.java:191)
	at liquibase.Scope.child(Scope.java:200)
	at liquibase.Scope.child(Scope.java:190)
	at liquibase.Scope.child(Scope.java:169)
	at liquibase.Scope.child(Scope.java:257)
	at liquibase.integration.spring.SpringLiquibase.afterPropertiesSet(SpringLiquibase.java:250)
	... 108 common frames omitted
Caused by: org.postgresql.util.PSQLException: LMCL (FATAL): "dbpts" veritaban mevcut deil
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2725)
	at org.postgresql.core.v3.QueryExecutorImpl.readStartupMessages(QueryExecutorImpl.java:2837)
	at org.postgresql.core.v3.QueryExecutorImpl.<init>(QueryExecutorImpl.java:175)
	at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:317)
	at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)
	at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:273)
	at org.postgresql.Driver.makeConnection(Driver.java:446)
	at org.postgresql.Driver.connect(Driver.java:298)
	at com.zaxxer.hikari.util.DriverDataSource.getConnection(DriverDataSource.java:137)
	at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:360)
	at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:202)
	at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:461)
	at com.zaxxer.hikari.pool.HikariPool.checkFailFast(HikariPool.java:550)
	at com.zaxxer.hikari.pool.HikariPool.<init>(HikariPool.java:98)
	at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:111)
	at liquibase.integration.spring.SpringLiquibase.lambda$afterPropertiesSet$0(SpringLiquibase.java:254)
	... 114 common frames omitted
2025-06-20 06:07:04 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - Starting PersonelTrackingSystemApplication using Java 21.0.7 with PID 1032 (C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes started by Lenovo in C:\Users\Lenovo\Desktop\personnel-tracking-system)
2025-06-20 06:07:04 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - No active profile set, falling back to 1 default profile: "default"
2025-06-20 06:07:07 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-20 06:07:07 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-20 06:07:07 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.30]
2025-06-20 06:07:07 [restartedMain] INFO  o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-20 06:07:07 [restartedMain] WARN  c.h.i.impl.HazelcastInstanceFactory - Hazelcast is starting in a Java modular environment (Java 9 and newer) but without proper access to required Java packages. Use additional Java arguments to provide Hazelcast access to Java internal API. The internal API access is used to get the best performance results. Arguments to be used:
 --add-modules java.se --add-exports java.base/jdk.internal.ref=ALL-UNNAMED --add-opens java.base/java.lang=ALL-UNNAMED --add-opens java.base/sun.nio.ch=ALL-UNNAMED --add-opens java.management/sun.management=ALL-UNNAMED --add-opens jdk.management/com.sun.management.internal=ALL-UNNAMED
2025-06-20 06:07:07 [restartedMain] INFO  c.hazelcast.instance.AddressPicker - [LOCAL] [pts-cluster] [5.4.0] Interfaces is disabled, trying to pick one address from TCP-IP config addresses: [127.0.0.1]
2025-06-20 06:07:07 [restartedMain] INFO  com.hazelcast.system.logo - [127.0.0.1]:5701 [pts-cluster] [5.4.0] 
	o    o     o     o---o   o--o o      o---o     o     o----o o--o--o
	|    |    / \       /         |     /         / \    |         |   
	o----o       o     o   o----o |    o             o   o----o    |   
	|    |  *     \   /           |     \       *     \       |    |   
	o    o *       o o---o   o--o o----o o---o *       o o----o    o   
2025-06-20 06:07:07 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Copyright (c) 2008-2024, Hazelcast, Inc. All Rights Reserved.
2025-06-20 06:07:07 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Hazelcast Platform 5.4.0 (20240415) starting at [127.0.0.1]:5701
2025-06-20 06:07:07 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Cluster name: pts-cluster
2025-06-20 06:07:07 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Integrity Checker is disabled. Fail-fast on corrupted executables will not be performed. For more information, see the documentation for Integrity Checker.
2025-06-20 06:07:07 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] The Jet engine is disabled.
To enable the Jet engine on the members, do one of the following:
  - Change member config using Java API: config.getJetConfig().setEnabled(true)
  - Change XML/YAML configuration property: Set hazelcast.jet.enabled to true
  - Add system property: -Dhz.jet.enabled=true (for Hazelcast embedded, works only when loading config via Config.load)
  - Add environment variable: HZ_JET_ENABLED=true (recommended when running container image. For Hazelcast embedded, works only when loading config via Config.load)
2025-06-20 06:07:08 [restartedMain] INFO  com.hazelcast.system.security - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Enable DEBUG/FINE log level for log category com.hazelcast.system.security  or use -Dhazelcast.security.recommendations system property to see security recommendations and the status of current config.
2025-06-20 06:07:08 [restartedMain] INFO  com.hazelcast.instance.impl.Node - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Using TCP/IP discovery
2025-06-20 06:07:08 [restartedMain] WARN  com.hazelcast.cp.CPSubsystem - [127.0.0.1]:5701 [pts-cluster] [5.4.0] CP Subsystem is not enabled. CP data structures will operate in UNSAFE mode! Please note that UNSAFE mode will not provide strong consistency guarantees.
2025-06-20 06:07:08 [restartedMain] INFO  c.h.internal.diagnostics.Diagnostics - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Diagnostics disabled. To enable add -Dhazelcast.diagnostics.enabled=true to the JVM arguments.
2025-06-20 06:07:08 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is STARTING
2025-06-20 06:07:08 [hz.personnel-tracking-system.cached.thread-3] INFO  c.h.i.cluster.impl.TcpIpJoiner - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5702 is added to the blacklist.
2025-06-20 06:07:08 [hz.personnel-tracking-system.cached.thread-2] INFO  c.h.i.cluster.impl.TcpIpJoiner - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5703 is added to the blacklist.
2025-06-20 06:07:09 [restartedMain] INFO  c.h.internal.cluster.ClusterService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] 

Members {size:1, ver:1} [
	Member [127.0.0.1]:5701 - da4888c2-52ad-4c65-8e9b-6812ca8b40e6 this
]

2025-06-20 06:07:09 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is STARTED
2025-06-20 06:07:10 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-1 - Starting...
2025-06-20 06:07:10 [restartedMain] INFO  com.zaxxer.hikari.pool.HikariPool - HikariPool-1 - Added connection org.postgresql.jdbc.PgConnection@62992aac
2025-06-20 06:07:10 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-1 - Start completed.
2025-06-20 06:07:10 [restartedMain] INFO  liquibase.changelog - Creating database history table with name: dbpersonel.databasechangelog
2025-06-20 06:07:10 [restartedMain] INFO  liquibase.changelog - Reading from dbpersonel.databasechangelog
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.lockservice - Successfully acquired change log lock
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.command - Using deploymentId: 0388831040
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Reading from dbpersonel.databasechangelog
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::1::liquibase
2025-06-20 06:07:11 [restartedMain] WARN  liquibase.executor - "dbpersonel" emas zaten mevcut, atlanyor
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::1::liquibase ran successfully in 21ms
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::2::liquibase
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::2::liquibase ran successfully in 116ms
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::3::liquibase
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Table personel_type created
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::3::liquibase ran successfully in 10ms
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::4::liquibase
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Table building created
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::4::liquibase ran successfully in 5ms
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::5::liquibase
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Table floor created
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::5::liquibase ran successfully in 13ms
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::6::liquibase
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Table personel created
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::6::liquibase ran successfully in 12ms
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::7::liquibase
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Table unit created
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::7::liquibase ran successfully in 10ms
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::8::liquibase
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Table personel_unit created
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::8::liquibase ran successfully in 9ms
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::9::liquibase
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Table gate created
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::9::liquibase ran successfully in 9ms
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::10::liquibase
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Table turnstile created
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::10::liquibase ran successfully in 8ms
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::11::liquibase
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Table turnstile_registration_log created
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::11::liquibase ran successfully in 7ms
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::12::liquibase
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Table working_hours created
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::12::liquibase ran successfully in 9ms
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::13::liquibase
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Table salary created
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::13::liquibase ran successfully in 17ms
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::14::liquibase
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Table user created
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::14::liquibase ran successfully in 16ms
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::15::liquibase
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Table permission created
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::15::liquibase ran successfully in 13ms
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::16::liquibase
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Table role_permission created
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::16::liquibase ran successfully in 11ms
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::17::liquibase
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::17::liquibase ran successfully in 86ms
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::1::liquibase
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - New row inserted into personel_type
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - New row inserted into personel_type
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - New row inserted into personel_type
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::1::liquibase ran successfully in 7ms
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::2::liquibase
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - New row inserted into building
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - New row inserted into building
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - New row inserted into building
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::2::liquibase ran successfully in 12ms
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::3::liquibase
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Data loaded from 'db/changelog/data/floor-data.csv' into table 'floor'
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::3::liquibase ran successfully in 46ms
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::4::liquibase
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - New row inserted into personel
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - New row inserted into personel
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - New row inserted into personel
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - New row inserted into personel
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - New row inserted into personel
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - New row inserted into personel
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::4::liquibase ran successfully in 14ms
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::5::liquibase
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - New row inserted into unit
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - New row inserted into unit
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - New row inserted into unit
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - New row inserted into unit
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - New row inserted into unit
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::5::liquibase ran successfully in 13ms
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::6::liquibase
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - New row inserted into personel_unit
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - New row inserted into personel_unit
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - New row inserted into personel_unit
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - New row inserted into personel_unit
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::6::liquibase ran successfully in 11ms
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::7::liquibase
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Data loaded from 'db/changelog/data/gate-data.csv' into table 'gate'
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::7::liquibase ran successfully in 12ms
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::8::liquibase
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - Data loaded from 'db/changelog/data/turnstile-data.csv' into table 'turnstile'
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::8::liquibase ran successfully in 10ms
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::9::liquibase
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - New row inserted into working_hours
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - New row inserted into working_hours
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - New row inserted into working_hours
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::9::liquibase ran successfully in 8ms
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::10::liquibase
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - New row inserted into permission
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - New row inserted into permission
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - New row inserted into permission
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - New row inserted into permission
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - New row inserted into permission
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::10::liquibase ran successfully in 14ms
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::11::liquibase
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - New row inserted into role_permission
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - New row inserted into role_permission
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - New row inserted into role_permission
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - New row inserted into role_permission
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - New row inserted into role_permission
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::11::liquibase ran successfully in 10ms
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.util - UPDATE SUMMARY
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.util - Run:                         28
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.util - Previously run:               0
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.util - Filtered out:                 0
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.util - -------------------------------
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.util - Total change sets:           28
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.util - Update summary generated
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.command - Update command completed successfully.
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.ui - Liquibase: Update has been successful. Rows affected: 130
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.lockservice - Successfully released change log lock
2025-06-20 06:07:11 [restartedMain] INFO  liquibase.command - Command execution complete
2025-06-20 06:07:12 [restartedMain] WARN  org.hibernate.orm.deprecation - HHH90000025: PostgreSQLDialect does not need to be specified explicitly using 'hibernate.dialect' (remove the property setting and it will be selected by default)
2025-06-20 06:07:14 [restartedMain] WARN  o.s.s.c.a.a.c.InitializeUserDetailsBeanManagerConfigurer$InitializeUserDetailsManagerConfigurer - Global AuthenticationManager configured with an AuthenticationProvider bean. UserDetailsService beans will not be used for username/password login. Consider removing the AuthenticationProvider bean. Alternatively, consider using the UserDetailsService in a manually instantiated DaoAuthenticationProvider.
2025-06-20 06:07:14 [restartedMain] INFO  c.p.s.impl.PersonelTypeServiceImpl - Checking for default personnel types...
2025-06-20 06:07:15 [restartedMain] INFO  c.p.s.impl.PersonelTypeServiceImpl - Personnel types already exist in the database.
2025-06-20 06:07:15 [restartedMain] WARN  o.s.b.a.o.j.JpaBaseConfiguration$JpaWebConfiguration - spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
2025-06-20 06:07:16 [restartedMain] INFO  o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = personneltrackingsystem-admin-0
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-20 06:07:16 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 06:07:16 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 06:07:16 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750388836658
2025-06-20 06:07:17 [kafka-admin-client-thread | personneltrackingsystem-admin-0] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for personneltrackingsystem-admin-0 unregistered
2025-06-20 06:07:17 [kafka-admin-client-thread | personneltrackingsystem-admin-0] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-20 06:07:17 [kafka-admin-client-thread | personneltrackingsystem-admin-0] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-20 06:07:17 [kafka-admin-client-thread | personneltrackingsystem-admin-0] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-20 06:07:17 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-20 06:07:17 [restartedMain] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pts-group-reset-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pts-group-reset
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-06-20 06:07:17 [restartedMain] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 06:07:17 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 06:07:17 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 06:07:17 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750388837748
2025-06-20 06:07:17 [restartedMain] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Subscribed to topic(s): turnstile-request
2025-06-20 06:07:17 [restartedMain] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pts-group-reset-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pts-group-reset
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-06-20 06:07:17 [restartedMain] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 06:07:17 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 06:07:17 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 06:07:17 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750388837784
2025-06-20 06:07:17 [restartedMain] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Subscribed to topic(s): email-notification
2025-06-20 06:07:17 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 06:07:17 [restartedMain] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pts-group-reset-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pts-group-reset
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-06-20 06:07:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 06:07:17 [restartedMain] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 06:07:17 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 06:07:17 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 06:07:17 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750388837833
2025-06-20 06:07:17 [restartedMain] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Subscribed to topic(s): turnstile-passage
2025-06-20 06:07:17 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 06:07:17 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - Started PersonelTrackingSystemApplication in 13.73 seconds (process running for 14.323)
2025-06-20 06:07:18 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2025-06-20 06:07:18 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:07:18 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Request joining group due to: need to re-join with the given member-id: consumer-pts-group-reset-2-3c5c5a5e-c970-4194-84a0-22f5a6d6acee
2025-06-20 06:07:18 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:07:18 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=1, memberId='consumer-pts-group-reset-2-3c5c5a5e-c970-4194-84a0-22f5a6d6acee', protocol='range'}
2025-06-20 06:07:18 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Finished assignment for group at generation 1: {consumer-pts-group-reset-2-3c5c5a5e-c970-4194-84a0-22f5a6d6acee=Assignment(partitions=[email-notification-0])}
2025-06-20 06:07:18 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2025-06-20 06:07:18 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:07:18 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Request joining group due to: need to re-join with the given member-id: consumer-pts-group-reset-3-b77b4a69-5428-46c8-9b79-90903598ffb1
2025-06-20 06:07:18 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=1, memberId='consumer-pts-group-reset-2-3c5c5a5e-c970-4194-84a0-22f5a6d6acee', protocol='range'}
2025-06-20 06:07:18 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:07:18 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[email-notification-0])
2025-06-20 06:07:18 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Adding newly assigned partitions: email-notification-0
2025-06-20 06:07:18 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Found no committed offset for partition email-notification-0
2025-06-20 06:07:18 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Found no committed offset for partition email-notification-0
2025-06-20 06:07:18 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Resetting offset for partition email-notification-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}.
2025-06-20 06:07:19 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2025-06-20 06:07:19 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:07:19 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Request joining group due to: need to re-join with the given member-id: consumer-pts-group-reset-1-a159ce5b-e091-4154-b3ae-4900cfbfc7df
2025-06-20 06:07:19 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:07:21 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Request joining group due to: group is already rebalancing
2025-06-20 06:07:21 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Revoke previously assigned partitions email-notification-0
2025-06-20 06:07:21 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:07:21 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=2, memberId='consumer-pts-group-reset-1-a159ce5b-e091-4154-b3ae-4900cfbfc7df', protocol='range'}
2025-06-20 06:07:21 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=2, memberId='consumer-pts-group-reset-3-b77b4a69-5428-46c8-9b79-90903598ffb1', protocol='range'}
2025-06-20 06:07:21 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=2, memberId='consumer-pts-group-reset-2-3c5c5a5e-c970-4194-84a0-22f5a6d6acee', protocol='range'}
2025-06-20 06:07:22 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Finished assignment for group at generation 2: {consumer-pts-group-reset-2-3c5c5a5e-c970-4194-84a0-22f5a6d6acee=Assignment(partitions=[email-notification-0]), consumer-pts-group-reset-1-a159ce5b-e091-4154-b3ae-4900cfbfc7df=Assignment(partitions=[turnstile-request-0]), consumer-pts-group-reset-3-b77b4a69-5428-46c8-9b79-90903598ffb1=Assignment(partitions=[turnstile-passage-0])}
2025-06-20 06:07:22 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=2, memberId='consumer-pts-group-reset-3-b77b4a69-5428-46c8-9b79-90903598ffb1', protocol='range'}
2025-06-20 06:07:22 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=2, memberId='consumer-pts-group-reset-1-a159ce5b-e091-4154-b3ae-4900cfbfc7df', protocol='range'}
2025-06-20 06:07:22 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=2, memberId='consumer-pts-group-reset-2-3c5c5a5e-c970-4194-84a0-22f5a6d6acee', protocol='range'}
2025-06-20 06:07:22 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[turnstile-passage-0])
2025-06-20 06:07:22 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[turnstile-request-0])
2025-06-20 06:07:22 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[email-notification-0])
2025-06-20 06:07:22 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Adding newly assigned partitions: turnstile-passage-0
2025-06-20 06:07:22 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Adding newly assigned partitions: email-notification-0
2025-06-20 06:07:22 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Adding newly assigned partitions: turnstile-request-0
2025-06-20 06:07:22 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Found no committed offset for partition turnstile-request-0
2025-06-20 06:07:22 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Found no committed offset for partition turnstile-passage-0
2025-06-20 06:07:22 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Found no committed offset for partition turnstile-request-0
2025-06-20 06:07:22 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition email-notification-0 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 06:07:22 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Found no committed offset for partition turnstile-passage-0
2025-06-20 06:07:22 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Resetting offset for partition turnstile-request-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}.
2025-06-20 06:07:22 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.SubscriptionState - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Resetting offset for partition turnstile-passage-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}.
2025-06-20 06:07:48 [http-nio-8080-exec-1] INFO  o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-06-20 06:07:50 [http-nio-8080-exec-9] INFO  o.s.api.AbstractOpenApiResource - Init duration for springdoc-openapi is: 1129 ms
2025-06-20 06:16:51 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - Starting PersonelTrackingSystemApplication using Java 21.0.7 with PID 3280 (C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes started by Lenovo in C:\Users\Lenovo\Desktop\personnel-tracking-system)
2025-06-20 06:16:51 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - No active profile set, falling back to 1 default profile: "default"
2025-06-20 06:16:53 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-20 06:16:53 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-20 06:16:53 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.30]
2025-06-20 06:16:53 [restartedMain] INFO  o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-20 06:16:53 [restartedMain] WARN  c.h.i.impl.HazelcastInstanceFactory - Hazelcast is starting in a Java modular environment (Java 9 and newer) but without proper access to required Java packages. Use additional Java arguments to provide Hazelcast access to Java internal API. The internal API access is used to get the best performance results. Arguments to be used:
 --add-modules java.se --add-exports java.base/jdk.internal.ref=ALL-UNNAMED --add-opens java.base/java.lang=ALL-UNNAMED --add-opens java.base/sun.nio.ch=ALL-UNNAMED --add-opens java.management/sun.management=ALL-UNNAMED --add-opens jdk.management/com.sun.management.internal=ALL-UNNAMED
2025-06-20 06:16:53 [restartedMain] INFO  c.hazelcast.instance.AddressPicker - [LOCAL] [pts-cluster] [5.4.0] Interfaces is disabled, trying to pick one address from TCP-IP config addresses: [127.0.0.1]
2025-06-20 06:16:54 [restartedMain] INFO  com.hazelcast.system.logo - [127.0.0.1]:5701 [pts-cluster] [5.4.0] 
	o    o     o     o---o   o--o o      o---o     o     o----o o--o--o
	|    |    / \       /         |     /         / \    |         |   
	o----o       o     o   o----o |    o             o   o----o    |   
	|    |  *     \   /           |     \       *     \       |    |   
	o    o *       o o---o   o--o o----o o---o *       o o----o    o   
2025-06-20 06:16:54 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Copyright (c) 2008-2024, Hazelcast, Inc. All Rights Reserved.
2025-06-20 06:16:54 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Hazelcast Platform 5.4.0 (20240415) starting at [127.0.0.1]:5701
2025-06-20 06:16:54 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Cluster name: pts-cluster
2025-06-20 06:16:54 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Integrity Checker is disabled. Fail-fast on corrupted executables will not be performed. For more information, see the documentation for Integrity Checker.
2025-06-20 06:16:54 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] The Jet engine is disabled.
To enable the Jet engine on the members, do one of the following:
  - Change member config using Java API: config.getJetConfig().setEnabled(true)
  - Change XML/YAML configuration property: Set hazelcast.jet.enabled to true
  - Add system property: -Dhz.jet.enabled=true (for Hazelcast embedded, works only when loading config via Config.load)
  - Add environment variable: HZ_JET_ENABLED=true (recommended when running container image. For Hazelcast embedded, works only when loading config via Config.load)
2025-06-20 06:16:54 [restartedMain] INFO  com.hazelcast.system.security - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Enable DEBUG/FINE log level for log category com.hazelcast.system.security  or use -Dhazelcast.security.recommendations system property to see security recommendations and the status of current config.
2025-06-20 06:16:54 [restartedMain] INFO  com.hazelcast.instance.impl.Node - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Using TCP/IP discovery
2025-06-20 06:16:54 [restartedMain] WARN  com.hazelcast.cp.CPSubsystem - [127.0.0.1]:5701 [pts-cluster] [5.4.0] CP Subsystem is not enabled. CP data structures will operate in UNSAFE mode! Please note that UNSAFE mode will not provide strong consistency guarantees.
2025-06-20 06:16:54 [restartedMain] INFO  c.h.internal.diagnostics.Diagnostics - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Diagnostics disabled. To enable add -Dhazelcast.diagnostics.enabled=true to the JVM arguments.
2025-06-20 06:16:54 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is STARTING
2025-06-20 06:16:55 [hz.personnel-tracking-system.cached.thread-3] INFO  c.h.i.cluster.impl.TcpIpJoiner - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5703 is added to the blacklist.
2025-06-20 06:16:55 [hz.personnel-tracking-system.cached.thread-2] INFO  c.h.i.cluster.impl.TcpIpJoiner - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5702 is added to the blacklist.
2025-06-20 06:16:56 [restartedMain] INFO  c.h.internal.cluster.ClusterService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] 

Members {size:1, ver:1} [
	Member [127.0.0.1]:5701 - 58c73fd1-2c2d-45d7-9c00-e86322486995 this
]

2025-06-20 06:16:56 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is STARTED
2025-06-20 06:16:56 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-1 - Starting...
2025-06-20 06:16:56 [restartedMain] INFO  com.zaxxer.hikari.pool.HikariPool - HikariPool-1 - Added connection org.postgresql.jdbc.PgConnection@347c68ec
2025-06-20 06:16:56 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-1 - Start completed.
2025-06-20 06:16:57 [restartedMain] INFO  liquibase.changelog - Reading from dbpersonel.databasechangelog
2025-06-20 06:16:57 [restartedMain] INFO  liquibase.changelog - Change failed validation!
2025-06-20 06:16:57 [restartedMain] INFO  liquibase.command - Logging exception.
2025-06-20 06:16:57 [restartedMain] INFO  liquibase.ui - ERROR: Exception Details
2025-06-20 06:16:57 [restartedMain] INFO  liquibase.ui - ERROR: Exception Primary Class:  ValidationFailedException
2025-06-20 06:16:57 [restartedMain] INFO  liquibase.ui - ERROR: Exception Primary Reason:  Validation Failed:
     2 changesets check sum
          db/changelog/changes/002-insert-initial-data.yaml::4::liquibase was: 9:3f6144c3432e02fea49aea35f2e2e7c8 but is now: 9:d4666daa1e1cf3996a174df0e04044af
          db/changelog/changes/002-insert-initial-data.yaml::5::liquibase was: 9:e45b97d889e105e6b966e459a8d76408 but is now: 9:0a4c30cedd8a2c460ddf8b2e543dbefa

2025-06-20 06:16:57 [restartedMain] INFO  liquibase.ui - ERROR: Exception Primary Source:  4.29.2
2025-06-20 06:16:57 [restartedMain] INFO  liquibase.command - Command execution complete
2025-06-20 06:16:57 [restartedMain] ERROR o.s.b.w.e.tomcat.TomcatStarter - Error starting Tomcat context. Exception: org.springframework.beans.factory.UnsatisfiedDependencyException. Message: Error creating bean with name 'jwtAuthenticationFilter' defined in file [C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes\com\personneltrackingsystem\filter\JwtAuthenticationFilter.class]: Unsatisfied dependency expressed through constructor parameter 1: Error creating bean with name 'customUserDetailsServiceImpl' defined in file [C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes\com\personneltrackingsystem\service\impl\CustomUserDetailsServiceImpl.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'userRepository' defined in com.personneltrackingsystem.repository.UserRepository defined in @EnableJpaRepositories declared on JpaRepositoriesRegistrar.EnableJpaRepositoriesConfiguration: Cannot resolve reference to bean 'jpaSharedEM_entityManagerFactory' while setting bean property 'entityManager'
2025-06-20 06:16:57 [restartedMain] INFO  o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-06-20 06:16:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.event-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:435)
 com.hazelcast.internal.util.executor.StripedExecutor$Worker.run(StripedExecutor.java:227)
2025-06-20 06:16:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.event-2] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:435)
 com.hazelcast.internal.util.executor.StripedExecutor$Worker.run(StripedExecutor.java:227)
2025-06-20 06:16:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.event-3] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:435)
 com.hazelcast.internal.util.executor.StripedExecutor$Worker.run(StripedExecutor.java:227)
2025-06-20 06:16:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.event-4] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:435)
 com.hazelcast.internal.util.executor.StripedExecutor$Worker.run(StripedExecutor.java:227)
2025-06-20 06:16:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.event-5] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:435)
 com.hazelcast.internal.util.executor.StripedExecutor$Worker.run(StripedExecutor.java:227)
2025-06-20 06:16:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.MetricsRegistry.thread-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:269)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1763)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 06:16:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.MetricsRegistry.thread-2] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1177)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 06:16:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.migration] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:269)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1763)
 java.base/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:460)
 com.hazelcast.internal.partition.impl.MigrationQueue.poll(MigrationQueue.java:48)
 com.hazelcast.internal.partition.impl.MigrationThread.doRun(MigrationThread.java:91)
 com.hazelcast.internal.partition.impl.MigrationThread.run(MigrationThread.java:66)
2025-06-20 06:16:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.InvocationMonitorThread] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:269)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1763)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 06:16:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.SlowOperationDetectorThread] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/java.lang.Thread.sleep0(Native Method)
 java.base/java.lang.Thread.sleep(Thread.java:558)
 java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
 com.hazelcast.spi.impl.operationexecutor.slowoperationdetector.SlowOperationDetector$DetectorThread.sleepInterval(SlowOperationDetector.java:280)
 com.hazelcast.spi.impl.operationexecutor.slowoperationdetector.SlowOperationDetector$DetectorThread.run(SlowOperationDetector.java:153)
2025-06-20 06:16:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-in-0] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:142)
 com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:292)
 com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249)
 com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111)
2025-06-20 06:16:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-in-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:142)
 com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:292)
 com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249)
 com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111)
2025-06-20 06:16:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-in-2] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:142)
 com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:292)
 com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249)
 com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111)
2025-06-20 06:16:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-out-0] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:142)
 com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:292)
 com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249)
 com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111)
2025-06-20 06:16:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-out-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:142)
 com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:292)
 com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249)
 com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111)
2025-06-20 06:16:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-out-2] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:142)
 com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:292)
 com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249)
 com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111)
2025-06-20 06:16:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.BalancerThread] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:269)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1763)
 java.base/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:460)
 com.hazelcast.internal.networking.nio.iobalancer.IOBalancerThread.run(IOBalancerThread.java:65)
2025-06-20 06:16:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-Acceptor] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:147)
 com.hazelcast.internal.server.tcp.TcpServerAcceptor$AcceptorIOThread.acceptLoop(TcpServerAcceptor.java:186)
 com.hazelcast.internal.server.tcp.TcpServerAcceptor$AcceptorIOThread.run(TcpServerAcceptor.java:172)
2025-06-20 06:16:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.TcpServer.thread-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1177)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 06:16:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.TcpServer.thread-2] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1177)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 06:16:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.TcpServer.thread-3] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1170)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 06:16:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.TcpServer.thread-4] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1170)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 06:16:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.HealthMonitor] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/java.lang.Thread.sleep0(Native Method)
 java.base/java.lang.Thread.sleep(Thread.java:558)
 java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
 com.hazelcast.internal.diagnostics.HealthMonitor$HealthMonitorThread.run(HealthMonitor.java:164)
2025-06-20 06:16:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [HikariPool-1 housekeeper] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:269)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1763)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 06:16:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [HikariPool-1 connection adder] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.Net.poll(Native Method)
 java.base/sun.nio.ch.NioSocketImpl.park(NioSocketImpl.java:191)
 java.base/sun.nio.ch.NioSocketImpl.timedRead(NioSocketImpl.java:280)
 java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:304)
 java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:346)
 java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:796)
 java.base/java.net.Socket$SocketInputStream.read(Socket.java:1099)
 org.postgresql.core.VisibleBufferedInputStream.readMore(VisibleBufferedInputStream.java:162)
 org.postgresql.core.VisibleBufferedInputStream.ensureBytes(VisibleBufferedInputStream.java:129)
 org.postgresql.core.VisibleBufferedInputStream.ensureBytes(VisibleBufferedInputStream.java:114)
 org.postgresql.core.VisibleBufferedInputStream.read(VisibleBufferedInputStream.java:74)
 org.postgresql.core.PGStream.receiveChar(PGStream.java:467)
 org.postgresql.core.v3.ConnectionFactoryImpl.enableSSL(ConnectionFactoryImpl.java:594)
 org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:195)
 org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:262)
 org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)
 org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:273)
 org.postgresql.Driver.makeConnection(Driver.java:446)
 org.postgresql.Driver.connect(Driver.java:298)
 com.zaxxer.hikari.util.DriverDataSource.getConnection(DriverDataSource.java:137)
 com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:360)
 com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:202)
 com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:461)
 com.zaxxer.hikari.pool.HikariPool$PoolEntryCreator.call(HikariPool.java:724)
 com.zaxxer.hikari.pool.HikariPool$PoolEntryCreator.call(HikariPool.java:703)
 java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 06:16:57 [restartedMain] WARN  o.s.b.w.s.c.AnnotationConfigServletWebServerApplicationContext - Exception encountered during context initialization - cancelling refresh attempt: org.springframework.context.ApplicationContextException: Unable to start web server
2025-06-20 06:16:57 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-1 - Shutdown initiated...
2025-06-20 06:16:57 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-1 - Shutdown completed.
2025-06-20 06:16:57 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is SHUTTING_DOWN
2025-06-20 06:16:57 [restartedMain] INFO  com.hazelcast.instance.impl.Node - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Shutting down connection manager...
2025-06-20 06:16:57 [restartedMain] INFO  com.hazelcast.instance.impl.Node - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Shutting down node engine...
2025-06-20 06:16:57 [restartedMain] INFO  c.h.instance.impl.NodeExtension - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Destroying node NodeExtension.
2025-06-20 06:16:57 [restartedMain] INFO  com.hazelcast.instance.impl.Node - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Hazelcast Shutdown is completed in 27 ms.
2025-06-20 06:16:57 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is SHUTDOWN
2025-06-20 06:16:57 [restartedMain] ERROR o.s.boot.SpringApplication - Application run failed
org.springframework.context.ApplicationContextException: Unable to start web server
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.onRefresh(ServletWebServerApplicationContext.java:165)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:619)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:754)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:456)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:335)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1363)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1352)
	at com.personneltrackingsystem.PersonelTrackingSystemApplication.main(PersonelTrackingSystemApplication.java:12)
	at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)
	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
Caused by: org.springframework.boot.web.server.WebServerException: Unable to start embedded Tomcat
	at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.initialize(TomcatWebServer.java:147)
	at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.<init>(TomcatWebServer.java:107)
	at org.springframework.boot.web.embedded.tomcat.TomcatServletWebServerFactory.getTomcatWebServer(TomcatServletWebServerFactory.java:516)
	at org.springframework.boot.web.embedded.tomcat.TomcatServletWebServerFactory.getWebServer(TomcatServletWebServerFactory.java:222)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.createWebServer(ServletWebServerApplicationContext.java:188)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.onRefresh(ServletWebServerApplicationContext.java:162)
	... 11 common frames omitted
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'jwtAuthenticationFilter' defined in file [C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes\com\personneltrackingsystem\filter\JwtAuthenticationFilter.class]: Unsatisfied dependency expressed through constructor parameter 1: Error creating bean with name 'customUserDetailsServiceImpl' defined in file [C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes\com\personneltrackingsystem\service\impl\CustomUserDetailsServiceImpl.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'userRepository' defined in com.personneltrackingsystem.repository.UserRepository defined in @EnableJpaRepositories declared on JpaRepositoriesRegistrar.EnableJpaRepositoriesConfiguration: Cannot resolve reference to bean 'jpaSharedEM_entityManagerFactory' while setting bean property 'entityManager'
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:795)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:237)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1375)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1212)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:205)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.getOrderedBeansOfType(ServletContextInitializerBeans.java:211)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.addAsRegistrationBean(ServletContextInitializerBeans.java:174)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.addAsRegistrationBean(ServletContextInitializerBeans.java:169)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.addAdaptableBeans(ServletContextInitializerBeans.java:154)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.<init>(ServletContextInitializerBeans.java:87)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.getServletContextInitializerBeans(ServletWebServerApplicationContext.java:266)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.selfInitialize(ServletWebServerApplicationContext.java:240)
	at org.springframework.boot.web.embedded.tomcat.TomcatStarter.onStartup(TomcatStarter.java:52)
	at org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:4412)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1203)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1193)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at org.apache.tomcat.util.threads.InlineExecutorService.execute(InlineExecutorService.java:75)
	at java.base/java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:145)
	at org.apache.catalina.core.ContainerBase.startInternal(ContainerBase.java:749)
	at org.apache.catalina.core.StandardHost.startInternal(StandardHost.java:772)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1203)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1193)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at org.apache.tomcat.util.threads.InlineExecutorService.execute(InlineExecutorService.java:75)
	at java.base/java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:145)
	at org.apache.catalina.core.ContainerBase.startInternal(ContainerBase.java:749)
	at org.apache.catalina.core.StandardEngine.startInternal(StandardEngine.java:203)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.core.StandardService.startInternal(StandardService.java:415)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.core.StandardServer.startInternal(StandardServer.java:870)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.startup.Tomcat.start(Tomcat.java:437)
	at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.initialize(TomcatWebServer.java:128)
	... 16 common frames omitted
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'customUserDetailsServiceImpl' defined in file [C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes\com\personneltrackingsystem\service\impl\CustomUserDetailsServiceImpl.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'userRepository' defined in com.personneltrackingsystem.repository.UserRepository defined in @EnableJpaRepositories declared on JpaRepositoriesRegistrar.EnableJpaRepositoriesConfiguration: Cannot resolve reference to bean 'jpaSharedEM_entityManagerFactory' while setting bean property 'entityManager'
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:795)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:237)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1375)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1212)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1443)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1353)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:904)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:782)
	... 57 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'userRepository' defined in com.personneltrackingsystem.repository.UserRepository defined in @EnableJpaRepositories declared on JpaRepositoriesRegistrar.EnableJpaRepositoriesConfiguration: Cannot resolve reference to bean 'jpaSharedEM_entityManagerFactory' while setting bean property 'entityManager'
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:377)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:135)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyPropertyValues(AbstractAutowireCapableBeanFactory.java:1705)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1454)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:599)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1443)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1353)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:904)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:782)
	... 71 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'jpaSharedEM_entityManagerFactory': Cannot resolve reference to bean 'entityManagerFactory' while setting constructor argument
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:377)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:135)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveConstructorArguments(ConstructorResolver.java:682)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:509)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1355)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1185)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:365)
	... 85 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'liquibase' defined in class path resource [org/springframework/boot/autoconfigure/liquibase/LiquibaseAutoConfiguration$LiquibaseConfiguration.class]: liquibase.exception.CommandExecutionException: liquibase.exception.ValidationFailedException: Validation Failed:
     2 changesets check sum
          db/changelog/changes/002-insert-initial-data.yaml::4::liquibase was: 9:3f6144c3432e02fea49aea35f2e2e7c8 but is now: 9:d4666daa1e1cf3996a174df0e04044af
          db/changelog/changes/002-insert-initial-data.yaml::5::liquibase was: 9:e45b97d889e105e6b966e459a8d76408 but is now: 9:0a4c30cedd8a2c460ddf8b2e543dbefa

	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1806)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:313)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:365)
	... 97 common frames omitted
Caused by: liquibase.exception.UnexpectedLiquibaseException: liquibase.exception.CommandExecutionException: liquibase.exception.ValidationFailedException: Validation Failed:
     2 changesets check sum
          db/changelog/changes/002-insert-initial-data.yaml::4::liquibase was: 9:3f6144c3432e02fea49aea35f2e2e7c8 but is now: 9:d4666daa1e1cf3996a174df0e04044af
          db/changelog/changes/002-insert-initial-data.yaml::5::liquibase was: 9:e45b97d889e105e6b966e459a8d76408 but is now: 9:0a4c30cedd8a2c460ddf8b2e543dbefa

	at liquibase.integration.spring.SpringLiquibase.afterPropertiesSet(SpringLiquibase.java:267)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1853)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1802)
	... 106 common frames omitted
Caused by: liquibase.exception.CommandExecutionException: liquibase.exception.ValidationFailedException: Validation Failed:
     2 changesets check sum
          db/changelog/changes/002-insert-initial-data.yaml::4::liquibase was: 9:3f6144c3432e02fea49aea35f2e2e7c8 but is now: 9:d4666daa1e1cf3996a174df0e04044af
          db/changelog/changes/002-insert-initial-data.yaml::5::liquibase was: 9:e45b97d889e105e6b966e459a8d76408 but is now: 9:0a4c30cedd8a2c460ddf8b2e543dbefa

	at liquibase.command.CommandScope.execute(CommandScope.java:258)
	at liquibase.Liquibase.lambda$update$0(Liquibase.java:216)
	at liquibase.Scope.lambda$child$0(Scope.java:191)
	at liquibase.Scope.child(Scope.java:200)
	at liquibase.Scope.child(Scope.java:190)
	at liquibase.Scope.child(Scope.java:169)
	at liquibase.Liquibase.runInScope(Liquibase.java:1329)
	at liquibase.Liquibase.update(Liquibase.java:205)
	at liquibase.Liquibase.update(Liquibase.java:188)
	at liquibase.integration.spring.SpringLiquibase.performUpdate(SpringLiquibase.java:305)
	at liquibase.integration.spring.SpringLiquibase.lambda$afterPropertiesSet$0(SpringLiquibase.java:257)
	at liquibase.Scope.lambda$child$0(Scope.java:191)
	at liquibase.Scope.child(Scope.java:200)
	at liquibase.Scope.child(Scope.java:190)
	at liquibase.Scope.child(Scope.java:169)
	at liquibase.Scope.child(Scope.java:257)
	at liquibase.integration.spring.SpringLiquibase.afterPropertiesSet(SpringLiquibase.java:250)
	... 108 common frames omitted
Caused by: liquibase.exception.ValidationFailedException: Validation Failed:
     2 changesets check sum
          db/changelog/changes/002-insert-initial-data.yaml::4::liquibase was: 9:3f6144c3432e02fea49aea35f2e2e7c8 but is now: 9:d4666daa1e1cf3996a174df0e04044af
          db/changelog/changes/002-insert-initial-data.yaml::5::liquibase was: 9:e45b97d889e105e6b966e459a8d76408 but is now: 9:0a4c30cedd8a2c460ddf8b2e543dbefa

	at liquibase.changelog.DatabaseChangeLog.validate(DatabaseChangeLog.java:398)
	at liquibase.command.core.helpers.DatabaseChangelogCommandStep.run(DatabaseChangelogCommandStep.java:92)
	at liquibase.command.CommandScope.execute(CommandScope.java:220)
	... 124 common frames omitted
2025-06-20 06:17:26 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - Starting PersonelTrackingSystemApplication using Java 21.0.7 with PID 6148 (C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes started by Lenovo in C:\Users\Lenovo\Desktop\personnel-tracking-system)
2025-06-20 06:17:26 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - No active profile set, falling back to 1 default profile: "default"
2025-06-20 06:17:28 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-20 06:17:28 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-20 06:17:28 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.30]
2025-06-20 06:17:28 [restartedMain] INFO  o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-20 06:17:29 [restartedMain] WARN  c.h.i.impl.HazelcastInstanceFactory - Hazelcast is starting in a Java modular environment (Java 9 and newer) but without proper access to required Java packages. Use additional Java arguments to provide Hazelcast access to Java internal API. The internal API access is used to get the best performance results. Arguments to be used:
 --add-modules java.se --add-exports java.base/jdk.internal.ref=ALL-UNNAMED --add-opens java.base/java.lang=ALL-UNNAMED --add-opens java.base/sun.nio.ch=ALL-UNNAMED --add-opens java.management/sun.management=ALL-UNNAMED --add-opens jdk.management/com.sun.management.internal=ALL-UNNAMED
2025-06-20 06:17:29 [restartedMain] INFO  c.hazelcast.instance.AddressPicker - [LOCAL] [pts-cluster] [5.4.0] Interfaces is disabled, trying to pick one address from TCP-IP config addresses: [127.0.0.1]
2025-06-20 06:17:29 [restartedMain] INFO  com.hazelcast.system.logo - [127.0.0.1]:5701 [pts-cluster] [5.4.0] 
	o    o     o     o---o   o--o o      o---o     o     o----o o--o--o
	|    |    / \       /         |     /         / \    |         |   
	o----o       o     o   o----o |    o             o   o----o    |   
	|    |  *     \   /           |     \       *     \       |    |   
	o    o *       o o---o   o--o o----o o---o *       o o----o    o   
2025-06-20 06:17:29 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Copyright (c) 2008-2024, Hazelcast, Inc. All Rights Reserved.
2025-06-20 06:17:29 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Hazelcast Platform 5.4.0 (20240415) starting at [127.0.0.1]:5701
2025-06-20 06:17:29 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Cluster name: pts-cluster
2025-06-20 06:17:29 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Integrity Checker is disabled. Fail-fast on corrupted executables will not be performed. For more information, see the documentation for Integrity Checker.
2025-06-20 06:17:29 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] The Jet engine is disabled.
To enable the Jet engine on the members, do one of the following:
  - Change member config using Java API: config.getJetConfig().setEnabled(true)
  - Change XML/YAML configuration property: Set hazelcast.jet.enabled to true
  - Add system property: -Dhz.jet.enabled=true (for Hazelcast embedded, works only when loading config via Config.load)
  - Add environment variable: HZ_JET_ENABLED=true (recommended when running container image. For Hazelcast embedded, works only when loading config via Config.load)
2025-06-20 06:17:29 [restartedMain] INFO  com.hazelcast.system.security - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Enable DEBUG/FINE log level for log category com.hazelcast.system.security  or use -Dhazelcast.security.recommendations system property to see security recommendations and the status of current config.
2025-06-20 06:17:29 [restartedMain] INFO  com.hazelcast.instance.impl.Node - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Using TCP/IP discovery
2025-06-20 06:17:29 [restartedMain] WARN  com.hazelcast.cp.CPSubsystem - [127.0.0.1]:5701 [pts-cluster] [5.4.0] CP Subsystem is not enabled. CP data structures will operate in UNSAFE mode! Please note that UNSAFE mode will not provide strong consistency guarantees.
2025-06-20 06:17:30 [restartedMain] INFO  c.h.internal.diagnostics.Diagnostics - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Diagnostics disabled. To enable add -Dhazelcast.diagnostics.enabled=true to the JVM arguments.
2025-06-20 06:17:30 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is STARTING
2025-06-20 06:17:30 [hz.personnel-tracking-system.cached.thread-3] INFO  c.h.i.cluster.impl.TcpIpJoiner - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5703 is added to the blacklist.
2025-06-20 06:17:30 [hz.personnel-tracking-system.cached.thread-2] INFO  c.h.i.cluster.impl.TcpIpJoiner - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5702 is added to the blacklist.
2025-06-20 06:17:31 [restartedMain] INFO  c.h.internal.cluster.ClusterService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] 

Members {size:1, ver:1} [
	Member [127.0.0.1]:5701 - 3ffd0447-0ec8-4445-8a9b-a7e04d25d8ff this
]

2025-06-20 06:17:31 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is STARTED
2025-06-20 06:17:31 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-1 - Starting...
2025-06-20 06:17:31 [restartedMain] INFO  com.zaxxer.hikari.pool.HikariPool - HikariPool-1 - Added connection org.postgresql.jdbc.PgConnection@62992aac
2025-06-20 06:17:31 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-1 - Start completed.
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Creating database history table with name: dbpersonel.databasechangelog
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Reading from dbpersonel.databasechangelog
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.lockservice - Successfully acquired change log lock
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.command - Using deploymentId: 0389452391
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Reading from dbpersonel.databasechangelog
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::1::liquibase
2025-06-20 06:17:32 [restartedMain] WARN  liquibase.executor - "dbpersonel" emas zaten mevcut, atlanyor
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::1::liquibase ran successfully in 17ms
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::2::liquibase
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::2::liquibase ran successfully in 99ms
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::3::liquibase
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Table personel_type created
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::3::liquibase ran successfully in 9ms
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::4::liquibase
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Table building created
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::4::liquibase ran successfully in 6ms
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::5::liquibase
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Table floor created
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::5::liquibase ran successfully in 8ms
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::6::liquibase
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Table personel created
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::6::liquibase ran successfully in 7ms
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::7::liquibase
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Table unit created
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::7::liquibase ran successfully in 10ms
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::8::liquibase
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Table personel_unit created
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::8::liquibase ran successfully in 9ms
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::9::liquibase
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Table gate created
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::9::liquibase ran successfully in 8ms
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::10::liquibase
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Table turnstile created
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::10::liquibase ran successfully in 8ms
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::11::liquibase
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Table turnstile_registration_log created
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::11::liquibase ran successfully in 8ms
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::12::liquibase
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Table working_hours created
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::12::liquibase ran successfully in 10ms
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::13::liquibase
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Table salary created
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::13::liquibase ran successfully in 14ms
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::14::liquibase
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Table user created
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::14::liquibase ran successfully in 15ms
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::15::liquibase
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Table permission created
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::15::liquibase ran successfully in 14ms
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::16::liquibase
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Table role_permission created
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::16::liquibase ran successfully in 8ms
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::17::liquibase
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::17::liquibase ran successfully in 73ms
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::1::liquibase
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - New row inserted into personel_type
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - New row inserted into personel_type
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - New row inserted into personel_type
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::1::liquibase ran successfully in 6ms
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::2::liquibase
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - New row inserted into building
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - New row inserted into building
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - New row inserted into building
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::2::liquibase ran successfully in 10ms
2025-06-20 06:17:32 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::3::liquibase
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.changelog - Data loaded from 'db/changelog/data/floor-data.csv' into table 'floor'
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::3::liquibase ran successfully in 50ms
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::4::liquibase
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.changelog - New row inserted into personel
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.changelog - New row inserted into personel
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.changelog - New row inserted into personel
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.changelog - New row inserted into personel
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.changelog - New row inserted into personel
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.changelog - New row inserted into personel
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.changelog - New row inserted into personel
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.changelog - New row inserted into personel
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::4::liquibase ran successfully in 26ms
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::5::liquibase
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.changelog - New row inserted into unit
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.changelog - New row inserted into unit
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.changelog - New row inserted into unit
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.changelog - New row inserted into unit
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.changelog - New row inserted into unit
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::5::liquibase ran successfully in 19ms
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::6::liquibase
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.changelog - New row inserted into personel_unit
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.changelog - New row inserted into personel_unit
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.changelog - New row inserted into personel_unit
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.changelog - New row inserted into personel_unit
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::6::liquibase ran successfully in 15ms
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::7::liquibase
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.changelog - Data loaded from 'db/changelog/data/gate-data.csv' into table 'gate'
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::7::liquibase ran successfully in 16ms
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::8::liquibase
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.changelog - Data loaded from 'db/changelog/data/turnstile-data.csv' into table 'turnstile'
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::8::liquibase ran successfully in 13ms
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::9::liquibase
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.changelog - New row inserted into working_hours
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.changelog - New row inserted into working_hours
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.changelog - New row inserted into working_hours
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::9::liquibase ran successfully in 6ms
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::10::liquibase
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.changelog - New row inserted into permission
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.changelog - New row inserted into permission
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.changelog - New row inserted into permission
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.changelog - New row inserted into permission
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.changelog - New row inserted into permission
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::10::liquibase ran successfully in 9ms
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::11::liquibase
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.changelog - New row inserted into role_permission
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.changelog - New row inserted into role_permission
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.changelog - New row inserted into role_permission
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.changelog - New row inserted into role_permission
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.changelog - New row inserted into role_permission
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::11::liquibase ran successfully in 13ms
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.util - UPDATE SUMMARY
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.util - Run:                         28
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.util - Previously run:               0
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.util - Filtered out:                 0
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.util - -------------------------------
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.util - Total change sets:           28
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.util - Update summary generated
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.command - Update command completed successfully.
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.ui - Liquibase: Update has been successful. Rows affected: 132
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.lockservice - Successfully released change log lock
2025-06-20 06:17:33 [restartedMain] INFO  liquibase.command - Command execution complete
2025-06-20 06:17:33 [restartedMain] WARN  org.hibernate.orm.deprecation - HHH90000025: PostgreSQLDialect does not need to be specified explicitly using 'hibernate.dialect' (remove the property setting and it will be selected by default)
2025-06-20 06:17:35 [restartedMain] WARN  o.s.s.c.a.a.c.InitializeUserDetailsBeanManagerConfigurer$InitializeUserDetailsManagerConfigurer - Global AuthenticationManager configured with an AuthenticationProvider bean. UserDetailsService beans will not be used for username/password login. Consider removing the AuthenticationProvider bean. Alternatively, consider using the UserDetailsService in a manually instantiated DaoAuthenticationProvider.
2025-06-20 06:17:36 [restartedMain] INFO  c.p.s.impl.PersonelTypeServiceImpl - Checking for default personnel types...
2025-06-20 06:17:36 [restartedMain] INFO  c.p.s.impl.PersonelTypeServiceImpl - Personnel types already exist in the database.
2025-06-20 06:17:36 [restartedMain] WARN  o.s.b.a.o.j.JpaBaseConfiguration$JpaWebConfiguration - spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
2025-06-20 06:17:37 [restartedMain] INFO  o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = personneltrackingsystem-admin-0
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-20 06:17:37 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 06:17:37 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 06:17:37 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750389457599
2025-06-20 06:17:37 [kafka-admin-client-thread | personneltrackingsystem-admin-0] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for personneltrackingsystem-admin-0 unregistered
2025-06-20 06:17:37 [kafka-admin-client-thread | personneltrackingsystem-admin-0] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-20 06:17:37 [kafka-admin-client-thread | personneltrackingsystem-admin-0] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-20 06:17:37 [kafka-admin-client-thread | personneltrackingsystem-admin-0] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-20 06:17:37 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-20 06:17:38 [restartedMain] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pts-group-reset-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pts-group-reset
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-06-20 06:17:38 [restartedMain] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 06:17:38 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 06:17:38 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 06:17:38 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750389458147
2025-06-20 06:17:38 [restartedMain] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Subscribed to topic(s): turnstile-request
2025-06-20 06:17:38 [restartedMain] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pts-group-reset-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pts-group-reset
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-06-20 06:17:38 [restartedMain] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 06:17:38 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 06:17:38 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 06:17:38 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 06:17:38 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750389458172
2025-06-20 06:17:38 [restartedMain] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Subscribed to topic(s): email-notification
2025-06-20 06:17:38 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2025-06-20 06:17:38 [restartedMain] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pts-group-reset-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pts-group-reset
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-06-20 06:17:38 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:17:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 06:17:38 [restartedMain] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 06:17:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2025-06-20 06:17:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:17:38 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 06:17:38 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 06:17:38 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750389458204
2025-06-20 06:17:38 [restartedMain] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Subscribed to topic(s): turnstile-passage
2025-06-20 06:17:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Request joining group due to: need to re-join with the given member-id: consumer-pts-group-reset-2-50f1db06-ebb8-429f-b35c-78665e36d3cd
2025-06-20 06:17:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:17:38 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Request joining group due to: need to re-join with the given member-id: consumer-pts-group-reset-1-6dd4dde9-61e6-4247-b30d-32487585fdc7
2025-06-20 06:17:38 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:17:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=4, memberId='consumer-pts-group-reset-2-50f1db06-ebb8-429f-b35c-78665e36d3cd', protocol='range'}
2025-06-20 06:17:38 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 06:17:38 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2025-06-20 06:17:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Finished assignment for group at generation 4: {consumer-pts-group-reset-2-50f1db06-ebb8-429f-b35c-78665e36d3cd=Assignment(partitions=[email-notification-0])}
2025-06-20 06:17:38 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:17:38 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Request joining group due to: need to re-join with the given member-id: consumer-pts-group-reset-3-9155a3e9-9c08-4dbf-a590-38bd41090adf
2025-06-20 06:17:38 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:17:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] SyncGroup failed: The group began another rebalance. Need to re-join the group. Sent generation was Generation{generationId=4, memberId='consumer-pts-group-reset-2-50f1db06-ebb8-429f-b35c-78665e36d3cd', protocol='range'}
2025-06-20 06:17:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Request joining group due to: rebalance failed due to 'The group is rebalancing, so a rejoin is needed.' (RebalanceInProgressException)
2025-06-20 06:17:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:17:38 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=5, memberId='consumer-pts-group-reset-1-6dd4dde9-61e6-4247-b30d-32487585fdc7', protocol='range'}
2025-06-20 06:17:38 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=5, memberId='consumer-pts-group-reset-3-9155a3e9-9c08-4dbf-a590-38bd41090adf', protocol='range'}
2025-06-20 06:17:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=5, memberId='consumer-pts-group-reset-2-50f1db06-ebb8-429f-b35c-78665e36d3cd', protocol='range'}
2025-06-20 06:17:38 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - Started PersonelTrackingSystemApplication in 12.174 seconds (process running for 12.738)
2025-06-20 06:17:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Finished assignment for group at generation 5: {consumer-pts-group-reset-2-50f1db06-ebb8-429f-b35c-78665e36d3cd=Assignment(partitions=[email-notification-0]), consumer-pts-group-reset-1-6dd4dde9-61e6-4247-b30d-32487585fdc7=Assignment(partitions=[turnstile-request-0]), consumer-pts-group-reset-3-9155a3e9-9c08-4dbf-a590-38bd41090adf=Assignment(partitions=[turnstile-passage-0])}
2025-06-20 06:17:38 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=5, memberId='consumer-pts-group-reset-3-9155a3e9-9c08-4dbf-a590-38bd41090adf', protocol='range'}
2025-06-20 06:17:38 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=5, memberId='consumer-pts-group-reset-1-6dd4dde9-61e6-4247-b30d-32487585fdc7', protocol='range'}
2025-06-20 06:17:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=5, memberId='consumer-pts-group-reset-2-50f1db06-ebb8-429f-b35c-78665e36d3cd', protocol='range'}
2025-06-20 06:17:38 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[turnstile-passage-0])
2025-06-20 06:17:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[email-notification-0])
2025-06-20 06:17:38 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[turnstile-request-0])
2025-06-20 06:17:38 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Adding newly assigned partitions: turnstile-passage-0
2025-06-20 06:17:38 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Adding newly assigned partitions: turnstile-request-0
2025-06-20 06:17:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Adding newly assigned partitions: email-notification-0
2025-06-20 06:17:38 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition turnstile-request-0 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 06:17:38 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition turnstile-passage-0 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 06:17:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition email-notification-0 to the committed offset FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 06:18:08 [http-nio-8080-exec-1] INFO  o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-06-20 06:18:33 [http-nio-8080-exec-3] INFO  c.p.c.impl.TurnstileControllerImpl - Turnstile passage request received: DtoTurnstilePassageFullRequest(wantedToEnterTurnstileId=1, personelId=1, operationType=IN, operationTimeStr=09:17:35)
2025-06-20 06:18:33 [http-nio-8080-exec-3] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - No previous operation found for personel 1 on turnstile 1
2025-06-20 06:18:33 [http-nio-8080-exec-3] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Turnstile passage validation - Personel: 1, Turnstile: 1, Requested Operation: IN, Last Operation: null
2025-06-20 06:18:33 [http-nio-8080-exec-3] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Operation validated successfully: IN (Personel: 1, Turnstile: 1)
2025-06-20 06:18:33 [http-nio-8080-exec-3] INFO  c.p.s.k.i.KafkaProducerServiceImpl - Sending turnstile request event to Kafka: TurnstileRequestEvent(wantedToEnterTurnstileId=1, personelId=1, operationType=IN, operationTimeStr=09:17:35)
2025-06-20 06:18:33 [http-nio-8080-exec-3] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = personneltrackingsystem-producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 3
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.springframework.kafka.support.serializer.JsonSerializer

2025-06-20 06:18:33 [http-nio-8080-exec-3] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 06:18:33 [http-nio-8080-exec-3] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=personneltrackingsystem-producer-1] Instantiated an idempotent producer.
2025-06-20 06:18:33 [http-nio-8080-exec-3] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 06:18:33 [http-nio-8080-exec-3] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 06:18:33 [http-nio-8080-exec-3] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750389513592
2025-06-20 06:18:33 [kafka-producer-network-thread | personneltrackingsystem-producer-1] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=personneltrackingsystem-producer-1] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 06:18:33 [kafka-producer-network-thread | personneltrackingsystem-producer-1] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=personneltrackingsystem-producer-1] ProducerId set to 0 with epoch 0
2025-06-20 06:18:33 [kafka-producer-network-thread | personneltrackingsystem-producer-1] INFO  c.p.s.k.i.KafkaProducerServiceImpl - Turnstile request event sent successfully to topic: turnstile-request, partition: 0, offset: 0
2025-06-20 06:18:33 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.k.i.TurnstileRequestConsumerServiceImpl - Received turnstile request event: TurnstileRequestEvent(wantedToEnterTurnstileId=1, personelId=1, operationType=IN, operationTimeStr=09:17:35)
2025-06-20 06:18:34 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.c.i.PersonelCacheServiceImpl - Personnel not found in cache with ID: 1
2025-06-20 06:18:34 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.c.i.PersonelCacheServiceImpl - Personnel cached successfully with ID: 1
2025-06-20 06:18:34 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Saved turnstile registration log: Personel 1 - Turnstile 1 - Operation IN
2025-06-20 06:18:34 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.k.i.KafkaProducerServiceImpl - Sending turnstile passage event to Kafka: TurnstilePassageEvent(personelId=1, personelName=Arif zcan, personelEmail=zcanarif@gmail.com, turnstileId=1, turnstileName=Turnstile 1, passageTime=2025-06-20T09:17:35, operationType=IN, recipientEmail=talha.unal@gelirler.gov.tr, recipientName=Talha nal, isAdminNotification=true, isLateArrival=true, minutesLate=17)
2025-06-20 06:18:34 [kafka-producer-network-thread | personneltrackingsystem-producer-1] INFO  c.p.s.k.i.KafkaProducerServiceImpl - Turnstile passage event sent successfully to topic: turnstile-passage, partition: 0, offset: 0
2025-06-20 06:18:34 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.k.i.TurnstileRequestConsumerServiceImpl - Turnstile request event processed successfully
2025-06-20 06:18:34 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  c.p.s.k.i.TurnstilePassageConsumerServiceImpl - Received turnstile passage event: TurnstilePassageEvent(personelId=1, personelName=Arif zcan, personelEmail=zcanarif@gmail.com, turnstileId=1, turnstileName=Turnstile 1, passageTime=2025-06-20T09:17:35, operationType=IN, recipientEmail=talha.unal@gelirler.gov.tr, recipientName=Talha nal, isAdminNotification=true, isLateArrival=true, minutesLate=17)
2025-06-20 06:18:34 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  c.p.s.k.i.KafkaProducerServiceImpl - Sending email event to Kafka: EmailEvent(recipientEmail=talha.unal@gelirler.gov.tr, recipientName=Talha nal, subject=Late Arrival Notification - Arif zcan, message=Dear Talha nal,

This is to inform you that Arif zcan has arrived late to work today.

Details:
- Personnel Name: Arif zcan
- Arrival Time: 2025-06-20 09:17:35
- Minutes Late: 17 minutes
- Entrance: Turnstile 1


This is an automated message from the Personnel Tracking System.
, timestamp=2025-06-20T06:18:34.523074600)
2025-06-20 06:18:34 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  c.p.s.k.i.TurnstilePassageConsumerServiceImpl - Turnstile passage event processed successfully
2025-06-20 06:18:34 [kafka-producer-network-thread | personneltrackingsystem-producer-1] INFO  c.p.s.k.i.KafkaProducerServiceImpl - Email event sent successfully to topic: email-notification, partition: 0, offset: 0
2025-06-20 06:18:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  c.p.s.k.i.EmailConsumerServiceImpl - Received email event: EmailEvent(recipientEmail=talha.unal@gelirler.gov.tr, recipientName=Talha nal, subject=Late Arrival Notification - Arif zcan, message=Dear Talha nal,

This is to inform you that Arif zcan has arrived late to work today.

Details:
- Personnel Name: Arif zcan
- Arrival Time: 2025-06-20 09:17:35
- Minutes Late: 17 minutes
- Entrance: Turnstile 1


This is an automated message from the Personnel Tracking System.
, timestamp=2025-06-20T06:18:34.523074600)
2025-06-20 06:18:36 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  c.p.s.k.i.EmailConsumerServiceImpl - Email sent successfully to talha.unal@gelirler.gov.tr
2025-06-20 06:18:36 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  c.p.s.k.i.EmailConsumerServiceImpl - Email event processed successfully
2025-06-20 06:21:06 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Revoke previously assigned partitions turnstile-request-0
2025-06-20 06:21:06 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Revoke previously assigned partitions email-notification-0
2025-06-20 06:21:06 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Revoke previously assigned partitions turnstile-passage-0
2025-06-20 06:21:06 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Member consumer-pts-group-reset-2-50f1db06-ebb8-429f-b35c-78665e36d3cd sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2025-06-20 06:21:06 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Member consumer-pts-group-reset-1-6dd4dde9-61e6-4247-b30d-32487585fdc7 sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2025-06-20 06:21:06 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Member consumer-pts-group-reset-3-9155a3e9-9c08-4dbf-a590-38bd41090adf sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2025-06-20 06:21:06 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-20 06:21:06 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-20 06:21:06 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-20 06:21:06 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Request joining group due to: consumer pro-actively leaving the group
2025-06-20 06:21:06 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Request joining group due to: consumer pro-actively leaving the group
2025-06-20 06:21:06 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Request joining group due to: consumer pro-actively leaving the group
2025-06-20 06:21:06 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Unsubscribed all topics or patterns and assigned partitions
2025-06-20 06:21:06 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Unsubscribed all topics or patterns and assigned partitions
2025-06-20 06:21:06 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Unsubscribed all topics or patterns and assigned partitions
2025-06-20 06:21:06 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-20 06:21:06 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-20 06:21:06 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-20 06:21:06 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Request joining group due to: consumer pro-actively leaving the group
2025-06-20 06:21:06 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Request joining group due to: consumer pro-actively leaving the group
2025-06-20 06:21:06 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Request joining group due to: consumer pro-actively leaving the group
2025-06-20 06:21:07 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-20 06:21:07 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-20 06:21:07 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-20 06:21:07 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-20 06:21:07 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-pts-group-reset-3 unregistered
2025-06-20 06:21:07 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-20 06:21:07 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-20 06:21:07 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-20 06:21:07 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-20 06:21:07 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-pts-group-reset-1 unregistered
2025-06-20 06:21:07 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-20 06:21:07 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-20 06:21:07 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-20 06:21:07 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-20 06:21:07 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-pts-group-reset-2 unregistered
2025-06-20 06:21:07 [Thread-5] INFO  o.a.coyote.http11.Http11NioProtocol - Stopping ProtocolHandler ["http-nio-8080"]
2025-06-20 06:21:07 [Thread-5] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=personneltrackingsystem-producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2025-06-20 06:21:07 [Thread-5] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-20 06:21:07 [Thread-5] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-20 06:21:07 [Thread-5] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-20 06:21:07 [Thread-5] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-20 06:21:07 [Thread-5] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.producer for personneltrackingsystem-producer-1 unregistered
2025-06-20 06:21:07 [Thread-5] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-1 - Shutdown initiated...
2025-06-20 06:21:07 [Thread-5] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-1 - Shutdown completed.
2025-06-20 06:21:07 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - Starting PersonelTrackingSystemApplication using Java 21.0.7 with PID 6148 (C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes started by Lenovo in C:\Users\Lenovo\Desktop\personnel-tracking-system)
2025-06-20 06:21:07 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - No active profile set, falling back to 1 default profile: "default"
2025-06-20 06:21:08 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-20 06:21:08 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-20 06:21:08 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.30]
2025-06-20 06:21:08 [restartedMain] INFO  o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-20 06:21:08 [restartedMain] INFO  c.hazelcast.instance.AddressPicker - [LOCAL] [pts-cluster] [5.4.0] Interfaces is disabled, trying to pick one address from TCP-IP config addresses: [127.0.0.1]
2025-06-20 06:21:08 [restartedMain] INFO  com.hazelcast.system.logo - [127.0.0.1]:5701 [pts-cluster] [5.4.0] 
	o    o     o     o---o   o--o o      o---o     o     o----o o--o--o
	|    |    / \       /         |     /         / \    |         |   
	o----o       o     o   o----o |    o             o   o----o    |   
	|    |  *     \   /           |     \       *     \       |    |   
	o    o *       o o---o   o--o o----o o---o *       o o----o    o   
2025-06-20 06:21:08 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Copyright (c) 2008-2024, Hazelcast, Inc. All Rights Reserved.
2025-06-20 06:21:08 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Hazelcast Platform 5.4.0 (20240415) starting at [127.0.0.1]:5701
2025-06-20 06:21:08 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Cluster name: pts-cluster
2025-06-20 06:21:08 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Integrity Checker is disabled. Fail-fast on corrupted executables will not be performed. For more information, see the documentation for Integrity Checker.
2025-06-20 06:21:08 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] The Jet engine is disabled.
To enable the Jet engine on the members, do one of the following:
  - Change member config using Java API: config.getJetConfig().setEnabled(true)
  - Change XML/YAML configuration property: Set hazelcast.jet.enabled to true
  - Add system property: -Dhz.jet.enabled=true (for Hazelcast embedded, works only when loading config via Config.load)
  - Add environment variable: HZ_JET_ENABLED=true (recommended when running container image. For Hazelcast embedded, works only when loading config via Config.load)
2025-06-20 06:21:08 [restartedMain] INFO  com.hazelcast.system.security - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Enable DEBUG/FINE log level for log category com.hazelcast.system.security  or use -Dhazelcast.security.recommendations system property to see security recommendations and the status of current config.
2025-06-20 06:21:08 [restartedMain] INFO  com.hazelcast.instance.impl.Node - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Using TCP/IP discovery
2025-06-20 06:21:08 [restartedMain] WARN  com.hazelcast.cp.CPSubsystem - [127.0.0.1]:5701 [pts-cluster] [5.4.0] CP Subsystem is not enabled. CP data structures will operate in UNSAFE mode! Please note that UNSAFE mode will not provide strong consistency guarantees.
2025-06-20 06:21:08 [restartedMain] INFO  c.h.internal.diagnostics.Diagnostics - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Diagnostics disabled. To enable add -Dhazelcast.diagnostics.enabled=true to the JVM arguments.
2025-06-20 06:21:08 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is STARTING
2025-06-20 06:21:08 [hz.personnel-tracking-system.cached.thread-1] INFO  c.h.i.cluster.impl.TcpIpJoiner - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5703 is added to the blacklist.
2025-06-20 06:21:08 [hz.personnel-tracking-system.cached.thread-3] INFO  c.h.i.cluster.impl.TcpIpJoiner - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5702 is added to the blacklist.
2025-06-20 06:21:09 [restartedMain] INFO  c.h.internal.cluster.ClusterService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] 

Members {size:1, ver:1} [
	Member [127.0.0.1]:5701 - e02b9472-738a-4e54-8e51-de9510c9ef39 this
]

2025-06-20 06:21:09 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is STARTED
2025-06-20 06:21:09 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-2 - Starting...
2025-06-20 06:21:09 [restartedMain] INFO  com.zaxxer.hikari.pool.HikariPool - HikariPool-2 - Added connection org.postgresql.jdbc.PgConnection@2a19005a
2025-06-20 06:21:09 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-2 - Start completed.
2025-06-20 06:21:09 [restartedMain] INFO  liquibase.changelog - Reading from dbpersonel.databasechangelog
2025-06-20 06:21:09 [restartedMain] INFO  liquibase.changelog - Change failed validation!
2025-06-20 06:21:09 [restartedMain] INFO  liquibase.command - Logging exception.
2025-06-20 06:21:09 [restartedMain] INFO  liquibase.ui - ERROR: Exception Details
2025-06-20 06:21:09 [restartedMain] INFO  liquibase.ui - ERROR: Exception Primary Class:  ValidationFailedException
2025-06-20 06:21:09 [restartedMain] INFO  liquibase.ui - ERROR: Exception Primary Reason:  Validation Failed:
     1 changesets check sum
          db/changelog/changes/002-insert-initial-data.yaml::4::liquibase was: 9:d4666daa1e1cf3996a174df0e04044af but is now: 9:4afce42b7d8b7a4616d1482f6afedbf5

2025-06-20 06:21:09 [restartedMain] INFO  liquibase.ui - ERROR: Exception Primary Source:  4.29.2
2025-06-20 06:21:09 [restartedMain] INFO  liquibase.command - Command execution complete
2025-06-20 06:21:09 [restartedMain] ERROR o.s.b.w.e.tomcat.TomcatStarter - Error starting Tomcat context. Exception: org.springframework.beans.factory.UnsatisfiedDependencyException. Message: Error creating bean with name 'jwtAuthenticationFilter' defined in file [C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes\com\personneltrackingsystem\filter\JwtAuthenticationFilter.class]: Unsatisfied dependency expressed through constructor parameter 1: Error creating bean with name 'customUserDetailsServiceImpl' defined in file [C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes\com\personneltrackingsystem\service\impl\CustomUserDetailsServiceImpl.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'userRepository' defined in com.personneltrackingsystem.repository.UserRepository defined in @EnableJpaRepositories declared on JpaRepositoriesRegistrar.EnableJpaRepositoriesConfiguration: Cannot resolve reference to bean 'jpaSharedEM_entityManagerFactory' while setting bean property 'entityManager'
2025-06-20 06:21:09 [restartedMain] INFO  o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-06-20 06:21:09 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.event-6] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:435)
 com.hazelcast.internal.util.executor.StripedExecutor$Worker.run(StripedExecutor.java:227)
2025-06-20 06:21:09 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.event-7] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:435)
 com.hazelcast.internal.util.executor.StripedExecutor$Worker.run(StripedExecutor.java:227)
2025-06-20 06:21:09 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.event-8] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:435)
 com.hazelcast.internal.util.executor.StripedExecutor$Worker.run(StripedExecutor.java:227)
2025-06-20 06:21:09 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.event-9] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:435)
 com.hazelcast.internal.util.executor.StripedExecutor$Worker.run(StripedExecutor.java:227)
2025-06-20 06:21:09 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.event-10] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:435)
 com.hazelcast.internal.util.executor.StripedExecutor$Worker.run(StripedExecutor.java:227)
2025-06-20 06:21:09 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.MetricsRegistry.thread-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:269)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1763)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 06:21:09 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.MetricsRegistry.thread-2] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1177)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 06:21:09 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.migration] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:269)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1763)
 java.base/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:460)
 com.hazelcast.internal.partition.impl.MigrationQueue.poll(MigrationQueue.java:48)
 com.hazelcast.internal.partition.impl.MigrationThread.doRun(MigrationThread.java:91)
 com.hazelcast.internal.partition.impl.MigrationThread.run(MigrationThread.java:66)
2025-06-20 06:21:09 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.InvocationMonitorThread] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:269)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1763)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 06:21:09 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.SlowOperationDetectorThread] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/java.lang.Thread.sleep0(Native Method)
 java.base/java.lang.Thread.sleep(Thread.java:558)
 java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
 com.hazelcast.spi.impl.operationexecutor.slowoperationdetector.SlowOperationDetector$DetectorThread.sleepInterval(SlowOperationDetector.java:280)
 com.hazelcast.spi.impl.operationexecutor.slowoperationdetector.SlowOperationDetector$DetectorThread.run(SlowOperationDetector.java:153)
2025-06-20 06:21:09 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-in-0] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:142)
 com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:292)
 com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249)
 com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111)
2025-06-20 06:21:09 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-in-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:142)
 com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:292)
 com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249)
 com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111)
2025-06-20 06:21:09 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-in-2] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:142)
 com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:292)
 com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249)
 com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111)
2025-06-20 06:21:09 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-out-0] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:142)
 com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:292)
 com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249)
 com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111)
2025-06-20 06:21:09 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-out-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:142)
 com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:292)
 com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249)
 com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111)
2025-06-20 06:21:09 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-out-2] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:142)
 com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:292)
 com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249)
 com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111)
2025-06-20 06:21:09 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.BalancerThread] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:269)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1763)
 java.base/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:460)
 com.hazelcast.internal.networking.nio.iobalancer.IOBalancerThread.run(IOBalancerThread.java:65)
2025-06-20 06:21:09 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-Acceptor] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:147)
 com.hazelcast.internal.server.tcp.TcpServerAcceptor$AcceptorIOThread.acceptLoop(TcpServerAcceptor.java:186)
 com.hazelcast.internal.server.tcp.TcpServerAcceptor$AcceptorIOThread.run(TcpServerAcceptor.java:172)
2025-06-20 06:21:09 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.TcpServer.thread-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1177)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 06:21:09 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.TcpServer.thread-2] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:269)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1763)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 06:21:09 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.TcpServer.thread-3] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1177)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 06:21:09 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.TcpServer.thread-4] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1177)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 06:21:09 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.HealthMonitor] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/java.lang.Thread.sleep0(Native Method)
 java.base/java.lang.Thread.sleep(Thread.java:558)
 java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
 com.hazelcast.internal.diagnostics.HealthMonitor$HealthMonitorThread.run(HealthMonitor.java:164)
2025-06-20 06:21:09 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [HikariPool-2 housekeeper] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:269)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1763)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 06:21:09 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [HikariPool-2 connection adder] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.Net.poll(Native Method)
 java.base/sun.nio.ch.NioSocketImpl.park(NioSocketImpl.java:191)
 java.base/sun.nio.ch.NioSocketImpl.timedRead(NioSocketImpl.java:280)
 java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:304)
 java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:346)
 java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:796)
 java.base/java.net.Socket$SocketInputStream.read(Socket.java:1099)
 org.postgresql.core.VisibleBufferedInputStream.readMore(VisibleBufferedInputStream.java:162)
 org.postgresql.core.VisibleBufferedInputStream.ensureBytes(VisibleBufferedInputStream.java:129)
 org.postgresql.core.VisibleBufferedInputStream.ensureBytes(VisibleBufferedInputStream.java:114)
 org.postgresql.core.VisibleBufferedInputStream.read(VisibleBufferedInputStream.java:74)
 org.postgresql.core.PGStream.receiveChar(PGStream.java:467)
 org.postgresql.core.v3.ConnectionFactoryImpl.enableSSL(ConnectionFactoryImpl.java:594)
 org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:195)
 org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:262)
 org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)
 org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:273)
 org.postgresql.Driver.makeConnection(Driver.java:446)
 org.postgresql.Driver.connect(Driver.java:298)
 com.zaxxer.hikari.util.DriverDataSource.getConnection(DriverDataSource.java:137)
 com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:360)
 com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:202)
 com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:461)
 com.zaxxer.hikari.pool.HikariPool$PoolEntryCreator.call(HikariPool.java:724)
 com.zaxxer.hikari.pool.HikariPool$PoolEntryCreator.call(HikariPool.java:703)
 java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 06:21:09 [restartedMain] WARN  o.s.b.w.s.c.AnnotationConfigServletWebServerApplicationContext - Exception encountered during context initialization - cancelling refresh attempt: org.springframework.context.ApplicationContextException: Unable to start web server
2025-06-20 06:21:09 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-2 - Shutdown initiated...
2025-06-20 06:21:09 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-2 - Shutdown completed.
2025-06-20 06:21:09 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is SHUTTING_DOWN
2025-06-20 06:21:09 [restartedMain] INFO  com.hazelcast.instance.impl.Node - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Shutting down connection manager...
2025-06-20 06:21:09 [restartedMain] INFO  com.hazelcast.instance.impl.Node - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Shutting down node engine...
2025-06-20 06:21:09 [restartedMain] INFO  c.h.instance.impl.NodeExtension - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Destroying node NodeExtension.
2025-06-20 06:21:09 [restartedMain] INFO  com.hazelcast.instance.impl.Node - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Hazelcast Shutdown is completed in 16 ms.
2025-06-20 06:21:09 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is SHUTDOWN
2025-06-20 06:21:10 [restartedMain] ERROR o.s.boot.SpringApplication - Application run failed
org.springframework.context.ApplicationContextException: Unable to start web server
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.onRefresh(ServletWebServerApplicationContext.java:165)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:619)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:754)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:456)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:335)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1363)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1352)
	at com.personneltrackingsystem.PersonelTrackingSystemApplication.main(PersonelTrackingSystemApplication.java:12)
	at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)
	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
Caused by: org.springframework.boot.web.server.WebServerException: Unable to start embedded Tomcat
	at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.initialize(TomcatWebServer.java:147)
	at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.<init>(TomcatWebServer.java:107)
	at org.springframework.boot.web.embedded.tomcat.TomcatServletWebServerFactory.getTomcatWebServer(TomcatServletWebServerFactory.java:516)
	at org.springframework.boot.web.embedded.tomcat.TomcatServletWebServerFactory.getWebServer(TomcatServletWebServerFactory.java:222)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.createWebServer(ServletWebServerApplicationContext.java:188)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.onRefresh(ServletWebServerApplicationContext.java:162)
	... 11 common frames omitted
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'jwtAuthenticationFilter' defined in file [C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes\com\personneltrackingsystem\filter\JwtAuthenticationFilter.class]: Unsatisfied dependency expressed through constructor parameter 1: Error creating bean with name 'customUserDetailsServiceImpl' defined in file [C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes\com\personneltrackingsystem\service\impl\CustomUserDetailsServiceImpl.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'userRepository' defined in com.personneltrackingsystem.repository.UserRepository defined in @EnableJpaRepositories declared on JpaRepositoriesRegistrar.EnableJpaRepositoriesConfiguration: Cannot resolve reference to bean 'jpaSharedEM_entityManagerFactory' while setting bean property 'entityManager'
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:795)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:237)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1375)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1212)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:205)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.getOrderedBeansOfType(ServletContextInitializerBeans.java:211)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.addAsRegistrationBean(ServletContextInitializerBeans.java:174)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.addAsRegistrationBean(ServletContextInitializerBeans.java:169)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.addAdaptableBeans(ServletContextInitializerBeans.java:154)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.<init>(ServletContextInitializerBeans.java:87)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.getServletContextInitializerBeans(ServletWebServerApplicationContext.java:266)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.selfInitialize(ServletWebServerApplicationContext.java:240)
	at org.springframework.boot.web.embedded.tomcat.TomcatStarter.onStartup(TomcatStarter.java:52)
	at org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:4412)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1203)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1193)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at org.apache.tomcat.util.threads.InlineExecutorService.execute(InlineExecutorService.java:75)
	at java.base/java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:145)
	at org.apache.catalina.core.ContainerBase.startInternal(ContainerBase.java:749)
	at org.apache.catalina.core.StandardHost.startInternal(StandardHost.java:772)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1203)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1193)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at org.apache.tomcat.util.threads.InlineExecutorService.execute(InlineExecutorService.java:75)
	at java.base/java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:145)
	at org.apache.catalina.core.ContainerBase.startInternal(ContainerBase.java:749)
	at org.apache.catalina.core.StandardEngine.startInternal(StandardEngine.java:203)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.core.StandardService.startInternal(StandardService.java:415)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.core.StandardServer.startInternal(StandardServer.java:870)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.startup.Tomcat.start(Tomcat.java:437)
	at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.initialize(TomcatWebServer.java:128)
	... 16 common frames omitted
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'customUserDetailsServiceImpl' defined in file [C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes\com\personneltrackingsystem\service\impl\CustomUserDetailsServiceImpl.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'userRepository' defined in com.personneltrackingsystem.repository.UserRepository defined in @EnableJpaRepositories declared on JpaRepositoriesRegistrar.EnableJpaRepositoriesConfiguration: Cannot resolve reference to bean 'jpaSharedEM_entityManagerFactory' while setting bean property 'entityManager'
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:795)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:237)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1375)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1212)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1443)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1353)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:904)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:782)
	... 57 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'userRepository' defined in com.personneltrackingsystem.repository.UserRepository defined in @EnableJpaRepositories declared on JpaRepositoriesRegistrar.EnableJpaRepositoriesConfiguration: Cannot resolve reference to bean 'jpaSharedEM_entityManagerFactory' while setting bean property 'entityManager'
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:377)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:135)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyPropertyValues(AbstractAutowireCapableBeanFactory.java:1705)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1454)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:599)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1443)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1353)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:904)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:782)
	... 71 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'jpaSharedEM_entityManagerFactory': Cannot resolve reference to bean 'entityManagerFactory' while setting constructor argument
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:377)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:135)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveConstructorArguments(ConstructorResolver.java:682)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:509)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1355)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1185)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:365)
	... 85 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'liquibase' defined in class path resource [org/springframework/boot/autoconfigure/liquibase/LiquibaseAutoConfiguration$LiquibaseConfiguration.class]: liquibase.exception.CommandExecutionException: liquibase.exception.ValidationFailedException: Validation Failed:
     1 changesets check sum
          db/changelog/changes/002-insert-initial-data.yaml::4::liquibase was: 9:d4666daa1e1cf3996a174df0e04044af but is now: 9:4afce42b7d8b7a4616d1482f6afedbf5

	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1806)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:313)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:365)
	... 97 common frames omitted
Caused by: liquibase.exception.UnexpectedLiquibaseException: liquibase.exception.CommandExecutionException: liquibase.exception.ValidationFailedException: Validation Failed:
     1 changesets check sum
          db/changelog/changes/002-insert-initial-data.yaml::4::liquibase was: 9:d4666daa1e1cf3996a174df0e04044af but is now: 9:4afce42b7d8b7a4616d1482f6afedbf5

	at liquibase.integration.spring.SpringLiquibase.afterPropertiesSet(SpringLiquibase.java:267)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1853)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1802)
	... 106 common frames omitted
Caused by: liquibase.exception.CommandExecutionException: liquibase.exception.ValidationFailedException: Validation Failed:
     1 changesets check sum
          db/changelog/changes/002-insert-initial-data.yaml::4::liquibase was: 9:d4666daa1e1cf3996a174df0e04044af but is now: 9:4afce42b7d8b7a4616d1482f6afedbf5

	at liquibase.command.CommandScope.execute(CommandScope.java:258)
	at liquibase.Liquibase.lambda$update$0(Liquibase.java:216)
	at liquibase.Scope.lambda$child$0(Scope.java:191)
	at liquibase.Scope.child(Scope.java:200)
	at liquibase.Scope.child(Scope.java:190)
	at liquibase.Scope.child(Scope.java:169)
	at liquibase.Liquibase.runInScope(Liquibase.java:1329)
	at liquibase.Liquibase.update(Liquibase.java:205)
	at liquibase.Liquibase.update(Liquibase.java:188)
	at liquibase.integration.spring.SpringLiquibase.performUpdate(SpringLiquibase.java:305)
	at liquibase.integration.spring.SpringLiquibase.lambda$afterPropertiesSet$0(SpringLiquibase.java:257)
	at liquibase.Scope.lambda$child$0(Scope.java:191)
	at liquibase.Scope.child(Scope.java:200)
	at liquibase.Scope.child(Scope.java:190)
	at liquibase.Scope.child(Scope.java:169)
	at liquibase.Scope.child(Scope.java:257)
	at liquibase.integration.spring.SpringLiquibase.afterPropertiesSet(SpringLiquibase.java:250)
	... 108 common frames omitted
Caused by: liquibase.exception.ValidationFailedException: Validation Failed:
     1 changesets check sum
          db/changelog/changes/002-insert-initial-data.yaml::4::liquibase was: 9:d4666daa1e1cf3996a174df0e04044af but is now: 9:4afce42b7d8b7a4616d1482f6afedbf5

	at liquibase.changelog.DatabaseChangeLog.validate(DatabaseChangeLog.java:398)
	at liquibase.command.core.helpers.DatabaseChangelogCommandStep.run(DatabaseChangelogCommandStep.java:92)
	at liquibase.command.CommandScope.execute(CommandScope.java:220)
	... 124 common frames omitted
2025-06-20 06:23:29 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - Starting PersonelTrackingSystemApplication using Java 21.0.7 with PID 6148 (C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes started by Lenovo in C:\Users\Lenovo\Desktop\personnel-tracking-system)
2025-06-20 06:23:29 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - No active profile set, falling back to 1 default profile: "default"
2025-06-20 06:23:30 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-20 06:23:30 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-20 06:23:30 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.30]
2025-06-20 06:23:30 [restartedMain] INFO  o.a.c.c.C.[Tomcat-1].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-20 06:23:30 [restartedMain] INFO  c.hazelcast.instance.AddressPicker - [LOCAL] [pts-cluster] [5.4.0] Interfaces is disabled, trying to pick one address from TCP-IP config addresses: [127.0.0.1]
2025-06-20 06:23:30 [restartedMain] INFO  com.hazelcast.system.logo - [127.0.0.1]:5701 [pts-cluster] [5.4.0] 
	o    o     o     o---o   o--o o      o---o     o     o----o o--o--o
	|    |    / \       /         |     /         / \    |         |   
	o----o       o     o   o----o |    o             o   o----o    |   
	|    |  *     \   /           |     \       *     \       |    |   
	o    o *       o o---o   o--o o----o o---o *       o o----o    o   
2025-06-20 06:23:30 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Copyright (c) 2008-2024, Hazelcast, Inc. All Rights Reserved.
2025-06-20 06:23:30 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Hazelcast Platform 5.4.0 (20240415) starting at [127.0.0.1]:5701
2025-06-20 06:23:30 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Cluster name: pts-cluster
2025-06-20 06:23:30 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Integrity Checker is disabled. Fail-fast on corrupted executables will not be performed. For more information, see the documentation for Integrity Checker.
2025-06-20 06:23:30 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] The Jet engine is disabled.
To enable the Jet engine on the members, do one of the following:
  - Change member config using Java API: config.getJetConfig().setEnabled(true)
  - Change XML/YAML configuration property: Set hazelcast.jet.enabled to true
  - Add system property: -Dhz.jet.enabled=true (for Hazelcast embedded, works only when loading config via Config.load)
  - Add environment variable: HZ_JET_ENABLED=true (recommended when running container image. For Hazelcast embedded, works only when loading config via Config.load)
2025-06-20 06:23:30 [restartedMain] INFO  com.hazelcast.system.security - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Enable DEBUG/FINE log level for log category com.hazelcast.system.security  or use -Dhazelcast.security.recommendations system property to see security recommendations and the status of current config.
2025-06-20 06:23:30 [restartedMain] INFO  com.hazelcast.instance.impl.Node - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Using TCP/IP discovery
2025-06-20 06:23:30 [restartedMain] WARN  com.hazelcast.cp.CPSubsystem - [127.0.0.1]:5701 [pts-cluster] [5.4.0] CP Subsystem is not enabled. CP data structures will operate in UNSAFE mode! Please note that UNSAFE mode will not provide strong consistency guarantees.
2025-06-20 06:23:30 [restartedMain] INFO  c.h.internal.diagnostics.Diagnostics - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Diagnostics disabled. To enable add -Dhazelcast.diagnostics.enabled=true to the JVM arguments.
2025-06-20 06:23:30 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is STARTING
2025-06-20 06:23:30 [hz.personnel-tracking-system.cached.thread-1] INFO  c.h.i.cluster.impl.TcpIpJoiner - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5703 is added to the blacklist.
2025-06-20 06:23:30 [hz.personnel-tracking-system.cached.thread-3] INFO  c.h.i.cluster.impl.TcpIpJoiner - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5702 is added to the blacklist.
2025-06-20 06:23:31 [restartedMain] INFO  c.h.internal.cluster.ClusterService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] 

Members {size:1, ver:1} [
	Member [127.0.0.1]:5701 - f014c98a-8a39-45ae-969a-6b1b5d9c3d6d this
]

2025-06-20 06:23:31 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is STARTED
2025-06-20 06:23:31 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-3 - Starting...
2025-06-20 06:23:31 [restartedMain] INFO  com.zaxxer.hikari.pool.HikariPool - HikariPool-3 - Added connection org.postgresql.jdbc.PgConnection@157b4949
2025-06-20 06:23:31 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-3 - Start completed.
2025-06-20 06:23:31 [restartedMain] INFO  liquibase.changelog - Reading from dbpersonel.databasechangelog
2025-06-20 06:23:31 [restartedMain] INFO  liquibase.changelog - Change failed validation!
2025-06-20 06:23:31 [restartedMain] INFO  liquibase.command - Logging exception.
2025-06-20 06:23:31 [restartedMain] INFO  liquibase.ui - ERROR: Exception Details
2025-06-20 06:23:31 [restartedMain] INFO  liquibase.ui - ERROR: Exception Primary Class:  ValidationFailedException
2025-06-20 06:23:31 [restartedMain] INFO  liquibase.ui - ERROR: Exception Primary Reason:  Validation Failed:
     1 changesets check sum
          db/changelog/changes/002-insert-initial-data.yaml::4::liquibase was: 9:d4666daa1e1cf3996a174df0e04044af but is now: 9:ead9528497b20384816bbdec5e8e2c36

2025-06-20 06:23:31 [restartedMain] INFO  liquibase.ui - ERROR: Exception Primary Source:  4.29.2
2025-06-20 06:23:31 [restartedMain] INFO  liquibase.command - Command execution complete
2025-06-20 06:23:31 [restartedMain] ERROR o.s.b.w.e.tomcat.TomcatStarter - Error starting Tomcat context. Exception: org.springframework.beans.factory.UnsatisfiedDependencyException. Message: Error creating bean with name 'jwtAuthenticationFilter' defined in file [C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes\com\personneltrackingsystem\filter\JwtAuthenticationFilter.class]: Unsatisfied dependency expressed through constructor parameter 1: Error creating bean with name 'customUserDetailsServiceImpl' defined in file [C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes\com\personneltrackingsystem\service\impl\CustomUserDetailsServiceImpl.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'userRepository' defined in com.personneltrackingsystem.repository.UserRepository defined in @EnableJpaRepositories declared on JpaRepositoriesRegistrar.EnableJpaRepositoriesConfiguration: Cannot resolve reference to bean 'jpaSharedEM_entityManagerFactory' while setting bean property 'entityManager'
2025-06-20 06:23:31 [restartedMain] INFO  o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-06-20 06:23:31 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.event-11] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:435)
 com.hazelcast.internal.util.executor.StripedExecutor$Worker.run(StripedExecutor.java:227)
2025-06-20 06:23:31 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.event-12] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:435)
 com.hazelcast.internal.util.executor.StripedExecutor$Worker.run(StripedExecutor.java:227)
2025-06-20 06:23:31 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.event-13] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:435)
 com.hazelcast.internal.util.executor.StripedExecutor$Worker.run(StripedExecutor.java:227)
2025-06-20 06:23:31 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.event-14] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:435)
 com.hazelcast.internal.util.executor.StripedExecutor$Worker.run(StripedExecutor.java:227)
2025-06-20 06:23:31 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.event-15] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:435)
 com.hazelcast.internal.util.executor.StripedExecutor$Worker.run(StripedExecutor.java:227)
2025-06-20 06:23:31 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.MetricsRegistry.thread-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1177)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 06:23:31 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.MetricsRegistry.thread-2] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:269)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1763)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 06:23:31 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.migration] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:269)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1763)
 java.base/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:460)
 com.hazelcast.internal.partition.impl.MigrationQueue.poll(MigrationQueue.java:48)
 com.hazelcast.internal.partition.impl.MigrationThread.doRun(MigrationThread.java:91)
 com.hazelcast.internal.partition.impl.MigrationThread.run(MigrationThread.java:66)
2025-06-20 06:23:31 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.InvocationMonitorThread] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:269)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1763)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 06:23:31 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.SlowOperationDetectorThread] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/java.lang.Thread.sleep0(Native Method)
 java.base/java.lang.Thread.sleep(Thread.java:558)
 java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
 com.hazelcast.spi.impl.operationexecutor.slowoperationdetector.SlowOperationDetector$DetectorThread.sleepInterval(SlowOperationDetector.java:280)
 com.hazelcast.spi.impl.operationexecutor.slowoperationdetector.SlowOperationDetector$DetectorThread.run(SlowOperationDetector.java:153)
2025-06-20 06:23:31 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-in-0] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:142)
 com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:292)
 com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249)
 com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111)
2025-06-20 06:23:31 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-in-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:142)
 com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:292)
 com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249)
 com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111)
2025-06-20 06:23:31 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-in-2] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:142)
 com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:292)
 com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249)
 com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111)
2025-06-20 06:23:31 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-out-0] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:142)
 com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:292)
 com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249)
 com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111)
2025-06-20 06:23:31 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-out-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:142)
 com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:292)
 com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249)
 com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111)
2025-06-20 06:23:31 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-out-2] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:142)
 com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:292)
 com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249)
 com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111)
2025-06-20 06:23:31 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.BalancerThread] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:269)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1763)
 java.base/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:460)
 com.hazelcast.internal.networking.nio.iobalancer.IOBalancerThread.run(IOBalancerThread.java:65)
2025-06-20 06:23:31 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-Acceptor] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:147)
 com.hazelcast.internal.server.tcp.TcpServerAcceptor$AcceptorIOThread.acceptLoop(TcpServerAcceptor.java:186)
 com.hazelcast.internal.server.tcp.TcpServerAcceptor$AcceptorIOThread.run(TcpServerAcceptor.java:172)
2025-06-20 06:23:31 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.TcpServer.thread-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1177)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 06:23:31 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.TcpServer.thread-2] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:269)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1763)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 06:23:31 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.TcpServer.thread-3] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1177)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 06:23:31 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.TcpServer.thread-4] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1177)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 06:23:31 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.HealthMonitor] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/java.lang.Thread.sleep0(Native Method)
 java.base/java.lang.Thread.sleep(Thread.java:558)
 java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
 com.hazelcast.internal.diagnostics.HealthMonitor$HealthMonitorThread.run(HealthMonitor.java:164)
2025-06-20 06:23:31 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [HikariPool-3 housekeeper] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:269)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1763)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 06:23:31 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [HikariPool-3 connection adder] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/java.lang.Thread.sleep0(Native Method)
 java.base/java.lang.Thread.sleep(Thread.java:509)
 com.zaxxer.hikari.util.UtilityElf.quietlySleep(UtilityElf.java:53)
 com.zaxxer.hikari.pool.HikariPool$PoolEntryCreator.call(HikariPool.java:729)
 com.zaxxer.hikari.pool.HikariPool$PoolEntryCreator.call(HikariPool.java:703)
 java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 06:23:31 [restartedMain] WARN  o.s.b.w.s.c.AnnotationConfigServletWebServerApplicationContext - Exception encountered during context initialization - cancelling refresh attempt: org.springframework.context.ApplicationContextException: Unable to start web server
2025-06-20 06:23:31 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-3 - Shutdown initiated...
2025-06-20 06:23:31 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-3 - Shutdown completed.
2025-06-20 06:23:31 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is SHUTTING_DOWN
2025-06-20 06:23:31 [restartedMain] INFO  com.hazelcast.instance.impl.Node - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Shutting down connection manager...
2025-06-20 06:23:31 [restartedMain] INFO  com.hazelcast.instance.impl.Node - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Shutting down node engine...
2025-06-20 06:23:31 [restartedMain] INFO  c.h.instance.impl.NodeExtension - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Destroying node NodeExtension.
2025-06-20 06:23:31 [restartedMain] INFO  com.hazelcast.instance.impl.Node - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Hazelcast Shutdown is completed in 17 ms.
2025-06-20 06:23:31 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is SHUTDOWN
2025-06-20 06:23:31 [restartedMain] ERROR o.s.boot.SpringApplication - Application run failed
org.springframework.context.ApplicationContextException: Unable to start web server
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.onRefresh(ServletWebServerApplicationContext.java:165)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:619)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:754)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:456)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:335)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1363)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1352)
	at com.personneltrackingsystem.PersonelTrackingSystemApplication.main(PersonelTrackingSystemApplication.java:12)
	at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)
	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
Caused by: org.springframework.boot.web.server.WebServerException: Unable to start embedded Tomcat
	at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.initialize(TomcatWebServer.java:147)
	at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.<init>(TomcatWebServer.java:107)
	at org.springframework.boot.web.embedded.tomcat.TomcatServletWebServerFactory.getTomcatWebServer(TomcatServletWebServerFactory.java:516)
	at org.springframework.boot.web.embedded.tomcat.TomcatServletWebServerFactory.getWebServer(TomcatServletWebServerFactory.java:222)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.createWebServer(ServletWebServerApplicationContext.java:188)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.onRefresh(ServletWebServerApplicationContext.java:162)
	... 11 common frames omitted
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'jwtAuthenticationFilter' defined in file [C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes\com\personneltrackingsystem\filter\JwtAuthenticationFilter.class]: Unsatisfied dependency expressed through constructor parameter 1: Error creating bean with name 'customUserDetailsServiceImpl' defined in file [C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes\com\personneltrackingsystem\service\impl\CustomUserDetailsServiceImpl.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'userRepository' defined in com.personneltrackingsystem.repository.UserRepository defined in @EnableJpaRepositories declared on JpaRepositoriesRegistrar.EnableJpaRepositoriesConfiguration: Cannot resolve reference to bean 'jpaSharedEM_entityManagerFactory' while setting bean property 'entityManager'
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:795)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:237)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1375)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1212)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:205)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.getOrderedBeansOfType(ServletContextInitializerBeans.java:211)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.addAsRegistrationBean(ServletContextInitializerBeans.java:174)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.addAsRegistrationBean(ServletContextInitializerBeans.java:169)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.addAdaptableBeans(ServletContextInitializerBeans.java:154)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.<init>(ServletContextInitializerBeans.java:87)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.getServletContextInitializerBeans(ServletWebServerApplicationContext.java:266)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.selfInitialize(ServletWebServerApplicationContext.java:240)
	at org.springframework.boot.web.embedded.tomcat.TomcatStarter.onStartup(TomcatStarter.java:52)
	at org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:4412)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1203)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1193)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at org.apache.tomcat.util.threads.InlineExecutorService.execute(InlineExecutorService.java:75)
	at java.base/java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:145)
	at org.apache.catalina.core.ContainerBase.startInternal(ContainerBase.java:749)
	at org.apache.catalina.core.StandardHost.startInternal(StandardHost.java:772)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1203)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1193)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at org.apache.tomcat.util.threads.InlineExecutorService.execute(InlineExecutorService.java:75)
	at java.base/java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:145)
	at org.apache.catalina.core.ContainerBase.startInternal(ContainerBase.java:749)
	at org.apache.catalina.core.StandardEngine.startInternal(StandardEngine.java:203)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.core.StandardService.startInternal(StandardService.java:415)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.core.StandardServer.startInternal(StandardServer.java:870)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.startup.Tomcat.start(Tomcat.java:437)
	at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.initialize(TomcatWebServer.java:128)
	... 16 common frames omitted
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'customUserDetailsServiceImpl' defined in file [C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes\com\personneltrackingsystem\service\impl\CustomUserDetailsServiceImpl.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'userRepository' defined in com.personneltrackingsystem.repository.UserRepository defined in @EnableJpaRepositories declared on JpaRepositoriesRegistrar.EnableJpaRepositoriesConfiguration: Cannot resolve reference to bean 'jpaSharedEM_entityManagerFactory' while setting bean property 'entityManager'
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:795)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:237)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1375)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1212)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1443)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1353)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:904)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:782)
	... 57 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'userRepository' defined in com.personneltrackingsystem.repository.UserRepository defined in @EnableJpaRepositories declared on JpaRepositoriesRegistrar.EnableJpaRepositoriesConfiguration: Cannot resolve reference to bean 'jpaSharedEM_entityManagerFactory' while setting bean property 'entityManager'
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:377)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:135)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyPropertyValues(AbstractAutowireCapableBeanFactory.java:1705)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1454)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:599)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1443)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1353)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:904)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:782)
	... 71 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'jpaSharedEM_entityManagerFactory': Cannot resolve reference to bean 'entityManagerFactory' while setting constructor argument
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:377)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:135)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveConstructorArguments(ConstructorResolver.java:682)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:509)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1355)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1185)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:365)
	... 85 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'liquibase' defined in class path resource [org/springframework/boot/autoconfigure/liquibase/LiquibaseAutoConfiguration$LiquibaseConfiguration.class]: liquibase.exception.CommandExecutionException: liquibase.exception.ValidationFailedException: Validation Failed:
     1 changesets check sum
          db/changelog/changes/002-insert-initial-data.yaml::4::liquibase was: 9:d4666daa1e1cf3996a174df0e04044af but is now: 9:ead9528497b20384816bbdec5e8e2c36

	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1806)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:313)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:365)
	... 97 common frames omitted
Caused by: liquibase.exception.UnexpectedLiquibaseException: liquibase.exception.CommandExecutionException: liquibase.exception.ValidationFailedException: Validation Failed:
     1 changesets check sum
          db/changelog/changes/002-insert-initial-data.yaml::4::liquibase was: 9:d4666daa1e1cf3996a174df0e04044af but is now: 9:ead9528497b20384816bbdec5e8e2c36

	at liquibase.integration.spring.SpringLiquibase.afterPropertiesSet(SpringLiquibase.java:267)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1853)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1802)
	... 106 common frames omitted
Caused by: liquibase.exception.CommandExecutionException: liquibase.exception.ValidationFailedException: Validation Failed:
     1 changesets check sum
          db/changelog/changes/002-insert-initial-data.yaml::4::liquibase was: 9:d4666daa1e1cf3996a174df0e04044af but is now: 9:ead9528497b20384816bbdec5e8e2c36

	at liquibase.command.CommandScope.execute(CommandScope.java:258)
	at liquibase.Liquibase.lambda$update$0(Liquibase.java:216)
	at liquibase.Scope.lambda$child$0(Scope.java:191)
	at liquibase.Scope.child(Scope.java:200)
	at liquibase.Scope.child(Scope.java:190)
	at liquibase.Scope.child(Scope.java:169)
	at liquibase.Liquibase.runInScope(Liquibase.java:1329)
	at liquibase.Liquibase.update(Liquibase.java:205)
	at liquibase.Liquibase.update(Liquibase.java:188)
	at liquibase.integration.spring.SpringLiquibase.performUpdate(SpringLiquibase.java:305)
	at liquibase.integration.spring.SpringLiquibase.lambda$afterPropertiesSet$0(SpringLiquibase.java:257)
	at liquibase.Scope.lambda$child$0(Scope.java:191)
	at liquibase.Scope.child(Scope.java:200)
	at liquibase.Scope.child(Scope.java:190)
	at liquibase.Scope.child(Scope.java:169)
	at liquibase.Scope.child(Scope.java:257)
	at liquibase.integration.spring.SpringLiquibase.afterPropertiesSet(SpringLiquibase.java:250)
	... 108 common frames omitted
Caused by: liquibase.exception.ValidationFailedException: Validation Failed:
     1 changesets check sum
          db/changelog/changes/002-insert-initial-data.yaml::4::liquibase was: 9:d4666daa1e1cf3996a174df0e04044af but is now: 9:ead9528497b20384816bbdec5e8e2c36

	at liquibase.changelog.DatabaseChangeLog.validate(DatabaseChangeLog.java:398)
	at liquibase.command.core.helpers.DatabaseChangelogCommandStep.run(DatabaseChangelogCommandStep.java:92)
	at liquibase.command.CommandScope.execute(CommandScope.java:220)
	... 124 common frames omitted
2025-06-20 06:24:49 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - Starting PersonelTrackingSystemApplication using Java 21.0.7 with PID 6148 (C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes started by Lenovo in C:\Users\Lenovo\Desktop\personnel-tracking-system)
2025-06-20 06:24:49 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - No active profile set, falling back to 1 default profile: "default"
2025-06-20 06:24:49 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-20 06:24:49 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-20 06:24:49 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.30]
2025-06-20 06:24:49 [restartedMain] INFO  o.a.c.c.C.[Tomcat-2].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-20 06:24:49 [restartedMain] INFO  c.hazelcast.instance.AddressPicker - [LOCAL] [pts-cluster] [5.4.0] Interfaces is disabled, trying to pick one address from TCP-IP config addresses: [127.0.0.1]
2025-06-20 06:24:49 [restartedMain] INFO  com.hazelcast.system.logo - [127.0.0.1]:5701 [pts-cluster] [5.4.0] 
	o    o     o     o---o   o--o o      o---o     o     o----o o--o--o
	|    |    / \       /         |     /         / \    |         |   
	o----o       o     o   o----o |    o             o   o----o    |   
	|    |  *     \   /           |     \       *     \       |    |   
	o    o *       o o---o   o--o o----o o---o *       o o----o    o   
2025-06-20 06:24:49 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Copyright (c) 2008-2024, Hazelcast, Inc. All Rights Reserved.
2025-06-20 06:24:49 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Hazelcast Platform 5.4.0 (20240415) starting at [127.0.0.1]:5701
2025-06-20 06:24:49 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Cluster name: pts-cluster
2025-06-20 06:24:49 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Integrity Checker is disabled. Fail-fast on corrupted executables will not be performed. For more information, see the documentation for Integrity Checker.
2025-06-20 06:24:49 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] The Jet engine is disabled.
To enable the Jet engine on the members, do one of the following:
  - Change member config using Java API: config.getJetConfig().setEnabled(true)
  - Change XML/YAML configuration property: Set hazelcast.jet.enabled to true
  - Add system property: -Dhz.jet.enabled=true (for Hazelcast embedded, works only when loading config via Config.load)
  - Add environment variable: HZ_JET_ENABLED=true (recommended when running container image. For Hazelcast embedded, works only when loading config via Config.load)
2025-06-20 06:24:49 [restartedMain] INFO  com.hazelcast.system.security - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Enable DEBUG/FINE log level for log category com.hazelcast.system.security  or use -Dhazelcast.security.recommendations system property to see security recommendations and the status of current config.
2025-06-20 06:24:49 [restartedMain] INFO  com.hazelcast.instance.impl.Node - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Using TCP/IP discovery
2025-06-20 06:24:49 [restartedMain] WARN  com.hazelcast.cp.CPSubsystem - [127.0.0.1]:5701 [pts-cluster] [5.4.0] CP Subsystem is not enabled. CP data structures will operate in UNSAFE mode! Please note that UNSAFE mode will not provide strong consistency guarantees.
2025-06-20 06:24:49 [restartedMain] INFO  c.h.internal.diagnostics.Diagnostics - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Diagnostics disabled. To enable add -Dhazelcast.diagnostics.enabled=true to the JVM arguments.
2025-06-20 06:24:49 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is STARTING
2025-06-20 06:24:49 [hz.personnel-tracking-system.cached.thread-4] INFO  c.h.i.cluster.impl.TcpIpJoiner - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5703 is added to the blacklist.
2025-06-20 06:24:49 [hz.personnel-tracking-system.cached.thread-5] INFO  c.h.i.cluster.impl.TcpIpJoiner - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5702 is added to the blacklist.
2025-06-20 06:24:50 [restartedMain] INFO  c.h.internal.cluster.ClusterService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] 

Members {size:1, ver:1} [
	Member [127.0.0.1]:5701 - 1c48f2db-cc02-4e7c-8ec9-b04369da83a2 this
]

2025-06-20 06:24:50 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is STARTED
2025-06-20 06:24:50 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-4 - Starting...
2025-06-20 06:24:50 [restartedMain] INFO  com.zaxxer.hikari.pool.HikariPool - HikariPool-4 - Added connection org.postgresql.jdbc.PgConnection@6c9fdf7
2025-06-20 06:24:50 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-4 - Start completed.
2025-06-20 06:24:51 [restartedMain] INFO  liquibase.changelog - Reading from dbpersonel.databasechangelog
2025-06-20 06:24:51 [restartedMain] INFO  liquibase.changelog - Change failed validation!
2025-06-20 06:24:51 [restartedMain] INFO  liquibase.command - Logging exception.
2025-06-20 06:24:51 [restartedMain] INFO  liquibase.ui - ERROR: Exception Details
2025-06-20 06:24:51 [restartedMain] INFO  liquibase.ui - ERROR: Exception Primary Class:  ValidationFailedException
2025-06-20 06:24:51 [restartedMain] INFO  liquibase.ui - ERROR: Exception Primary Reason:  Validation Failed:
     2 changesets check sum
          db/changelog/changes/002-insert-initial-data.yaml::4::liquibase was: 9:d4666daa1e1cf3996a174df0e04044af but is now: 9:ead9528497b20384816bbdec5e8e2c36
          db/changelog/changes/002-insert-initial-data.yaml::5::liquibase was: 9:0a4c30cedd8a2c460ddf8b2e543dbefa but is now: 9:72d3a05465cfc3ddc522447fe86e8499

2025-06-20 06:24:51 [restartedMain] INFO  liquibase.ui - ERROR: Exception Primary Source:  4.29.2
2025-06-20 06:24:51 [restartedMain] INFO  liquibase.command - Command execution complete
2025-06-20 06:24:51 [restartedMain] ERROR o.s.b.w.e.tomcat.TomcatStarter - Error starting Tomcat context. Exception: org.springframework.beans.factory.UnsatisfiedDependencyException. Message: Error creating bean with name 'jwtAuthenticationFilter' defined in file [C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes\com\personneltrackingsystem\filter\JwtAuthenticationFilter.class]: Unsatisfied dependency expressed through constructor parameter 1: Error creating bean with name 'customUserDetailsServiceImpl' defined in file [C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes\com\personneltrackingsystem\service\impl\CustomUserDetailsServiceImpl.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'userRepository' defined in com.personneltrackingsystem.repository.UserRepository defined in @EnableJpaRepositories declared on JpaRepositoriesRegistrar.EnableJpaRepositoriesConfiguration: Cannot resolve reference to bean 'jpaSharedEM_entityManagerFactory' while setting bean property 'entityManager'
2025-06-20 06:24:51 [restartedMain] INFO  o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-06-20 06:24:51 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.event-16] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:435)
 com.hazelcast.internal.util.executor.StripedExecutor$Worker.run(StripedExecutor.java:227)
2025-06-20 06:24:51 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.event-17] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:435)
 com.hazelcast.internal.util.executor.StripedExecutor$Worker.run(StripedExecutor.java:227)
2025-06-20 06:24:51 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.event-18] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:435)
 com.hazelcast.internal.util.executor.StripedExecutor$Worker.run(StripedExecutor.java:227)
2025-06-20 06:24:51 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.event-19] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:435)
 com.hazelcast.internal.util.executor.StripedExecutor$Worker.run(StripedExecutor.java:227)
2025-06-20 06:24:51 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.event-20] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:435)
 com.hazelcast.internal.util.executor.StripedExecutor$Worker.run(StripedExecutor.java:227)
2025-06-20 06:24:51 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.MetricsRegistry.thread-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:269)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1763)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 06:24:51 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.MetricsRegistry.thread-2] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1177)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 06:24:51 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.migration] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:269)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1763)
 java.base/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:460)
 com.hazelcast.internal.partition.impl.MigrationQueue.poll(MigrationQueue.java:48)
 com.hazelcast.internal.partition.impl.MigrationThread.doRun(MigrationThread.java:91)
 com.hazelcast.internal.partition.impl.MigrationThread.run(MigrationThread.java:66)
2025-06-20 06:24:51 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.InvocationMonitorThread] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:269)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1763)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 06:24:51 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.SlowOperationDetectorThread] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/java.lang.Thread.sleep0(Native Method)
 java.base/java.lang.Thread.sleep(Thread.java:558)
 java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
 com.hazelcast.spi.impl.operationexecutor.slowoperationdetector.SlowOperationDetector$DetectorThread.sleepInterval(SlowOperationDetector.java:280)
 com.hazelcast.spi.impl.operationexecutor.slowoperationdetector.SlowOperationDetector$DetectorThread.run(SlowOperationDetector.java:153)
2025-06-20 06:24:51 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-in-0] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:142)
 com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:292)
 com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249)
 com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111)
2025-06-20 06:24:51 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-in-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:142)
 com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:292)
 com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249)
 com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111)
2025-06-20 06:24:51 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-in-2] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:142)
 com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:292)
 com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249)
 com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111)
2025-06-20 06:24:51 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-out-0] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:142)
 com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:292)
 com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249)
 com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111)
2025-06-20 06:24:51 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-out-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:142)
 com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:292)
 com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249)
 com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111)
2025-06-20 06:24:51 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-out-2] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:142)
 com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:292)
 com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249)
 com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111)
2025-06-20 06:24:51 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.BalancerThread] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:269)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1763)
 java.base/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:460)
 com.hazelcast.internal.networking.nio.iobalancer.IOBalancerThread.run(IOBalancerThread.java:65)
2025-06-20 06:24:51 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-Acceptor] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:147)
 com.hazelcast.internal.server.tcp.TcpServerAcceptor$AcceptorIOThread.acceptLoop(TcpServerAcceptor.java:186)
 com.hazelcast.internal.server.tcp.TcpServerAcceptor$AcceptorIOThread.run(TcpServerAcceptor.java:172)
2025-06-20 06:24:51 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.TcpServer.thread-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1177)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 06:24:51 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.TcpServer.thread-2] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1177)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 06:24:51 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.TcpServer.thread-3] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1177)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 06:24:51 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.TcpServer.thread-4] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:269)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1763)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 06:24:51 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.HealthMonitor] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/java.lang.Thread.sleep0(Native Method)
 java.base/java.lang.Thread.sleep(Thread.java:558)
 java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
 com.hazelcast.internal.diagnostics.HealthMonitor$HealthMonitorThread.run(HealthMonitor.java:164)
2025-06-20 06:24:51 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [HikariPool-4 housekeeper] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:269)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1763)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 06:24:51 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [HikariPool-4 connection adder] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/java.lang.Thread.sleep0(Native Method)
 java.base/java.lang.Thread.sleep(Thread.java:509)
 com.zaxxer.hikari.util.UtilityElf.quietlySleep(UtilityElf.java:53)
 com.zaxxer.hikari.pool.HikariPool$PoolEntryCreator.call(HikariPool.java:729)
 com.zaxxer.hikari.pool.HikariPool$PoolEntryCreator.call(HikariPool.java:703)
 java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 06:24:51 [restartedMain] WARN  o.s.b.w.s.c.AnnotationConfigServletWebServerApplicationContext - Exception encountered during context initialization - cancelling refresh attempt: org.springframework.context.ApplicationContextException: Unable to start web server
2025-06-20 06:24:51 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-4 - Shutdown initiated...
2025-06-20 06:24:51 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-4 - Shutdown completed.
2025-06-20 06:24:51 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is SHUTTING_DOWN
2025-06-20 06:24:51 [restartedMain] INFO  com.hazelcast.instance.impl.Node - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Shutting down connection manager...
2025-06-20 06:24:51 [restartedMain] INFO  com.hazelcast.instance.impl.Node - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Shutting down node engine...
2025-06-20 06:24:51 [restartedMain] INFO  c.h.instance.impl.NodeExtension - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Destroying node NodeExtension.
2025-06-20 06:24:51 [restartedMain] INFO  com.hazelcast.instance.impl.Node - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Hazelcast Shutdown is completed in 14 ms.
2025-06-20 06:24:51 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is SHUTDOWN
2025-06-20 06:24:51 [restartedMain] ERROR o.s.boot.SpringApplication - Application run failed
org.springframework.context.ApplicationContextException: Unable to start web server
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.onRefresh(ServletWebServerApplicationContext.java:165)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:619)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:754)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:456)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:335)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1363)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1352)
	at com.personneltrackingsystem.PersonelTrackingSystemApplication.main(PersonelTrackingSystemApplication.java:12)
	at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)
	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
Caused by: org.springframework.boot.web.server.WebServerException: Unable to start embedded Tomcat
	at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.initialize(TomcatWebServer.java:147)
	at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.<init>(TomcatWebServer.java:107)
	at org.springframework.boot.web.embedded.tomcat.TomcatServletWebServerFactory.getTomcatWebServer(TomcatServletWebServerFactory.java:516)
	at org.springframework.boot.web.embedded.tomcat.TomcatServletWebServerFactory.getWebServer(TomcatServletWebServerFactory.java:222)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.createWebServer(ServletWebServerApplicationContext.java:188)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.onRefresh(ServletWebServerApplicationContext.java:162)
	... 11 common frames omitted
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'jwtAuthenticationFilter' defined in file [C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes\com\personneltrackingsystem\filter\JwtAuthenticationFilter.class]: Unsatisfied dependency expressed through constructor parameter 1: Error creating bean with name 'customUserDetailsServiceImpl' defined in file [C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes\com\personneltrackingsystem\service\impl\CustomUserDetailsServiceImpl.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'userRepository' defined in com.personneltrackingsystem.repository.UserRepository defined in @EnableJpaRepositories declared on JpaRepositoriesRegistrar.EnableJpaRepositoriesConfiguration: Cannot resolve reference to bean 'jpaSharedEM_entityManagerFactory' while setting bean property 'entityManager'
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:795)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:237)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1375)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1212)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:205)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.getOrderedBeansOfType(ServletContextInitializerBeans.java:211)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.addAsRegistrationBean(ServletContextInitializerBeans.java:174)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.addAsRegistrationBean(ServletContextInitializerBeans.java:169)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.addAdaptableBeans(ServletContextInitializerBeans.java:154)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.<init>(ServletContextInitializerBeans.java:87)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.getServletContextInitializerBeans(ServletWebServerApplicationContext.java:266)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.selfInitialize(ServletWebServerApplicationContext.java:240)
	at org.springframework.boot.web.embedded.tomcat.TomcatStarter.onStartup(TomcatStarter.java:52)
	at org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:4412)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1203)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1193)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at org.apache.tomcat.util.threads.InlineExecutorService.execute(InlineExecutorService.java:75)
	at java.base/java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:145)
	at org.apache.catalina.core.ContainerBase.startInternal(ContainerBase.java:749)
	at org.apache.catalina.core.StandardHost.startInternal(StandardHost.java:772)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1203)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1193)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at org.apache.tomcat.util.threads.InlineExecutorService.execute(InlineExecutorService.java:75)
	at java.base/java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:145)
	at org.apache.catalina.core.ContainerBase.startInternal(ContainerBase.java:749)
	at org.apache.catalina.core.StandardEngine.startInternal(StandardEngine.java:203)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.core.StandardService.startInternal(StandardService.java:415)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.core.StandardServer.startInternal(StandardServer.java:870)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.startup.Tomcat.start(Tomcat.java:437)
	at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.initialize(TomcatWebServer.java:128)
	... 16 common frames omitted
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'customUserDetailsServiceImpl' defined in file [C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes\com\personneltrackingsystem\service\impl\CustomUserDetailsServiceImpl.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'userRepository' defined in com.personneltrackingsystem.repository.UserRepository defined in @EnableJpaRepositories declared on JpaRepositoriesRegistrar.EnableJpaRepositoriesConfiguration: Cannot resolve reference to bean 'jpaSharedEM_entityManagerFactory' while setting bean property 'entityManager'
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:795)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:237)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1375)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1212)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1443)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1353)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:904)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:782)
	... 57 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'userRepository' defined in com.personneltrackingsystem.repository.UserRepository defined in @EnableJpaRepositories declared on JpaRepositoriesRegistrar.EnableJpaRepositoriesConfiguration: Cannot resolve reference to bean 'jpaSharedEM_entityManagerFactory' while setting bean property 'entityManager'
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:377)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:135)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyPropertyValues(AbstractAutowireCapableBeanFactory.java:1705)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1454)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:599)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1443)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1353)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:904)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:782)
	... 71 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'jpaSharedEM_entityManagerFactory': Cannot resolve reference to bean 'entityManagerFactory' while setting constructor argument
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:377)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:135)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveConstructorArguments(ConstructorResolver.java:682)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:509)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1355)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1185)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:365)
	... 85 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'liquibase' defined in class path resource [org/springframework/boot/autoconfigure/liquibase/LiquibaseAutoConfiguration$LiquibaseConfiguration.class]: liquibase.exception.CommandExecutionException: liquibase.exception.ValidationFailedException: Validation Failed:
     2 changesets check sum
          db/changelog/changes/002-insert-initial-data.yaml::4::liquibase was: 9:d4666daa1e1cf3996a174df0e04044af but is now: 9:ead9528497b20384816bbdec5e8e2c36
          db/changelog/changes/002-insert-initial-data.yaml::5::liquibase was: 9:0a4c30cedd8a2c460ddf8b2e543dbefa but is now: 9:72d3a05465cfc3ddc522447fe86e8499

	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1806)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:313)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:365)
	... 97 common frames omitted
Caused by: liquibase.exception.UnexpectedLiquibaseException: liquibase.exception.CommandExecutionException: liquibase.exception.ValidationFailedException: Validation Failed:
     2 changesets check sum
          db/changelog/changes/002-insert-initial-data.yaml::4::liquibase was: 9:d4666daa1e1cf3996a174df0e04044af but is now: 9:ead9528497b20384816bbdec5e8e2c36
          db/changelog/changes/002-insert-initial-data.yaml::5::liquibase was: 9:0a4c30cedd8a2c460ddf8b2e543dbefa but is now: 9:72d3a05465cfc3ddc522447fe86e8499

	at liquibase.integration.spring.SpringLiquibase.afterPropertiesSet(SpringLiquibase.java:267)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1853)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1802)
	... 106 common frames omitted
Caused by: liquibase.exception.CommandExecutionException: liquibase.exception.ValidationFailedException: Validation Failed:
     2 changesets check sum
          db/changelog/changes/002-insert-initial-data.yaml::4::liquibase was: 9:d4666daa1e1cf3996a174df0e04044af but is now: 9:ead9528497b20384816bbdec5e8e2c36
          db/changelog/changes/002-insert-initial-data.yaml::5::liquibase was: 9:0a4c30cedd8a2c460ddf8b2e543dbefa but is now: 9:72d3a05465cfc3ddc522447fe86e8499

	at liquibase.command.CommandScope.execute(CommandScope.java:258)
	at liquibase.Liquibase.lambda$update$0(Liquibase.java:216)
	at liquibase.Scope.lambda$child$0(Scope.java:191)
	at liquibase.Scope.child(Scope.java:200)
	at liquibase.Scope.child(Scope.java:190)
	at liquibase.Scope.child(Scope.java:169)
	at liquibase.Liquibase.runInScope(Liquibase.java:1329)
	at liquibase.Liquibase.update(Liquibase.java:205)
	at liquibase.Liquibase.update(Liquibase.java:188)
	at liquibase.integration.spring.SpringLiquibase.performUpdate(SpringLiquibase.java:305)
	at liquibase.integration.spring.SpringLiquibase.lambda$afterPropertiesSet$0(SpringLiquibase.java:257)
	at liquibase.Scope.lambda$child$0(Scope.java:191)
	at liquibase.Scope.child(Scope.java:200)
	at liquibase.Scope.child(Scope.java:190)
	at liquibase.Scope.child(Scope.java:169)
	at liquibase.Scope.child(Scope.java:257)
	at liquibase.integration.spring.SpringLiquibase.afterPropertiesSet(SpringLiquibase.java:250)
	... 108 common frames omitted
Caused by: liquibase.exception.ValidationFailedException: Validation Failed:
     2 changesets check sum
          db/changelog/changes/002-insert-initial-data.yaml::4::liquibase was: 9:d4666daa1e1cf3996a174df0e04044af but is now: 9:ead9528497b20384816bbdec5e8e2c36
          db/changelog/changes/002-insert-initial-data.yaml::5::liquibase was: 9:0a4c30cedd8a2c460ddf8b2e543dbefa but is now: 9:72d3a05465cfc3ddc522447fe86e8499

	at liquibase.changelog.DatabaseChangeLog.validate(DatabaseChangeLog.java:398)
	at liquibase.command.core.helpers.DatabaseChangelogCommandStep.run(DatabaseChangelogCommandStep.java:92)
	at liquibase.command.CommandScope.execute(CommandScope.java:220)
	... 124 common frames omitted
2025-06-20 06:26:09 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - Starting PersonelTrackingSystemApplication using Java 21.0.7 with PID 13468 (C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes started by Lenovo in C:\Users\Lenovo\Desktop\personnel-tracking-system)
2025-06-20 06:26:09 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - No active profile set, falling back to 1 default profile: "default"
2025-06-20 06:26:12 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-20 06:26:12 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-20 06:26:12 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.30]
2025-06-20 06:26:12 [restartedMain] INFO  o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-20 06:26:12 [restartedMain] WARN  c.h.i.impl.HazelcastInstanceFactory - Hazelcast is starting in a Java modular environment (Java 9 and newer) but without proper access to required Java packages. Use additional Java arguments to provide Hazelcast access to Java internal API. The internal API access is used to get the best performance results. Arguments to be used:
 --add-modules java.se --add-exports java.base/jdk.internal.ref=ALL-UNNAMED --add-opens java.base/java.lang=ALL-UNNAMED --add-opens java.base/sun.nio.ch=ALL-UNNAMED --add-opens java.management/sun.management=ALL-UNNAMED --add-opens jdk.management/com.sun.management.internal=ALL-UNNAMED
2025-06-20 06:26:12 [restartedMain] INFO  c.hazelcast.instance.AddressPicker - [LOCAL] [pts-cluster] [5.4.0] Interfaces is disabled, trying to pick one address from TCP-IP config addresses: [127.0.0.1]
2025-06-20 06:26:12 [restartedMain] INFO  com.hazelcast.system.logo - [127.0.0.1]:5701 [pts-cluster] [5.4.0] 
	o    o     o     o---o   o--o o      o---o     o     o----o o--o--o
	|    |    / \       /         |     /         / \    |         |   
	o----o       o     o   o----o |    o             o   o----o    |   
	|    |  *     \   /           |     \       *     \       |    |   
	o    o *       o o---o   o--o o----o o---o *       o o----o    o   
2025-06-20 06:26:12 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Copyright (c) 2008-2024, Hazelcast, Inc. All Rights Reserved.
2025-06-20 06:26:12 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Hazelcast Platform 5.4.0 (20240415) starting at [127.0.0.1]:5701
2025-06-20 06:26:12 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Cluster name: pts-cluster
2025-06-20 06:26:12 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Integrity Checker is disabled. Fail-fast on corrupted executables will not be performed. For more information, see the documentation for Integrity Checker.
2025-06-20 06:26:12 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] The Jet engine is disabled.
To enable the Jet engine on the members, do one of the following:
  - Change member config using Java API: config.getJetConfig().setEnabled(true)
  - Change XML/YAML configuration property: Set hazelcast.jet.enabled to true
  - Add system property: -Dhz.jet.enabled=true (for Hazelcast embedded, works only when loading config via Config.load)
  - Add environment variable: HZ_JET_ENABLED=true (recommended when running container image. For Hazelcast embedded, works only when loading config via Config.load)
2025-06-20 06:26:13 [restartedMain] INFO  com.hazelcast.system.security - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Enable DEBUG/FINE log level for log category com.hazelcast.system.security  or use -Dhazelcast.security.recommendations system property to see security recommendations and the status of current config.
2025-06-20 06:26:13 [restartedMain] INFO  com.hazelcast.instance.impl.Node - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Using TCP/IP discovery
2025-06-20 06:26:13 [restartedMain] WARN  com.hazelcast.cp.CPSubsystem - [127.0.0.1]:5701 [pts-cluster] [5.4.0] CP Subsystem is not enabled. CP data structures will operate in UNSAFE mode! Please note that UNSAFE mode will not provide strong consistency guarantees.
2025-06-20 06:26:13 [restartedMain] INFO  c.h.internal.diagnostics.Diagnostics - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Diagnostics disabled. To enable add -Dhazelcast.diagnostics.enabled=true to the JVM arguments.
2025-06-20 06:26:13 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is STARTING
2025-06-20 06:26:13 [hz.personnel-tracking-system.cached.thread-2] INFO  c.h.i.cluster.impl.TcpIpJoiner - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5703 is added to the blacklist.
2025-06-20 06:26:13 [hz.personnel-tracking-system.cached.thread-3] INFO  c.h.i.cluster.impl.TcpIpJoiner - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5702 is added to the blacklist.
2025-06-20 06:26:14 [restartedMain] INFO  c.h.internal.cluster.ClusterService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] 

Members {size:1, ver:1} [
	Member [127.0.0.1]:5701 - 4c31b353-3e12-40a6-a85c-4dccad7fb2e4 this
]

2025-06-20 06:26:14 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is STARTED
2025-06-20 06:26:14 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-1 - Starting...
2025-06-20 06:26:15 [restartedMain] INFO  com.zaxxer.hikari.pool.HikariPool - HikariPool-1 - Added connection org.postgresql.jdbc.PgConnection@d40eb85
2025-06-20 06:26:15 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-1 - Start completed.
2025-06-20 06:26:15 [restartedMain] INFO  liquibase.changelog - Creating database history table with name: dbpersonel.databasechangelog
2025-06-20 06:26:15 [restartedMain] INFO  liquibase.changelog - Reading from dbpersonel.databasechangelog
2025-06-20 06:26:15 [restartedMain] INFO  liquibase.lockservice - Successfully acquired change log lock
2025-06-20 06:26:15 [restartedMain] INFO  liquibase.command - Using deploymentId: 0389975739
2025-06-20 06:26:15 [restartedMain] INFO  liquibase.changelog - Reading from dbpersonel.databasechangelog
2025-06-20 06:26:15 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::1::liquibase
2025-06-20 06:26:15 [restartedMain] WARN  liquibase.executor - "dbpersonel" emas zaten mevcut, atlanyor
2025-06-20 06:26:15 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:26:15 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::1::liquibase ran successfully in 10ms
2025-06-20 06:26:15 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::2::liquibase
2025-06-20 06:26:15 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:26:15 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:26:15 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:26:15 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:26:15 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:26:15 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:26:15 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:26:15 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:26:15 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:26:15 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:26:15 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:26:15 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:26:15 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:26:15 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:26:15 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:26:15 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:26:15 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:26:15 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:26:15 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:26:15 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:26:15 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:26:15 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:26:15 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:26:15 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:26:15 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:26:15 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:26:15 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::2::liquibase ran successfully in 72ms
2025-06-20 06:26:15 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::3::liquibase
2025-06-20 06:26:15 [restartedMain] INFO  liquibase.changelog - Table personel_type created
2025-06-20 06:26:15 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::3::liquibase ran successfully in 11ms
2025-06-20 06:26:15 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::4::liquibase
2025-06-20 06:26:15 [restartedMain] INFO  liquibase.changelog - Table building created
2025-06-20 06:26:15 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::4::liquibase ran successfully in 5ms
2025-06-20 06:26:15 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::5::liquibase
2025-06-20 06:26:15 [restartedMain] INFO  liquibase.changelog - Table floor created
2025-06-20 06:26:15 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::5::liquibase ran successfully in 5ms
2025-06-20 06:26:15 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::6::liquibase
2025-06-20 06:26:15 [restartedMain] INFO  liquibase.changelog - Table personel created
2025-06-20 06:26:15 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::6::liquibase ran successfully in 9ms
2025-06-20 06:26:15 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::7::liquibase
2025-06-20 06:26:15 [restartedMain] INFO  liquibase.changelog - Table unit created
2025-06-20 06:26:15 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::7::liquibase ran successfully in 12ms
2025-06-20 06:26:15 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::8::liquibase
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - Table personel_unit created
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::8::liquibase ran successfully in 5ms
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::9::liquibase
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - Table gate created
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::9::liquibase ran successfully in 4ms
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::10::liquibase
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - Table turnstile created
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::10::liquibase ran successfully in 6ms
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::11::liquibase
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - Table turnstile_registration_log created
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::11::liquibase ran successfully in 5ms
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::12::liquibase
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - Table working_hours created
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::12::liquibase ran successfully in 10ms
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::13::liquibase
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - Table salary created
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::13::liquibase ran successfully in 10ms
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::14::liquibase
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - Table user created
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::14::liquibase ran successfully in 7ms
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::15::liquibase
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - Table permission created
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::15::liquibase ran successfully in 7ms
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::16::liquibase
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - Table role_permission created
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::16::liquibase ran successfully in 10ms
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::17::liquibase
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::17::liquibase ran successfully in 83ms
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::1::liquibase
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - New row inserted into personel_type
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - New row inserted into personel_type
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - New row inserted into personel_type
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::1::liquibase ran successfully in 11ms
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::2::liquibase
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - New row inserted into building
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - New row inserted into building
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - New row inserted into building
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::2::liquibase ran successfully in 10ms
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::3::liquibase
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - Data loaded from 'db/changelog/data/floor-data.csv' into table 'floor'
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::3::liquibase ran successfully in 63ms
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::4::liquibase
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - New row inserted into personel
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - New row inserted into personel
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - New row inserted into personel
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::4::liquibase ran successfully in 7ms
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::5::liquibase
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - New row inserted into unit
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - New row inserted into unit
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - New row inserted into unit
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::5::liquibase ran successfully in 7ms
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::6::liquibase
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - New row inserted into personel_unit
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - New row inserted into personel_unit
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - New row inserted into personel_unit
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::6::liquibase ran successfully in 8ms
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::7::liquibase
2025-06-20 06:26:16 [restartedMain] ERROR liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::7::liquibase encountered an exception.
liquibase.exception.DatabaseException: HATA: "gate" tablosu zerindeki ekleme veya gncelleme ilemi "fk_gate_unit" foreign key kstlamasn ihlal ediyor
  Ayrnt: "unit" tablosunda (fk_unit_id)=(4) anahtar mevcut deildir. [Failed SQL: (0) INSERT INTO dbpersonel.gate (gate_id, gate_name, main_entrance, fk_unit_id) VALUES (1, 'Gate Main Entrance', TRUE, 4),(2, 'Gate 1', FALSE, 4),(3, 'Gate 2', FALSE, 3),(4, 'Gate 3', FALSE, 2),(5, 'Gate 4', FALSE, 1),(6, 'Gate 5', FALSE, 1),(7, 'Gate 6', FALSE, 3),(8, 'Gate 7', FALSE, 1),(9, 'Gate 8', FALSE, 2),(10, 'Gate 9', FALSE, 1),(11, 'Gate 10', FALSE, 4),(12, 'Gate 11', FALSE, 1 );]
	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.java:497)
	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:83)
	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:185)
	at liquibase.executor.AbstractExecutor.execute(AbstractExecutor.java:141)
	at liquibase.database.AbstractJdbcDatabase.executeStatements(AbstractJdbcDatabase.java:1189)
	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:776)
	at liquibase.changelog.visitor.UpdateVisitor.executeAcceptedChange(UpdateVisitor.java:126)
	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:70)
	at liquibase.changelog.ChangeLogIterator.lambda$run$0(ChangeLogIterator.java:131)
	at liquibase.Scope.lambda$child$0(Scope.java:191)
	at liquibase.Scope.child(Scope.java:200)
	at liquibase.Scope.child(Scope.java:190)
	at liquibase.Scope.child(Scope.java:169)
	at liquibase.changelog.ChangeLogIterator.lambda$run$1(ChangeLogIterator.java:120)
	at liquibase.Scope.lambda$child$0(Scope.java:191)
	at liquibase.Scope.child(Scope.java:200)
	at liquibase.Scope.child(Scope.java:190)
	at liquibase.Scope.child(Scope.java:169)
	at liquibase.Scope.child(Scope.java:257)
	at liquibase.Scope.child(Scope.java:261)
	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:89)
	at liquibase.command.core.AbstractUpdateCommandStep.lambda$run$0(AbstractUpdateCommandStep.java:114)
	at liquibase.Scope.lambda$child$0(Scope.java:191)
	at liquibase.Scope.child(Scope.java:200)
	at liquibase.Scope.child(Scope.java:190)
	at liquibase.Scope.child(Scope.java:169)
	at liquibase.command.core.AbstractUpdateCommandStep.run(AbstractUpdateCommandStep.java:112)
	at liquibase.command.core.UpdateCommandStep.run(UpdateCommandStep.java:105)
	at liquibase.command.CommandScope.execute(CommandScope.java:220)
	at liquibase.Liquibase.lambda$update$0(Liquibase.java:216)
	at liquibase.Scope.lambda$child$0(Scope.java:191)
	at liquibase.Scope.child(Scope.java:200)
	at liquibase.Scope.child(Scope.java:190)
	at liquibase.Scope.child(Scope.java:169)
	at liquibase.Liquibase.runInScope(Liquibase.java:1329)
	at liquibase.Liquibase.update(Liquibase.java:205)
	at liquibase.Liquibase.update(Liquibase.java:188)
	at liquibase.integration.spring.SpringLiquibase.performUpdate(SpringLiquibase.java:305)
	at liquibase.integration.spring.SpringLiquibase.lambda$afterPropertiesSet$0(SpringLiquibase.java:257)
	at liquibase.Scope.lambda$child$0(Scope.java:191)
	at liquibase.Scope.child(Scope.java:200)
	at liquibase.Scope.child(Scope.java:190)
	at liquibase.Scope.child(Scope.java:169)
	at liquibase.Scope.child(Scope.java:257)
	at liquibase.integration.spring.SpringLiquibase.afterPropertiesSet(SpringLiquibase.java:250)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1853)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1802)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:313)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:365)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:135)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveConstructorArguments(ConstructorResolver.java:682)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:509)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1355)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1185)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:365)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:135)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyPropertyValues(AbstractAutowireCapableBeanFactory.java:1705)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1454)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:599)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1443)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1353)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:904)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:782)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:237)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1375)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1212)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1443)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1353)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:904)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:782)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:237)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1375)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1212)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:205)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.getOrderedBeansOfType(ServletContextInitializerBeans.java:211)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.addAsRegistrationBean(ServletContextInitializerBeans.java:174)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.addAsRegistrationBean(ServletContextInitializerBeans.java:169)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.addAdaptableBeans(ServletContextInitializerBeans.java:154)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.<init>(ServletContextInitializerBeans.java:87)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.getServletContextInitializerBeans(ServletWebServerApplicationContext.java:266)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.selfInitialize(ServletWebServerApplicationContext.java:240)
	at org.springframework.boot.web.embedded.tomcat.TomcatStarter.onStartup(TomcatStarter.java:52)
	at org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:4412)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1203)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1193)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at org.apache.tomcat.util.threads.InlineExecutorService.execute(InlineExecutorService.java:75)
	at java.base/java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:145)
	at org.apache.catalina.core.ContainerBase.startInternal(ContainerBase.java:749)
	at org.apache.catalina.core.StandardHost.startInternal(StandardHost.java:772)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1203)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1193)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at org.apache.tomcat.util.threads.InlineExecutorService.execute(InlineExecutorService.java:75)
	at java.base/java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:145)
	at org.apache.catalina.core.ContainerBase.startInternal(ContainerBase.java:749)
	at org.apache.catalina.core.StandardEngine.startInternal(StandardEngine.java:203)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.core.StandardService.startInternal(StandardService.java:415)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.core.StandardServer.startInternal(StandardServer.java:870)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.startup.Tomcat.start(Tomcat.java:437)
	at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.initialize(TomcatWebServer.java:128)
	at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.<init>(TomcatWebServer.java:107)
	at org.springframework.boot.web.embedded.tomcat.TomcatServletWebServerFactory.getTomcatWebServer(TomcatServletWebServerFactory.java:516)
	at org.springframework.boot.web.embedded.tomcat.TomcatServletWebServerFactory.getWebServer(TomcatServletWebServerFactory.java:222)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.createWebServer(ServletWebServerApplicationContext.java:188)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.onRefresh(ServletWebServerApplicationContext.java:162)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:619)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:754)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:456)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:335)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1363)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1352)
	at com.personneltrackingsystem.PersonelTrackingSystemApplication.main(PersonelTrackingSystemApplication.java:12)
	at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)
	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
Caused by: org.postgresql.util.PSQLException: HATA: "gate" tablosu zerindeki ekleme veya gncelleme ilemi "fk_gate_unit" foreign key kstlamasn ihlal ediyor
  Ayrnt: "unit" tablosunda (fk_unit_id)=(4) anahtar mevcut deildir.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2725)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2412)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:371)
	at org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:502)
	at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:419)
	at org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:341)
	at org.postgresql.jdbc.PgStatement.executeCachedSql(PgStatement.java:326)
	at org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:302)
	at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:297)
	at com.zaxxer.hikari.pool.ProxyStatement.execute(ProxyStatement.java:94)
	at com.zaxxer.hikari.pool.HikariProxyStatement.execute(HikariProxyStatement.java)
	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.java:491)
	... 152 common frames omitted
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.util - UPDATE SUMMARY
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.util - Run:                         23
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.util - Previously run:               0
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.util - Filtered out:                 0
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.util - -------------------------------
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.util - Total change sets:           28
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.util - Update summary generated
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.command - Update command encountered an exception.
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.lockservice - Successfully released change log lock
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.command - Logging exception.
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.ui - ERROR: Exception Details
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.ui - ERROR: Exception Primary Class:  PSQLException
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.ui - ERROR: Exception Primary Reason:  HATA: "gate" tablosu zerindeki ekleme veya gncelleme ilemi "fk_gate_unit" foreign key kstlamasn ihlal ediyor
  Ayrnt: "unit" tablosunda (fk_unit_id)=(4) anahtar mevcut deildir.
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.ui - ERROR: Exception Primary Source:  PostgreSQL 17.4
2025-06-20 06:26:16 [restartedMain] INFO  liquibase.command - Command execution complete
2025-06-20 06:26:16 [restartedMain] ERROR o.s.b.w.e.tomcat.TomcatStarter - Error starting Tomcat context. Exception: org.springframework.beans.factory.UnsatisfiedDependencyException. Message: Error creating bean with name 'jwtAuthenticationFilter' defined in file [C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes\com\personneltrackingsystem\filter\JwtAuthenticationFilter.class]: Unsatisfied dependency expressed through constructor parameter 1: Error creating bean with name 'customUserDetailsServiceImpl' defined in file [C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes\com\personneltrackingsystem\service\impl\CustomUserDetailsServiceImpl.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'userRepository' defined in com.personneltrackingsystem.repository.UserRepository defined in @EnableJpaRepositories declared on JpaRepositoriesRegistrar.EnableJpaRepositoriesConfiguration: Cannot resolve reference to bean 'jpaSharedEM_entityManagerFactory' while setting bean property 'entityManager'
2025-06-20 06:26:16 [restartedMain] INFO  o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-06-20 06:26:16 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.event-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:435)
 com.hazelcast.internal.util.executor.StripedExecutor$Worker.run(StripedExecutor.java:227)
2025-06-20 06:26:16 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.event-2] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:435)
 com.hazelcast.internal.util.executor.StripedExecutor$Worker.run(StripedExecutor.java:227)
2025-06-20 06:26:16 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.event-3] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:435)
 com.hazelcast.internal.util.executor.StripedExecutor$Worker.run(StripedExecutor.java:227)
2025-06-20 06:26:16 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.event-4] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:435)
 com.hazelcast.internal.util.executor.StripedExecutor$Worker.run(StripedExecutor.java:227)
2025-06-20 06:26:16 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.event-5] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:435)
 com.hazelcast.internal.util.executor.StripedExecutor$Worker.run(StripedExecutor.java:227)
2025-06-20 06:26:16 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.MetricsRegistry.thread-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:269)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1763)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 06:26:16 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.MetricsRegistry.thread-2] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1177)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 06:26:16 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.migration] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:269)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1763)
 java.base/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:460)
 com.hazelcast.internal.partition.impl.MigrationQueue.poll(MigrationQueue.java:48)
 com.hazelcast.internal.partition.impl.MigrationThread.doRun(MigrationThread.java:91)
 com.hazelcast.internal.partition.impl.MigrationThread.run(MigrationThread.java:66)
2025-06-20 06:26:16 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.InvocationMonitorThread] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:269)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1763)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 06:26:16 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.SlowOperationDetectorThread] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/java.lang.Thread.sleep0(Native Method)
 java.base/java.lang.Thread.sleep(Thread.java:558)
 java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
 com.hazelcast.spi.impl.operationexecutor.slowoperationdetector.SlowOperationDetector$DetectorThread.sleepInterval(SlowOperationDetector.java:280)
 com.hazelcast.spi.impl.operationexecutor.slowoperationdetector.SlowOperationDetector$DetectorThread.run(SlowOperationDetector.java:153)
2025-06-20 06:26:16 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-in-0] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:142)
 com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:292)
 com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249)
 com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111)
2025-06-20 06:26:16 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-in-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:142)
 com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:292)
 com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249)
 com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111)
2025-06-20 06:26:16 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-in-2] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:142)
 com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:292)
 com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249)
 com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111)
2025-06-20 06:26:16 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-out-0] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:142)
 com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:292)
 com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249)
 com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111)
2025-06-20 06:26:16 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-out-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:142)
 com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:292)
 com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249)
 com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111)
2025-06-20 06:26:16 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-out-2] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:142)
 com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:292)
 com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249)
 com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111)
2025-06-20 06:26:16 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.BalancerThread] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:269)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1763)
 java.base/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:460)
 com.hazelcast.internal.networking.nio.iobalancer.IOBalancerThread.run(IOBalancerThread.java:65)
2025-06-20 06:26:16 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-Acceptor] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:147)
 com.hazelcast.internal.server.tcp.TcpServerAcceptor$AcceptorIOThread.acceptLoop(TcpServerAcceptor.java:186)
 com.hazelcast.internal.server.tcp.TcpServerAcceptor$AcceptorIOThread.run(TcpServerAcceptor.java:172)
2025-06-20 06:26:16 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.TcpServer.thread-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1177)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 06:26:16 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.TcpServer.thread-2] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1177)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 06:26:16 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.TcpServer.thread-3] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1170)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 06:26:16 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.TcpServer.thread-4] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1170)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 06:26:16 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.HealthMonitor] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/java.lang.Thread.sleep0(Native Method)
 java.base/java.lang.Thread.sleep(Thread.java:558)
 java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
 com.hazelcast.internal.diagnostics.HealthMonitor$HealthMonitorThread.run(HealthMonitor.java:164)
2025-06-20 06:26:16 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [HikariPool-1 housekeeper] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:269)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1763)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 06:26:16 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [HikariPool-1 connection adder] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:269)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1763)
 java.base/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:460)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1069)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 06:26:16 [restartedMain] WARN  o.s.b.w.s.c.AnnotationConfigServletWebServerApplicationContext - Exception encountered during context initialization - cancelling refresh attempt: org.springframework.context.ApplicationContextException: Unable to start web server
2025-06-20 06:26:16 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-1 - Shutdown initiated...
2025-06-20 06:26:16 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-1 - Shutdown completed.
2025-06-20 06:26:16 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is SHUTTING_DOWN
2025-06-20 06:26:16 [restartedMain] INFO  com.hazelcast.instance.impl.Node - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Shutting down connection manager...
2025-06-20 06:26:16 [restartedMain] INFO  com.hazelcast.instance.impl.Node - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Shutting down node engine...
2025-06-20 06:26:16 [restartedMain] INFO  c.h.instance.impl.NodeExtension - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Destroying node NodeExtension.
2025-06-20 06:26:16 [restartedMain] INFO  com.hazelcast.instance.impl.Node - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Hazelcast Shutdown is completed in 27 ms.
2025-06-20 06:26:16 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is SHUTDOWN
2025-06-20 06:26:16 [restartedMain] ERROR o.s.boot.SpringApplication - Application run failed
org.springframework.context.ApplicationContextException: Unable to start web server
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.onRefresh(ServletWebServerApplicationContext.java:165)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:619)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:754)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:456)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:335)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1363)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1352)
	at com.personneltrackingsystem.PersonelTrackingSystemApplication.main(PersonelTrackingSystemApplication.java:12)
	at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)
	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
Caused by: org.springframework.boot.web.server.WebServerException: Unable to start embedded Tomcat
	at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.initialize(TomcatWebServer.java:147)
	at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.<init>(TomcatWebServer.java:107)
	at org.springframework.boot.web.embedded.tomcat.TomcatServletWebServerFactory.getTomcatWebServer(TomcatServletWebServerFactory.java:516)
	at org.springframework.boot.web.embedded.tomcat.TomcatServletWebServerFactory.getWebServer(TomcatServletWebServerFactory.java:222)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.createWebServer(ServletWebServerApplicationContext.java:188)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.onRefresh(ServletWebServerApplicationContext.java:162)
	... 11 common frames omitted
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'jwtAuthenticationFilter' defined in file [C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes\com\personneltrackingsystem\filter\JwtAuthenticationFilter.class]: Unsatisfied dependency expressed through constructor parameter 1: Error creating bean with name 'customUserDetailsServiceImpl' defined in file [C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes\com\personneltrackingsystem\service\impl\CustomUserDetailsServiceImpl.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'userRepository' defined in com.personneltrackingsystem.repository.UserRepository defined in @EnableJpaRepositories declared on JpaRepositoriesRegistrar.EnableJpaRepositoriesConfiguration: Cannot resolve reference to bean 'jpaSharedEM_entityManagerFactory' while setting bean property 'entityManager'
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:795)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:237)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1375)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1212)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:205)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.getOrderedBeansOfType(ServletContextInitializerBeans.java:211)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.addAsRegistrationBean(ServletContextInitializerBeans.java:174)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.addAsRegistrationBean(ServletContextInitializerBeans.java:169)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.addAdaptableBeans(ServletContextInitializerBeans.java:154)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.<init>(ServletContextInitializerBeans.java:87)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.getServletContextInitializerBeans(ServletWebServerApplicationContext.java:266)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.selfInitialize(ServletWebServerApplicationContext.java:240)
	at org.springframework.boot.web.embedded.tomcat.TomcatStarter.onStartup(TomcatStarter.java:52)
	at org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:4412)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1203)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1193)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at org.apache.tomcat.util.threads.InlineExecutorService.execute(InlineExecutorService.java:75)
	at java.base/java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:145)
	at org.apache.catalina.core.ContainerBase.startInternal(ContainerBase.java:749)
	at org.apache.catalina.core.StandardHost.startInternal(StandardHost.java:772)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1203)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1193)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at org.apache.tomcat.util.threads.InlineExecutorService.execute(InlineExecutorService.java:75)
	at java.base/java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:145)
	at org.apache.catalina.core.ContainerBase.startInternal(ContainerBase.java:749)
	at org.apache.catalina.core.StandardEngine.startInternal(StandardEngine.java:203)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.core.StandardService.startInternal(StandardService.java:415)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.core.StandardServer.startInternal(StandardServer.java:870)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.startup.Tomcat.start(Tomcat.java:437)
	at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.initialize(TomcatWebServer.java:128)
	... 16 common frames omitted
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'customUserDetailsServiceImpl' defined in file [C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes\com\personneltrackingsystem\service\impl\CustomUserDetailsServiceImpl.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'userRepository' defined in com.personneltrackingsystem.repository.UserRepository defined in @EnableJpaRepositories declared on JpaRepositoriesRegistrar.EnableJpaRepositoriesConfiguration: Cannot resolve reference to bean 'jpaSharedEM_entityManagerFactory' while setting bean property 'entityManager'
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:795)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:237)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1375)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1212)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1443)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1353)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:904)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:782)
	... 57 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'userRepository' defined in com.personneltrackingsystem.repository.UserRepository defined in @EnableJpaRepositories declared on JpaRepositoriesRegistrar.EnableJpaRepositoriesConfiguration: Cannot resolve reference to bean 'jpaSharedEM_entityManagerFactory' while setting bean property 'entityManager'
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:377)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:135)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyPropertyValues(AbstractAutowireCapableBeanFactory.java:1705)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1454)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:599)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1443)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1353)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:904)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:782)
	... 71 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'jpaSharedEM_entityManagerFactory': Cannot resolve reference to bean 'entityManagerFactory' while setting constructor argument
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:377)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:135)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveConstructorArguments(ConstructorResolver.java:682)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:509)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1355)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1185)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:365)
	... 85 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'liquibase' defined in class path resource [org/springframework/boot/autoconfigure/liquibase/LiquibaseAutoConfiguration$LiquibaseConfiguration.class]: liquibase.exception.CommandExecutionException: liquibase.exception.LiquibaseException: liquibase.exception.MigrationFailedException: Migration failed for changeset db/changelog/changes/002-insert-initial-data.yaml::7::liquibase:
     Reason: liquibase.exception.DatabaseException: HATA: "gate" tablosu zerindeki ekleme veya gncelleme ilemi "fk_gate_unit" foreign key kstlamasn ihlal ediyor
  Ayrnt: "unit" tablosunda (fk_unit_id)=(4) anahtar mevcut deildir. [Failed SQL: (0) INSERT INTO dbpersonel.gate (gate_id, gate_name, main_entrance, fk_unit_id) VALUES (1, 'Gate Main Entrance', TRUE, 4),(2, 'Gate 1', FALSE, 4),(3, 'Gate 2', FALSE, 3),(4, 'Gate 3', FALSE, 2),(5, 'Gate 4', FALSE, 1),(6, 'Gate 5', FALSE, 1),(7, 'Gate 6', FALSE, 3),(8, 'Gate 7', FALSE, 1),(9, 'Gate 8', FALSE, 2),(10, 'Gate 9', FALSE, 1),(11, 'Gate 10', FALSE, 4),(12, 'Gate 11', FALSE, 1 );]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1806)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:313)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:365)
	... 97 common frames omitted
Caused by: liquibase.exception.UnexpectedLiquibaseException: liquibase.exception.CommandExecutionException: liquibase.exception.LiquibaseException: liquibase.exception.MigrationFailedException: Migration failed for changeset db/changelog/changes/002-insert-initial-data.yaml::7::liquibase:
     Reason: liquibase.exception.DatabaseException: HATA: "gate" tablosu zerindeki ekleme veya gncelleme ilemi "fk_gate_unit" foreign key kstlamasn ihlal ediyor
  Ayrnt: "unit" tablosunda (fk_unit_id)=(4) anahtar mevcut deildir. [Failed SQL: (0) INSERT INTO dbpersonel.gate (gate_id, gate_name, main_entrance, fk_unit_id) VALUES (1, 'Gate Main Entrance', TRUE, 4),(2, 'Gate 1', FALSE, 4),(3, 'Gate 2', FALSE, 3),(4, 'Gate 3', FALSE, 2),(5, 'Gate 4', FALSE, 1),(6, 'Gate 5', FALSE, 1),(7, 'Gate 6', FALSE, 3),(8, 'Gate 7', FALSE, 1),(9, 'Gate 8', FALSE, 2),(10, 'Gate 9', FALSE, 1),(11, 'Gate 10', FALSE, 4),(12, 'Gate 11', FALSE, 1 );]
	at liquibase.integration.spring.SpringLiquibase.afterPropertiesSet(SpringLiquibase.java:267)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1853)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1802)
	... 106 common frames omitted
Caused by: liquibase.exception.CommandExecutionException: liquibase.exception.LiquibaseException: liquibase.exception.MigrationFailedException: Migration failed for changeset db/changelog/changes/002-insert-initial-data.yaml::7::liquibase:
     Reason: liquibase.exception.DatabaseException: HATA: "gate" tablosu zerindeki ekleme veya gncelleme ilemi "fk_gate_unit" foreign key kstlamasn ihlal ediyor
  Ayrnt: "unit" tablosunda (fk_unit_id)=(4) anahtar mevcut deildir. [Failed SQL: (0) INSERT INTO dbpersonel.gate (gate_id, gate_name, main_entrance, fk_unit_id) VALUES (1, 'Gate Main Entrance', TRUE, 4),(2, 'Gate 1', FALSE, 4),(3, 'Gate 2', FALSE, 3),(4, 'Gate 3', FALSE, 2),(5, 'Gate 4', FALSE, 1),(6, 'Gate 5', FALSE, 1),(7, 'Gate 6', FALSE, 3),(8, 'Gate 7', FALSE, 1),(9, 'Gate 8', FALSE, 2),(10, 'Gate 9', FALSE, 1),(11, 'Gate 10', FALSE, 4),(12, 'Gate 11', FALSE, 1 );]
	at liquibase.command.CommandScope.execute(CommandScope.java:258)
	at liquibase.Liquibase.lambda$update$0(Liquibase.java:216)
	at liquibase.Scope.lambda$child$0(Scope.java:191)
	at liquibase.Scope.child(Scope.java:200)
	at liquibase.Scope.child(Scope.java:190)
	at liquibase.Scope.child(Scope.java:169)
	at liquibase.Liquibase.runInScope(Liquibase.java:1329)
	at liquibase.Liquibase.update(Liquibase.java:205)
	at liquibase.Liquibase.update(Liquibase.java:188)
	at liquibase.integration.spring.SpringLiquibase.performUpdate(SpringLiquibase.java:305)
	at liquibase.integration.spring.SpringLiquibase.lambda$afterPropertiesSet$0(SpringLiquibase.java:257)
	at liquibase.Scope.lambda$child$0(Scope.java:191)
	at liquibase.Scope.child(Scope.java:200)
	at liquibase.Scope.child(Scope.java:190)
	at liquibase.Scope.child(Scope.java:169)
	at liquibase.Scope.child(Scope.java:257)
	at liquibase.integration.spring.SpringLiquibase.afterPropertiesSet(SpringLiquibase.java:250)
	... 108 common frames omitted
Caused by: liquibase.exception.LiquibaseException: liquibase.exception.MigrationFailedException: Migration failed for changeset db/changelog/changes/002-insert-initial-data.yaml::7::liquibase:
     Reason: liquibase.exception.DatabaseException: HATA: "gate" tablosu zerindeki ekleme veya gncelleme ilemi "fk_gate_unit" foreign key kstlamasn ihlal ediyor
  Ayrnt: "unit" tablosunda (fk_unit_id)=(4) anahtar mevcut deildir. [Failed SQL: (0) INSERT INTO dbpersonel.gate (gate_id, gate_name, main_entrance, fk_unit_id) VALUES (1, 'Gate Main Entrance', TRUE, 4),(2, 'Gate 1', FALSE, 4),(3, 'Gate 2', FALSE, 3),(4, 'Gate 3', FALSE, 2),(5, 'Gate 4', FALSE, 1),(6, 'Gate 5', FALSE, 1),(7, 'Gate 6', FALSE, 3),(8, 'Gate 7', FALSE, 1),(9, 'Gate 8', FALSE, 2),(10, 'Gate 9', FALSE, 1),(11, 'Gate 10', FALSE, 4),(12, 'Gate 11', FALSE, 1 );]
	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:148)
	at liquibase.command.core.AbstractUpdateCommandStep.lambda$run$0(AbstractUpdateCommandStep.java:114)
	at liquibase.Scope.lambda$child$0(Scope.java:191)
	at liquibase.Scope.child(Scope.java:200)
	at liquibase.Scope.child(Scope.java:190)
	at liquibase.Scope.child(Scope.java:169)
	at liquibase.command.core.AbstractUpdateCommandStep.run(AbstractUpdateCommandStep.java:112)
	at liquibase.command.core.UpdateCommandStep.run(UpdateCommandStep.java:105)
	at liquibase.command.CommandScope.execute(CommandScope.java:220)
	... 124 common frames omitted
Caused by: liquibase.exception.MigrationFailedException: Migration failed for changeset db/changelog/changes/002-insert-initial-data.yaml::7::liquibase:
     Reason: liquibase.exception.DatabaseException: HATA: "gate" tablosu zerindeki ekleme veya gncelleme ilemi "fk_gate_unit" foreign key kstlamasn ihlal ediyor
  Ayrnt: "unit" tablosunda (fk_unit_id)=(4) anahtar mevcut deildir. [Failed SQL: (0) INSERT INTO dbpersonel.gate (gate_id, gate_name, main_entrance, fk_unit_id) VALUES (1, 'Gate Main Entrance', TRUE, 4),(2, 'Gate 1', FALSE, 4),(3, 'Gate 2', FALSE, 3),(4, 'Gate 3', FALSE, 2),(5, 'Gate 4', FALSE, 1),(6, 'Gate 5', FALSE, 1),(7, 'Gate 6', FALSE, 3),(8, 'Gate 7', FALSE, 1),(9, 'Gate 8', FALSE, 2),(10, 'Gate 9', FALSE, 1),(11, 'Gate 10', FALSE, 4),(12, 'Gate 11', FALSE, 1 );]
	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:820)
	at liquibase.changelog.visitor.UpdateVisitor.executeAcceptedChange(UpdateVisitor.java:126)
	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:70)
	at liquibase.changelog.ChangeLogIterator.lambda$run$0(ChangeLogIterator.java:131)
	at liquibase.Scope.lambda$child$0(Scope.java:191)
	at liquibase.Scope.child(Scope.java:200)
	at liquibase.Scope.child(Scope.java:190)
	at liquibase.Scope.child(Scope.java:169)
	at liquibase.changelog.ChangeLogIterator.lambda$run$1(ChangeLogIterator.java:120)
	at liquibase.Scope.lambda$child$0(Scope.java:191)
	at liquibase.Scope.child(Scope.java:200)
	at liquibase.Scope.child(Scope.java:190)
	at liquibase.Scope.child(Scope.java:169)
	at liquibase.Scope.child(Scope.java:257)
	at liquibase.Scope.child(Scope.java:261)
	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:89)
	... 132 common frames omitted
Caused by: liquibase.exception.DatabaseException: HATA: "gate" tablosu zerindeki ekleme veya gncelleme ilemi "fk_gate_unit" foreign key kstlamasn ihlal ediyor
  Ayrnt: "unit" tablosunda (fk_unit_id)=(4) anahtar mevcut deildir. [Failed SQL: (0) INSERT INTO dbpersonel.gate (gate_id, gate_name, main_entrance, fk_unit_id) VALUES (1, 'Gate Main Entrance', TRUE, 4),(2, 'Gate 1', FALSE, 4),(3, 'Gate 2', FALSE, 3),(4, 'Gate 3', FALSE, 2),(5, 'Gate 4', FALSE, 1),(6, 'Gate 5', FALSE, 1),(7, 'Gate 6', FALSE, 3),(8, 'Gate 7', FALSE, 1),(9, 'Gate 8', FALSE, 2),(10, 'Gate 9', FALSE, 1),(11, 'Gate 10', FALSE, 4),(12, 'Gate 11', FALSE, 1 );]
	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.java:497)
	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:83)
	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:185)
	at liquibase.executor.AbstractExecutor.execute(AbstractExecutor.java:141)
	at liquibase.database.AbstractJdbcDatabase.executeStatements(AbstractJdbcDatabase.java:1189)
	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:776)
	... 147 common frames omitted
Caused by: org.postgresql.util.PSQLException: HATA: "gate" tablosu zerindeki ekleme veya gncelleme ilemi "fk_gate_unit" foreign key kstlamasn ihlal ediyor
  Ayrnt: "unit" tablosunda (fk_unit_id)=(4) anahtar mevcut deildir.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2725)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2412)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:371)
	at org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:502)
	at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:419)
	at org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:341)
	at org.postgresql.jdbc.PgStatement.executeCachedSql(PgStatement.java:326)
	at org.postgresql.jdbc.PgStatement.executeWithFlags(PgStatement.java:302)
	at org.postgresql.jdbc.PgStatement.execute(PgStatement.java:297)
	at com.zaxxer.hikari.pool.ProxyStatement.execute(ProxyStatement.java:94)
	at com.zaxxer.hikari.pool.HikariProxyStatement.execute(HikariProxyStatement.java)
	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.java:491)
	... 152 common frames omitted
2025-06-20 06:32:56 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - Starting PersonelTrackingSystemApplication using Java 21.0.7 with PID 6280 (C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes started by Lenovo in C:\Users\Lenovo\Desktop\personnel-tracking-system)
2025-06-20 06:32:56 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - No active profile set, falling back to 1 default profile: "default"
2025-06-20 06:32:59 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-20 06:32:59 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-20 06:32:59 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.30]
2025-06-20 06:32:59 [restartedMain] INFO  o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-20 06:32:59 [restartedMain] WARN  c.h.i.impl.HazelcastInstanceFactory - Hazelcast is starting in a Java modular environment (Java 9 and newer) but without proper access to required Java packages. Use additional Java arguments to provide Hazelcast access to Java internal API. The internal API access is used to get the best performance results. Arguments to be used:
 --add-modules java.se --add-exports java.base/jdk.internal.ref=ALL-UNNAMED --add-opens java.base/java.lang=ALL-UNNAMED --add-opens java.base/sun.nio.ch=ALL-UNNAMED --add-opens java.management/sun.management=ALL-UNNAMED --add-opens jdk.management/com.sun.management.internal=ALL-UNNAMED
2025-06-20 06:32:59 [restartedMain] INFO  c.hazelcast.instance.AddressPicker - [LOCAL] [pts-cluster] [5.4.0] Interfaces is disabled, trying to pick one address from TCP-IP config addresses: [127.0.0.1]
2025-06-20 06:32:59 [restartedMain] INFO  com.hazelcast.system.logo - [127.0.0.1]:5701 [pts-cluster] [5.4.0] 
	o    o     o     o---o   o--o o      o---o     o     o----o o--o--o
	|    |    / \       /         |     /         / \    |         |   
	o----o       o     o   o----o |    o             o   o----o    |   
	|    |  *     \   /           |     \       *     \       |    |   
	o    o *       o o---o   o--o o----o o---o *       o o----o    o   
2025-06-20 06:32:59 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Copyright (c) 2008-2024, Hazelcast, Inc. All Rights Reserved.
2025-06-20 06:32:59 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Hazelcast Platform 5.4.0 (20240415) starting at [127.0.0.1]:5701
2025-06-20 06:32:59 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Cluster name: pts-cluster
2025-06-20 06:32:59 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Integrity Checker is disabled. Fail-fast on corrupted executables will not be performed. For more information, see the documentation for Integrity Checker.
2025-06-20 06:32:59 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] The Jet engine is disabled.
To enable the Jet engine on the members, do one of the following:
  - Change member config using Java API: config.getJetConfig().setEnabled(true)
  - Change XML/YAML configuration property: Set hazelcast.jet.enabled to true
  - Add system property: -Dhz.jet.enabled=true (for Hazelcast embedded, works only when loading config via Config.load)
  - Add environment variable: HZ_JET_ENABLED=true (recommended when running container image. For Hazelcast embedded, works only when loading config via Config.load)
2025-06-20 06:33:00 [restartedMain] INFO  com.hazelcast.system.security - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Enable DEBUG/FINE log level for log category com.hazelcast.system.security  or use -Dhazelcast.security.recommendations system property to see security recommendations and the status of current config.
2025-06-20 06:33:00 [restartedMain] INFO  com.hazelcast.instance.impl.Node - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Using TCP/IP discovery
2025-06-20 06:33:00 [restartedMain] WARN  com.hazelcast.cp.CPSubsystem - [127.0.0.1]:5701 [pts-cluster] [5.4.0] CP Subsystem is not enabled. CP data structures will operate in UNSAFE mode! Please note that UNSAFE mode will not provide strong consistency guarantees.
2025-06-20 06:33:00 [restartedMain] INFO  c.h.internal.diagnostics.Diagnostics - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Diagnostics disabled. To enable add -Dhazelcast.diagnostics.enabled=true to the JVM arguments.
2025-06-20 06:33:00 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is STARTING
2025-06-20 06:33:00 [hz.personnel-tracking-system.cached.thread-2] INFO  c.h.i.cluster.impl.TcpIpJoiner - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5703 is added to the blacklist.
2025-06-20 06:33:00 [hz.personnel-tracking-system.cached.thread-3] INFO  c.h.i.cluster.impl.TcpIpJoiner - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5702 is added to the blacklist.
2025-06-20 06:33:01 [restartedMain] INFO  c.h.internal.cluster.ClusterService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] 

Members {size:1, ver:1} [
	Member [127.0.0.1]:5701 - 34d4f548-c942-4b32-8cc0-105e177ce233 this
]

2025-06-20 06:33:01 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is STARTED
2025-06-20 06:33:02 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-1 - Starting...
2025-06-20 06:33:02 [restartedMain] INFO  com.zaxxer.hikari.pool.HikariPool - HikariPool-1 - Added connection org.postgresql.jdbc.PgConnection@4b283c8a
2025-06-20 06:33:02 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-1 - Start completed.
2025-06-20 06:33:02 [restartedMain] INFO  liquibase.changelog - Creating database history table with name: dbpersonel.databasechangelog
2025-06-20 06:33:02 [restartedMain] INFO  liquibase.changelog - Reading from dbpersonel.databasechangelog
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.lockservice - Successfully acquired change log lock
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.command - Using deploymentId: 0390383096
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Reading from dbpersonel.databasechangelog
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::1::liquibase
2025-06-20 06:33:03 [restartedMain] WARN  liquibase.executor - "dbpersonel" emas zaten mevcut, atlanyor
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::1::liquibase ran successfully in 13ms
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::2::liquibase
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::2::liquibase ran successfully in 96ms
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::3::liquibase
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Table personel_type created
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::3::liquibase ran successfully in 10ms
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::4::liquibase
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Table building created
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::4::liquibase ran successfully in 4ms
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::5::liquibase
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Table floor created
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::5::liquibase ran successfully in 10ms
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::6::liquibase
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Table personel created
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::6::liquibase ran successfully in 9ms
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::7::liquibase
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Table unit created
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::7::liquibase ran successfully in 10ms
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::8::liquibase
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Table personel_unit created
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::8::liquibase ran successfully in 7ms
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::9::liquibase
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Table gate created
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::9::liquibase ran successfully in 5ms
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::10::liquibase
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Table turnstile created
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::10::liquibase ran successfully in 6ms
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::11::liquibase
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Table turnstile_registration_log created
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::11::liquibase ran successfully in 7ms
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::12::liquibase
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Table working_hours created
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::12::liquibase ran successfully in 8ms
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::13::liquibase
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Table salary created
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::13::liquibase ran successfully in 15ms
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::14::liquibase
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Table user created
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::14::liquibase ran successfully in 16ms
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::15::liquibase
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Table permission created
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::15::liquibase ran successfully in 11ms
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::16::liquibase
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Table role_permission created
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::16::liquibase ran successfully in 8ms
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::17::liquibase
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::17::liquibase ran successfully in 59ms
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::1::liquibase
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - New row inserted into personel_type
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - New row inserted into personel_type
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - New row inserted into personel_type
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::1::liquibase ran successfully in 8ms
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::2::liquibase
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - New row inserted into building
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - New row inserted into building
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - New row inserted into building
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::2::liquibase ran successfully in 6ms
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::3::liquibase
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Data loaded from 'db/changelog/data/floor-data.csv' into table 'floor'
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::3::liquibase ran successfully in 48ms
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::4::liquibase
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - New row inserted into personel
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - New row inserted into personel
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - New row inserted into personel
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::4::liquibase ran successfully in 7ms
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::5::liquibase
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - New row inserted into unit
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - New row inserted into unit
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - New row inserted into unit
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::5::liquibase ran successfully in 7ms
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::6::liquibase
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - New row inserted into personel_unit
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - New row inserted into personel_unit
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - New row inserted into personel_unit
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::6::liquibase ran successfully in 9ms
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::7::liquibase
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Data loaded from 'db/changelog/data/gate-data.csv' into table 'gate'
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::7::liquibase ran successfully in 14ms
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::8::liquibase
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - Data loaded from 'db/changelog/data/turnstile-data.csv' into table 'turnstile'
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::8::liquibase ran successfully in 13ms
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::9::liquibase
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - New row inserted into working_hours
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - New row inserted into working_hours
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - New row inserted into working_hours
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::9::liquibase ran successfully in 8ms
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::10::liquibase
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - New row inserted into permission
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - New row inserted into permission
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - New row inserted into permission
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - New row inserted into permission
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - New row inserted into permission
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::10::liquibase ran successfully in 11ms
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::11::liquibase
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - New row inserted into role_permission
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - New row inserted into role_permission
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - New row inserted into role_permission
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - New row inserted into role_permission
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - New row inserted into role_permission
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::11::liquibase ran successfully in 11ms
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.util - UPDATE SUMMARY
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.util - Run:                         28
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.util - Previously run:               0
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.util - Filtered out:                 0
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.util - -------------------------------
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.util - Total change sets:           28
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.util - Update summary generated
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.command - Update command completed successfully.
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.ui - Liquibase: Update has been successful. Rows affected: 124
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.lockservice - Successfully released change log lock
2025-06-20 06:33:03 [restartedMain] INFO  liquibase.command - Command execution complete
2025-06-20 06:33:04 [restartedMain] WARN  org.hibernate.orm.deprecation - HHH90000025: PostgreSQLDialect does not need to be specified explicitly using 'hibernate.dialect' (remove the property setting and it will be selected by default)
2025-06-20 06:33:05 [restartedMain] WARN  o.s.s.c.a.a.c.InitializeUserDetailsBeanManagerConfigurer$InitializeUserDetailsManagerConfigurer - Global AuthenticationManager configured with an AuthenticationProvider bean. UserDetailsService beans will not be used for username/password login. Consider removing the AuthenticationProvider bean. Alternatively, consider using the UserDetailsService in a manually instantiated DaoAuthenticationProvider.
2025-06-20 06:33:06 [restartedMain] INFO  c.p.s.impl.PersonelTypeServiceImpl - Checking for default personnel types...
2025-06-20 06:33:06 [restartedMain] INFO  c.p.s.impl.PersonelTypeServiceImpl - Personnel types already exist in the database.
2025-06-20 06:33:07 [restartedMain] WARN  o.s.b.a.o.j.JpaBaseConfiguration$JpaWebConfiguration - spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
2025-06-20 06:33:08 [restartedMain] INFO  o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = personneltrackingsystem-admin-0
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-20 06:33:08 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 06:33:08 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 06:33:08 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750390388098
2025-06-20 06:33:08 [kafka-admin-client-thread | personneltrackingsystem-admin-0] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for personneltrackingsystem-admin-0 unregistered
2025-06-20 06:33:08 [kafka-admin-client-thread | personneltrackingsystem-admin-0] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-20 06:33:08 [kafka-admin-client-thread | personneltrackingsystem-admin-0] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-20 06:33:08 [kafka-admin-client-thread | personneltrackingsystem-admin-0] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-20 06:33:08 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-20 06:33:08 [restartedMain] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pts-group-reset-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pts-group-reset
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-06-20 06:33:08 [restartedMain] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 06:33:08 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 06:33:08 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 06:33:08 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750390388591
2025-06-20 06:33:08 [restartedMain] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Subscribed to topic(s): turnstile-request
2025-06-20 06:33:08 [restartedMain] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pts-group-reset-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pts-group-reset
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-06-20 06:33:08 [restartedMain] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 06:33:08 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 06:33:08 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 06:33:08 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 06:33:08 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750390388622
2025-06-20 06:33:08 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2025-06-20 06:33:08 [restartedMain] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Subscribed to topic(s): email-notification
2025-06-20 06:33:08 [restartedMain] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pts-group-reset-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pts-group-reset
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-06-20 06:33:08 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:33:08 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 06:33:08 [restartedMain] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 06:33:08 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2025-06-20 06:33:08 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:33:08 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 06:33:08 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 06:33:08 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750390388688
2025-06-20 06:33:08 [restartedMain] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Subscribed to topic(s): turnstile-passage
2025-06-20 06:33:08 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Request joining group due to: need to re-join with the given member-id: consumer-pts-group-reset-2-9a94534f-bc25-45b5-b1de-aaeb6f19ffc3
2025-06-20 06:33:08 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Request joining group due to: need to re-join with the given member-id: consumer-pts-group-reset-1-89f858a6-6560-40b1-b074-2fe834d96b72
2025-06-20 06:33:08 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:33:08 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:33:08 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=7, memberId='consumer-pts-group-reset-1-89f858a6-6560-40b1-b074-2fe834d96b72', protocol='range'}
2025-06-20 06:33:08 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=7, memberId='consumer-pts-group-reset-2-9a94534f-bc25-45b5-b1de-aaeb6f19ffc3', protocol='range'}
2025-06-20 06:33:08 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Finished assignment for group at generation 7: {consumer-pts-group-reset-1-89f858a6-6560-40b1-b074-2fe834d96b72=Assignment(partitions=[turnstile-request-0]), consumer-pts-group-reset-2-9a94534f-bc25-45b5-b1de-aaeb6f19ffc3=Assignment(partitions=[email-notification-0])}
2025-06-20 06:33:08 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 06:33:08 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=7, memberId='consumer-pts-group-reset-1-89f858a6-6560-40b1-b074-2fe834d96b72', protocol='range'}
2025-06-20 06:33:08 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2025-06-20 06:33:08 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[turnstile-request-0])
2025-06-20 06:33:08 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:33:08 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=7, memberId='consumer-pts-group-reset-2-9a94534f-bc25-45b5-b1de-aaeb6f19ffc3', protocol='range'}
2025-06-20 06:33:08 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Adding newly assigned partitions: turnstile-request-0
2025-06-20 06:33:08 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[email-notification-0])
2025-06-20 06:33:08 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Adding newly assigned partitions: email-notification-0
2025-06-20 06:33:08 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Request joining group due to: need to re-join with the given member-id: consumer-pts-group-reset-3-b95baa30-fd7f-47f5-a350-9ae4be41b8eb
2025-06-20 06:33:08 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:33:08 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition email-notification-0 to the committed offset FetchPosition{offset=1, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 06:33:08 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition turnstile-request-0 to the committed offset FetchPosition{offset=1, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 06:33:08 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - Started PersonelTrackingSystemApplication in 12.587 seconds (process running for 13.23)
2025-06-20 06:33:11 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Request joining group due to: group is already rebalancing
2025-06-20 06:33:11 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Request joining group due to: group is already rebalancing
2025-06-20 06:33:11 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Revoke previously assigned partitions turnstile-request-0
2025-06-20 06:33:11 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Revoke previously assigned partitions email-notification-0
2025-06-20 06:33:11 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:33:11 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:33:11 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=8, memberId='consumer-pts-group-reset-3-b95baa30-fd7f-47f5-a350-9ae4be41b8eb', protocol='range'}
2025-06-20 06:33:11 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=8, memberId='consumer-pts-group-reset-2-9a94534f-bc25-45b5-b1de-aaeb6f19ffc3', protocol='range'}
2025-06-20 06:33:11 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=8, memberId='consumer-pts-group-reset-1-89f858a6-6560-40b1-b074-2fe834d96b72', protocol='range'}
2025-06-20 06:33:11 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Finished assignment for group at generation 8: {consumer-pts-group-reset-1-89f858a6-6560-40b1-b074-2fe834d96b72=Assignment(partitions=[turnstile-request-0]), consumer-pts-group-reset-2-9a94534f-bc25-45b5-b1de-aaeb6f19ffc3=Assignment(partitions=[email-notification-0]), consumer-pts-group-reset-3-b95baa30-fd7f-47f5-a350-9ae4be41b8eb=Assignment(partitions=[turnstile-passage-0])}
2025-06-20 06:33:11 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=8, memberId='consumer-pts-group-reset-2-9a94534f-bc25-45b5-b1de-aaeb6f19ffc3', protocol='range'}
2025-06-20 06:33:11 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=8, memberId='consumer-pts-group-reset-1-89f858a6-6560-40b1-b074-2fe834d96b72', protocol='range'}
2025-06-20 06:33:11 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=8, memberId='consumer-pts-group-reset-3-b95baa30-fd7f-47f5-a350-9ae4be41b8eb', protocol='range'}
2025-06-20 06:33:11 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[email-notification-0])
2025-06-20 06:33:11 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[turnstile-request-0])
2025-06-20 06:33:11 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[turnstile-passage-0])
2025-06-20 06:33:11 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Adding newly assigned partitions: email-notification-0
2025-06-20 06:33:11 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Adding newly assigned partitions: turnstile-request-0
2025-06-20 06:33:11 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Adding newly assigned partitions: turnstile-passage-0
2025-06-20 06:33:11 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition email-notification-0 to the committed offset FetchPosition{offset=1, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 06:33:11 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition turnstile-request-0 to the committed offset FetchPosition{offset=1, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 06:33:11 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition turnstile-passage-0 to the committed offset FetchPosition{offset=1, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 06:33:43 [http-nio-8080-exec-1] INFO  o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-06-20 06:33:44 [http-nio-8080-exec-1] INFO  c.p.c.impl.TurnstileControllerImpl - Turnstile passage request received: DtoTurnstilePassageFullRequest(wantedToEnterTurnstileId=1, personelId=1, operationType=IN, operationTimeStr=09:17:35)
2025-06-20 06:33:44 [http-nio-8080-exec-1] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - No previous operation found for personel 1 on turnstile 1
2025-06-20 06:33:44 [http-nio-8080-exec-1] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Turnstile passage validation - Personel: 1, Turnstile: 1, Requested Operation: IN, Last Operation: null
2025-06-20 06:33:44 [http-nio-8080-exec-1] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Operation validated successfully: IN (Personel: 1, Turnstile: 1)
2025-06-20 06:33:44 [http-nio-8080-exec-1] INFO  c.p.s.k.i.KafkaProducerServiceImpl - Sending turnstile request event to Kafka: TurnstileRequestEvent(wantedToEnterTurnstileId=1, personelId=1, operationType=IN, operationTimeStr=09:17:35)
2025-06-20 06:33:44 [http-nio-8080-exec-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = personneltrackingsystem-producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 3
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.springframework.kafka.support.serializer.JsonSerializer

2025-06-20 06:33:44 [http-nio-8080-exec-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 06:33:44 [http-nio-8080-exec-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=personneltrackingsystem-producer-1] Instantiated an idempotent producer.
2025-06-20 06:33:44 [http-nio-8080-exec-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 06:33:44 [http-nio-8080-exec-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 06:33:44 [http-nio-8080-exec-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750390424417
2025-06-20 06:33:44 [kafka-producer-network-thread | personneltrackingsystem-producer-1] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=personneltrackingsystem-producer-1] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 06:33:44 [kafka-producer-network-thread | personneltrackingsystem-producer-1] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=personneltrackingsystem-producer-1] ProducerId set to 1 with epoch 0
2025-06-20 06:33:44 [kafka-producer-network-thread | personneltrackingsystem-producer-1] INFO  c.p.s.k.i.KafkaProducerServiceImpl - Turnstile request event sent successfully to topic: turnstile-request, partition: 0, offset: 1
2025-06-20 06:33:44 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.k.i.TurnstileRequestConsumerServiceImpl - Received turnstile request event: TurnstileRequestEvent(wantedToEnterTurnstileId=1, personelId=1, operationType=IN, operationTimeStr=09:17:35)
2025-06-20 06:33:45 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.c.i.PersonelCacheServiceImpl - Personnel found in cache with ID: 1
2025-06-20 06:33:45 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Saved turnstile registration log: Personel 1 - Turnstile 1 - Operation IN
2025-06-20 06:33:45 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.k.i.KafkaProducerServiceImpl - Sending turnstile passage event to Kafka: TurnstilePassageEvent(personelId=1, personelName=Arif zcan, personelEmail=zcanarif@gmail.com, turnstileId=1, turnstileName=Turnstile 1, passageTime=2025-06-20T09:17:35, operationType=IN, recipientEmail=arifozcan576@gmail.com, recipientName=Beyzanur Durmus, isAdminNotification=true, isLateArrival=true, minutesLate=17)
2025-06-20 06:33:45 [kafka-producer-network-thread | personneltrackingsystem-producer-1] INFO  c.p.s.k.i.KafkaProducerServiceImpl - Turnstile passage event sent successfully to topic: turnstile-passage, partition: 0, offset: 1
2025-06-20 06:33:45 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.k.i.TurnstileRequestConsumerServiceImpl - Turnstile request event processed successfully
2025-06-20 06:33:45 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  c.p.s.k.i.TurnstilePassageConsumerServiceImpl - Received turnstile passage event: TurnstilePassageEvent(personelId=1, personelName=Arif zcan, personelEmail=zcanarif@gmail.com, turnstileId=1, turnstileName=Turnstile 1, passageTime=2025-06-20T09:17:35, operationType=IN, recipientEmail=arifozcan576@gmail.com, recipientName=Beyzanur Durmus, isAdminNotification=true, isLateArrival=true, minutesLate=17)
2025-06-20 06:33:45 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  c.p.s.k.i.KafkaProducerServiceImpl - Sending email event to Kafka: EmailEvent(recipientEmail=arifozcan576@gmail.com, recipientName=Beyzanur Durmus, subject=Late Arrival Notification - Arif zcan, message=Dear Beyzanur Durmus,

This is to inform you that Arif zcan has arrived late to work today.

Details:
- Personnel Name: Arif zcan
- Arrival Time: 2025-06-20 09:17:35
- Minutes Late: 17 minutes
- Entrance: Turnstile 1


This is an automated message from the Personnel Tracking System.
, timestamp=2025-06-20T06:33:45.298589800)
2025-06-20 06:33:45 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  c.p.s.k.i.TurnstilePassageConsumerServiceImpl - Turnstile passage event processed successfully
2025-06-20 06:33:45 [kafka-producer-network-thread | personneltrackingsystem-producer-1] INFO  c.p.s.k.i.KafkaProducerServiceImpl - Email event sent successfully to topic: email-notification, partition: 0, offset: 1
2025-06-20 06:33:45 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  c.p.s.k.i.EmailConsumerServiceImpl - Received email event: EmailEvent(recipientEmail=arifozcan576@gmail.com, recipientName=Beyzanur Durmus, subject=Late Arrival Notification - Arif zcan, message=Dear Beyzanur Durmus,

This is to inform you that Arif zcan has arrived late to work today.

Details:
- Personnel Name: Arif zcan
- Arrival Time: 2025-06-20 09:17:35
- Minutes Late: 17 minutes
- Entrance: Turnstile 1


This is an automated message from the Personnel Tracking System.
, timestamp=2025-06-20T06:33:45.298589800)
2025-06-20 06:33:47 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  c.p.s.k.i.EmailConsumerServiceImpl - Email sent successfully to arifozcan576@gmail.com
2025-06-20 06:33:47 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  c.p.s.k.i.EmailConsumerServiceImpl - Email event processed successfully
2025-06-20 06:46:21 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - Starting PersonelTrackingSystemApplication using Java 21.0.7 with PID 16928 (C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes started by Lenovo in C:\Users\Lenovo\Desktop\personnel-tracking-system)
2025-06-20 06:46:21 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - No active profile set, falling back to 1 default profile: "default"
2025-06-20 06:46:24 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-20 06:46:24 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-20 06:46:24 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.30]
2025-06-20 06:46:24 [restartedMain] INFO  o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-20 06:46:24 [restartedMain] WARN  c.h.i.impl.HazelcastInstanceFactory - Hazelcast is starting in a Java modular environment (Java 9 and newer) but without proper access to required Java packages. Use additional Java arguments to provide Hazelcast access to Java internal API. The internal API access is used to get the best performance results. Arguments to be used:
 --add-modules java.se --add-exports java.base/jdk.internal.ref=ALL-UNNAMED --add-opens java.base/java.lang=ALL-UNNAMED --add-opens java.base/sun.nio.ch=ALL-UNNAMED --add-opens java.management/sun.management=ALL-UNNAMED --add-opens jdk.management/com.sun.management.internal=ALL-UNNAMED
2025-06-20 06:46:24 [restartedMain] INFO  c.hazelcast.instance.AddressPicker - [LOCAL] [pts-cluster] [5.4.0] Interfaces is disabled, trying to pick one address from TCP-IP config addresses: [127.0.0.1]
2025-06-20 06:46:24 [restartedMain] INFO  com.hazelcast.system.logo - [127.0.0.1]:5701 [pts-cluster] [5.4.0] 
	o    o     o     o---o   o--o o      o---o     o     o----o o--o--o
	|    |    / \       /         |     /         / \    |         |   
	o----o       o     o   o----o |    o             o   o----o    |   
	|    |  *     \   /           |     \       *     \       |    |   
	o    o *       o o---o   o--o o----o o---o *       o o----o    o   
2025-06-20 06:46:24 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Copyright (c) 2008-2024, Hazelcast, Inc. All Rights Reserved.
2025-06-20 06:46:24 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Hazelcast Platform 5.4.0 (20240415) starting at [127.0.0.1]:5701
2025-06-20 06:46:24 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Cluster name: pts-cluster
2025-06-20 06:46:24 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Integrity Checker is disabled. Fail-fast on corrupted executables will not be performed. For more information, see the documentation for Integrity Checker.
2025-06-20 06:46:24 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] The Jet engine is disabled.
To enable the Jet engine on the members, do one of the following:
  - Change member config using Java API: config.getJetConfig().setEnabled(true)
  - Change XML/YAML configuration property: Set hazelcast.jet.enabled to true
  - Add system property: -Dhz.jet.enabled=true (for Hazelcast embedded, works only when loading config via Config.load)
  - Add environment variable: HZ_JET_ENABLED=true (recommended when running container image. For Hazelcast embedded, works only when loading config via Config.load)
2025-06-20 06:46:25 [restartedMain] INFO  com.hazelcast.system.security - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Enable DEBUG/FINE log level for log category com.hazelcast.system.security  or use -Dhazelcast.security.recommendations system property to see security recommendations and the status of current config.
2025-06-20 06:46:25 [restartedMain] INFO  com.hazelcast.instance.impl.Node - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Using TCP/IP discovery
2025-06-20 06:46:25 [restartedMain] WARN  com.hazelcast.cp.CPSubsystem - [127.0.0.1]:5701 [pts-cluster] [5.4.0] CP Subsystem is not enabled. CP data structures will operate in UNSAFE mode! Please note that UNSAFE mode will not provide strong consistency guarantees.
2025-06-20 06:46:25 [restartedMain] INFO  c.h.internal.diagnostics.Diagnostics - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Diagnostics disabled. To enable add -Dhazelcast.diagnostics.enabled=true to the JVM arguments.
2025-06-20 06:46:25 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is STARTING
2025-06-20 06:46:25 [hz.personnel-tracking-system.cached.thread-3] INFO  c.h.i.cluster.impl.TcpIpJoiner - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5703 is added to the blacklist.
2025-06-20 06:46:25 [hz.personnel-tracking-system.cached.thread-2] INFO  c.h.i.cluster.impl.TcpIpJoiner - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5702 is added to the blacklist.
2025-06-20 06:46:26 [restartedMain] INFO  c.h.internal.cluster.ClusterService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] 

Members {size:1, ver:1} [
	Member [127.0.0.1]:5701 - af9906ca-560e-420f-a599-8ffad387a872 this
]

2025-06-20 06:46:26 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is STARTED
2025-06-20 06:46:27 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-1 - Starting...
2025-06-20 06:46:27 [restartedMain] INFO  com.zaxxer.hikari.pool.HikariPool - HikariPool-1 - Added connection org.postgresql.jdbc.PgConnection@347c68ec
2025-06-20 06:46:27 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-1 - Start completed.
2025-06-20 06:46:28 [restartedMain] INFO  liquibase.changelog - Reading from dbpersonel.databasechangelog
2025-06-20 06:46:28 [restartedMain] INFO  liquibase.ui - Database is up to date, no changesets to execute
2025-06-20 06:46:28 [restartedMain] INFO  liquibase.changelog - Reading from dbpersonel.databasechangelog
2025-06-20 06:46:28 [restartedMain] INFO  liquibase.util - UPDATE SUMMARY
2025-06-20 06:46:28 [restartedMain] INFO  liquibase.util - Run:                          0
2025-06-20 06:46:28 [restartedMain] INFO  liquibase.util - Previously run:              28
2025-06-20 06:46:28 [restartedMain] INFO  liquibase.util - Filtered out:                 0
2025-06-20 06:46:28 [restartedMain] INFO  liquibase.util - -------------------------------
2025-06-20 06:46:28 [restartedMain] INFO  liquibase.util - Total change sets:           28
2025-06-20 06:46:28 [restartedMain] INFO  liquibase.util - Update summary generated
2025-06-20 06:46:28 [restartedMain] INFO  liquibase.command - Command execution complete
2025-06-20 06:46:29 [restartedMain] WARN  org.hibernate.orm.deprecation - HHH90000025: PostgreSQLDialect does not need to be specified explicitly using 'hibernate.dialect' (remove the property setting and it will be selected by default)
2025-06-20 06:46:30 [restartedMain] WARN  o.s.s.c.a.a.c.InitializeUserDetailsBeanManagerConfigurer$InitializeUserDetailsManagerConfigurer - Global AuthenticationManager configured with an AuthenticationProvider bean. UserDetailsService beans will not be used for username/password login. Consider removing the AuthenticationProvider bean. Alternatively, consider using the UserDetailsService in a manually instantiated DaoAuthenticationProvider.
2025-06-20 06:46:31 [restartedMain] INFO  c.p.s.impl.PersonelTypeServiceImpl - Checking for default personnel types...
2025-06-20 06:46:31 [restartedMain] INFO  c.p.s.impl.PersonelTypeServiceImpl - Personnel types already exist in the database.
2025-06-20 06:46:32 [restartedMain] WARN  o.s.b.a.o.j.JpaBaseConfiguration$JpaWebConfiguration - spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
2025-06-20 06:46:33 [restartedMain] INFO  o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = personneltrackingsystem-admin-0
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-20 06:46:33 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 06:46:33 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 06:46:33 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750391193452
2025-06-20 06:46:33 [kafka-admin-client-thread | personneltrackingsystem-admin-0] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for personneltrackingsystem-admin-0 unregistered
2025-06-20 06:46:33 [kafka-admin-client-thread | personneltrackingsystem-admin-0] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-20 06:46:33 [kafka-admin-client-thread | personneltrackingsystem-admin-0] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-20 06:46:33 [kafka-admin-client-thread | personneltrackingsystem-admin-0] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-20 06:46:33 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-20 06:46:33 [restartedMain] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pts-group-reset-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pts-group-reset
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-06-20 06:46:33 [restartedMain] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 06:46:33 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 06:46:33 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 06:46:33 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750391193994
2025-06-20 06:46:33 [restartedMain] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Subscribed to topic(s): turnstile-request
2025-06-20 06:46:34 [restartedMain] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pts-group-reset-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pts-group-reset
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-06-20 06:46:34 [restartedMain] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 06:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 06:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2025-06-20 06:46:34 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 06:46:34 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 06:46:34 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750391194023
2025-06-20 06:46:34 [restartedMain] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Subscribed to topic(s): email-notification
2025-06-20 06:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:46:34 [restartedMain] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pts-group-reset-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pts-group-reset
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-06-20 06:46:34 [restartedMain] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 06:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 06:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2025-06-20 06:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Request joining group due to: need to re-join with the given member-id: consumer-pts-group-reset-1-620d489a-cc3a-4d20-bb6e-982d2678e9f1
2025-06-20 06:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Request joining group due to: need to re-join with the given member-id: consumer-pts-group-reset-2-f856e2da-0f48-47d7-9c0c-0f914150094c
2025-06-20 06:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=10, memberId='consumer-pts-group-reset-1-620d489a-cc3a-4d20-bb6e-982d2678e9f1', protocol='range'}
2025-06-20 06:46:34 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 06:46:34 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 06:46:34 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750391194064
2025-06-20 06:46:34 [restartedMain] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Subscribed to topic(s): turnstile-passage
2025-06-20 06:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Finished assignment for group at generation 10: {consumer-pts-group-reset-1-620d489a-cc3a-4d20-bb6e-982d2678e9f1=Assignment(partitions=[turnstile-request-0])}
2025-06-20 06:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 06:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2025-06-20 06:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:46:34 [kafka-coordinator-heartbeat-thread | pts-group-reset] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] SyncGroup failed: The group began another rebalance. Need to re-join the group. Sent generation was Generation{generationId=10, memberId='consumer-pts-group-reset-1-620d489a-cc3a-4d20-bb6e-982d2678e9f1', protocol='range'}
2025-06-20 06:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Request joining group due to: need to re-join with the given member-id: consumer-pts-group-reset-3-fba1657d-2e82-4239-a3cf-0fefe01de0e9
2025-06-20 06:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Request joining group due to: rebalance failed due to 'The group is rebalancing, so a rejoin is needed.' (RebalanceInProgressException)
2025-06-20 06:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=11, memberId='consumer-pts-group-reset-3-fba1657d-2e82-4239-a3cf-0fefe01de0e9', protocol='range'}
2025-06-20 06:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=11, memberId='consumer-pts-group-reset-2-f856e2da-0f48-47d7-9c0c-0f914150094c', protocol='range'}
2025-06-20 06:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=11, memberId='consumer-pts-group-reset-1-620d489a-cc3a-4d20-bb6e-982d2678e9f1', protocol='range'}
2025-06-20 06:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Finished assignment for group at generation 11: {consumer-pts-group-reset-3-fba1657d-2e82-4239-a3cf-0fefe01de0e9=Assignment(partitions=[turnstile-passage-0]), consumer-pts-group-reset-2-f856e2da-0f48-47d7-9c0c-0f914150094c=Assignment(partitions=[email-notification-0]), consumer-pts-group-reset-1-620d489a-cc3a-4d20-bb6e-982d2678e9f1=Assignment(partitions=[turnstile-request-0])}
2025-06-20 06:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=11, memberId='consumer-pts-group-reset-1-620d489a-cc3a-4d20-bb6e-982d2678e9f1', protocol='range'}
2025-06-20 06:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=11, memberId='consumer-pts-group-reset-2-f856e2da-0f48-47d7-9c0c-0f914150094c', protocol='range'}
2025-06-20 06:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=11, memberId='consumer-pts-group-reset-3-fba1657d-2e82-4239-a3cf-0fefe01de0e9', protocol='range'}
2025-06-20 06:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[turnstile-request-0])
2025-06-20 06:46:34 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - Started PersonelTrackingSystemApplication in 13.558 seconds (process running for 14.131)
2025-06-20 06:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[email-notification-0])
2025-06-20 06:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[turnstile-passage-0])
2025-06-20 06:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Adding newly assigned partitions: turnstile-request-0
2025-06-20 06:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Adding newly assigned partitions: turnstile-passage-0
2025-06-20 06:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Adding newly assigned partitions: email-notification-0
2025-06-20 06:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition email-notification-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 06:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition turnstile-request-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 06:46:34 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition turnstile-passage-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 06:46:39 [http-nio-8080-exec-1] INFO  o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-06-20 06:46:41 [http-nio-8080-exec-9] INFO  o.s.api.AbstractOpenApiResource - Init duration for springdoc-openapi is: 1441 ms
2025-06-20 06:47:13 [http-nio-8080-exec-2] INFO  c.p.c.impl.TurnstileControllerImpl - Turnstile passage request received: DtoTurnstilePassageFullRequest(wantedToEnterTurnstileId=1, personelId=1, operationType=IN, operationTimeStr=09:17:35)
2025-06-20 06:47:13 [http-nio-8080-exec-2] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Last operation found: IN (Personel: 1, Turnstile: 1)
2025-06-20 06:47:13 [http-nio-8080-exec-2] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Turnstile passage validation - Personel: 1, Turnstile: 1, Requested Operation: IN, Last Operation: IN
2025-06-20 06:47:13 [http-nio-8080-exec-2] WARN  c.p.s.i.TurnstileRegistrationLogServiceImpl - Only OUT operation is allowed when the last operation is IN (Personel: 1, Turnstile: 1)
2025-06-20 06:47:13 [http-nio-8080-exec-2] WARN  o.s.w.s.m.m.a.ExceptionHandlerExceptionResolver - Resolved [com.personneltrackingsystem.exception.ValidationException: turnstile.entry.requires.prior.exit]
2025-06-20 06:47:33 [http-nio-8080-exec-7] INFO  c.p.c.impl.TurnstileControllerImpl - Turnstile passage request received: DtoTurnstilePassageFullRequest(wantedToEnterTurnstileId=1, personelId=1, operationType=OUT, operationTimeStr=18:17:35)
2025-06-20 06:47:33 [http-nio-8080-exec-7] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Last operation found: IN (Personel: 1, Turnstile: 1)
2025-06-20 06:47:33 [http-nio-8080-exec-7] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Turnstile passage validation - Personel: 1, Turnstile: 1, Requested Operation: OUT, Last Operation: IN
2025-06-20 06:47:33 [http-nio-8080-exec-7] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Operation validated successfully: OUT (Personel: 1, Turnstile: 1)
2025-06-20 06:47:33 [http-nio-8080-exec-7] INFO  c.p.s.k.i.KafkaProducerServiceImpl - Sending turnstile request event to Kafka: TurnstileRequestEvent(wantedToEnterTurnstileId=1, personelId=1, operationType=OUT, operationTimeStr=18:17:35)
2025-06-20 06:47:33 [http-nio-8080-exec-7] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = personneltrackingsystem-producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 3
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.springframework.kafka.support.serializer.JsonSerializer

2025-06-20 06:47:33 [http-nio-8080-exec-7] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 06:47:33 [http-nio-8080-exec-7] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=personneltrackingsystem-producer-1] Instantiated an idempotent producer.
2025-06-20 06:47:33 [http-nio-8080-exec-7] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 06:47:33 [http-nio-8080-exec-7] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 06:47:33 [http-nio-8080-exec-7] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750391253851
2025-06-20 06:47:33 [kafka-producer-network-thread | personneltrackingsystem-producer-1] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=personneltrackingsystem-producer-1] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 06:47:33 [kafka-producer-network-thread | personneltrackingsystem-producer-1] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=personneltrackingsystem-producer-1] ProducerId set to 2 with epoch 0
2025-06-20 06:47:33 [kafka-producer-network-thread | personneltrackingsystem-producer-1] INFO  c.p.s.k.i.KafkaProducerServiceImpl - Turnstile request event sent successfully to topic: turnstile-request, partition: 0, offset: 2
2025-06-20 06:47:33 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.k.i.TurnstileRequestConsumerServiceImpl - Received turnstile request event: TurnstileRequestEvent(wantedToEnterTurnstileId=1, personelId=1, operationType=OUT, operationTimeStr=18:17:35)
2025-06-20 06:47:34 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.c.i.PersonelCacheServiceImpl - Personnel found in cache with ID: 1
2025-06-20 06:47:34 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Saved turnstile registration log: Personel 1 - Turnstile 1 - Operation OUT
2025-06-20 06:47:34 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.k.i.TurnstileRequestConsumerServiceImpl - Turnstile request event processed successfully
2025-06-20 06:47:51 [http-nio-8080-exec-1] INFO  c.p.c.impl.TurnstileControllerImpl - Turnstile passage request received: DtoTurnstilePassageFullRequest(wantedToEnterTurnstileId=1, personelId=1, operationType=IN, operationTimeStr=09:23:35)
2025-06-20 06:47:51 [http-nio-8080-exec-1] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Last operation found: OUT (Personel: 1, Turnstile: 1)
2025-06-20 06:47:51 [http-nio-8080-exec-1] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Turnstile passage validation - Personel: 1, Turnstile: 1, Requested Operation: IN, Last Operation: OUT
2025-06-20 06:47:51 [http-nio-8080-exec-1] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Operation validated successfully: IN (Personel: 1, Turnstile: 1)
2025-06-20 06:47:51 [http-nio-8080-exec-1] INFO  c.p.s.k.i.KafkaProducerServiceImpl - Sending turnstile request event to Kafka: TurnstileRequestEvent(wantedToEnterTurnstileId=1, personelId=1, operationType=IN, operationTimeStr=09:23:35)
2025-06-20 06:47:51 [kafka-producer-network-thread | personneltrackingsystem-producer-1] INFO  c.p.s.k.i.KafkaProducerServiceImpl - Turnstile request event sent successfully to topic: turnstile-request, partition: 0, offset: 3
2025-06-20 06:47:51 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.k.i.TurnstileRequestConsumerServiceImpl - Received turnstile request event: TurnstileRequestEvent(wantedToEnterTurnstileId=1, personelId=1, operationType=IN, operationTimeStr=09:23:35)
2025-06-20 06:47:51 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.c.i.PersonelCacheServiceImpl - Personnel found in cache with ID: 1
2025-06-20 06:47:51 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Saved turnstile registration log: Personel 1 - Turnstile 1 - Operation IN
2025-06-20 06:47:51 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.k.i.TurnstileRequestConsumerServiceImpl - Turnstile request event processed successfully
2025-06-20 06:49:44 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Revoke previously assigned partitions turnstile-passage-0
2025-06-20 06:49:44 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Revoke previously assigned partitions turnstile-request-0
2025-06-20 06:49:44 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Revoke previously assigned partitions email-notification-0
2025-06-20 06:49:44 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Member consumer-pts-group-reset-1-620d489a-cc3a-4d20-bb6e-982d2678e9f1 sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2025-06-20 06:49:44 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Member consumer-pts-group-reset-2-f856e2da-0f48-47d7-9c0c-0f914150094c sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2025-06-20 06:49:44 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Member consumer-pts-group-reset-3-fba1657d-2e82-4239-a3cf-0fefe01de0e9 sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2025-06-20 06:49:44 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-20 06:49:44 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-20 06:49:44 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Request joining group due to: consumer pro-actively leaving the group
2025-06-20 06:49:44 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-20 06:49:44 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Request joining group due to: consumer pro-actively leaving the group
2025-06-20 06:49:44 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Unsubscribed all topics or patterns and assigned partitions
2025-06-20 06:49:44 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Request joining group due to: consumer pro-actively leaving the group
2025-06-20 06:49:44 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Unsubscribed all topics or patterns and assigned partitions
2025-06-20 06:49:44 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Unsubscribed all topics or patterns and assigned partitions
2025-06-20 06:49:44 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-20 06:49:44 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-20 06:49:44 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-20 06:49:44 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Request joining group due to: consumer pro-actively leaving the group
2025-06-20 06:49:44 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Request joining group due to: consumer pro-actively leaving the group
2025-06-20 06:49:44 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Request joining group due to: consumer pro-actively leaving the group
2025-06-20 06:49:45 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-20 06:49:45 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-20 06:49:45 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-20 06:49:45 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-20 06:49:45 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-20 06:49:45 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-20 06:49:45 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-20 06:49:45 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-20 06:49:45 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-pts-group-reset-2 unregistered
2025-06-20 06:49:45 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-pts-group-reset-3 unregistered
2025-06-20 06:49:45 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-20 06:49:45 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-20 06:49:45 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-20 06:49:45 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-20 06:49:45 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-pts-group-reset-1 unregistered
2025-06-20 06:49:45 [Thread-5] INFO  o.a.coyote.http11.Http11NioProtocol - Stopping ProtocolHandler ["http-nio-8080"]
2025-06-20 06:49:45 [Thread-5] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=personneltrackingsystem-producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2025-06-20 06:49:45 [Thread-5] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-20 06:49:45 [Thread-5] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-20 06:49:45 [Thread-5] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-20 06:49:45 [Thread-5] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-20 06:49:45 [Thread-5] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.producer for personneltrackingsystem-producer-1 unregistered
2025-06-20 06:49:45 [Thread-5] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-1 - Shutdown initiated...
2025-06-20 06:49:45 [Thread-5] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-1 - Shutdown completed.
2025-06-20 06:49:45 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - Starting PersonelTrackingSystemApplication using Java 21.0.7 with PID 16928 (C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes started by Lenovo in C:\Users\Lenovo\Desktop\personnel-tracking-system)
2025-06-20 06:49:45 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - No active profile set, falling back to 1 default profile: "default"
2025-06-20 06:49:46 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-20 06:49:46 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-20 06:49:46 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.30]
2025-06-20 06:49:46 [restartedMain] INFO  o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-20 06:49:46 [restartedMain] INFO  c.hazelcast.instance.AddressPicker - [LOCAL] [pts-cluster] [5.4.0] Interfaces is disabled, trying to pick one address from TCP-IP config addresses: [127.0.0.1]
2025-06-20 06:49:46 [restartedMain] INFO  com.hazelcast.system.logo - [127.0.0.1]:5701 [pts-cluster] [5.4.0] 
	o    o     o     o---o   o--o o      o---o     o     o----o o--o--o
	|    |    / \       /         |     /         / \    |         |   
	o----o       o     o   o----o |    o             o   o----o    |   
	|    |  *     \   /           |     \       *     \       |    |   
	o    o *       o o---o   o--o o----o o---o *       o o----o    o   
2025-06-20 06:49:46 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Copyright (c) 2008-2024, Hazelcast, Inc. All Rights Reserved.
2025-06-20 06:49:46 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Hazelcast Platform 5.4.0 (20240415) starting at [127.0.0.1]:5701
2025-06-20 06:49:46 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Cluster name: pts-cluster
2025-06-20 06:49:46 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Integrity Checker is disabled. Fail-fast on corrupted executables will not be performed. For more information, see the documentation for Integrity Checker.
2025-06-20 06:49:46 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] The Jet engine is disabled.
To enable the Jet engine on the members, do one of the following:
  - Change member config using Java API: config.getJetConfig().setEnabled(true)
  - Change XML/YAML configuration property: Set hazelcast.jet.enabled to true
  - Add system property: -Dhz.jet.enabled=true (for Hazelcast embedded, works only when loading config via Config.load)
  - Add environment variable: HZ_JET_ENABLED=true (recommended when running container image. For Hazelcast embedded, works only when loading config via Config.load)
2025-06-20 06:49:46 [restartedMain] INFO  com.hazelcast.system.security - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Enable DEBUG/FINE log level for log category com.hazelcast.system.security  or use -Dhazelcast.security.recommendations system property to see security recommendations and the status of current config.
2025-06-20 06:49:46 [restartedMain] INFO  com.hazelcast.instance.impl.Node - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Using TCP/IP discovery
2025-06-20 06:49:46 [restartedMain] WARN  com.hazelcast.cp.CPSubsystem - [127.0.0.1]:5701 [pts-cluster] [5.4.0] CP Subsystem is not enabled. CP data structures will operate in UNSAFE mode! Please note that UNSAFE mode will not provide strong consistency guarantees.
2025-06-20 06:49:46 [restartedMain] INFO  c.h.internal.diagnostics.Diagnostics - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Diagnostics disabled. To enable add -Dhazelcast.diagnostics.enabled=true to the JVM arguments.
2025-06-20 06:49:46 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is STARTING
2025-06-20 06:49:46 [hz.personnel-tracking-system.cached.thread-4] INFO  c.h.i.cluster.impl.TcpIpJoiner - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5703 is added to the blacklist.
2025-06-20 06:49:46 [hz.personnel-tracking-system.cached.thread-5] INFO  c.h.i.cluster.impl.TcpIpJoiner - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5702 is added to the blacklist.
2025-06-20 06:49:47 [restartedMain] INFO  c.h.internal.cluster.ClusterService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] 

Members {size:1, ver:1} [
	Member [127.0.0.1]:5701 - c3c6a018-d616-4fd8-9c73-3c42a80e241b this
]

2025-06-20 06:49:47 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is STARTED
2025-06-20 06:49:47 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-2 - Starting...
2025-06-20 06:49:47 [restartedMain] INFO  com.zaxxer.hikari.pool.HikariPool - HikariPool-2 - Added connection org.postgresql.jdbc.PgConnection@27015fe9
2025-06-20 06:49:47 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-2 - Start completed.
2025-06-20 06:49:48 [restartedMain] INFO  liquibase.changelog - Reading from dbpersonel.databasechangelog
2025-06-20 06:49:48 [restartedMain] INFO  liquibase.ui - Database is up to date, no changesets to execute
2025-06-20 06:49:48 [restartedMain] INFO  liquibase.changelog - Reading from dbpersonel.databasechangelog
2025-06-20 06:49:48 [restartedMain] INFO  liquibase.util - UPDATE SUMMARY
2025-06-20 06:49:48 [restartedMain] INFO  liquibase.util - Run:                          0
2025-06-20 06:49:48 [restartedMain] INFO  liquibase.util - Previously run:              28
2025-06-20 06:49:48 [restartedMain] INFO  liquibase.util - Filtered out:                 0
2025-06-20 06:49:48 [restartedMain] INFO  liquibase.util - -------------------------------
2025-06-20 06:49:48 [restartedMain] INFO  liquibase.util - Total change sets:           28
2025-06-20 06:49:48 [restartedMain] INFO  liquibase.util - Update summary generated
2025-06-20 06:49:48 [restartedMain] INFO  liquibase.command - Command execution complete
2025-06-20 06:49:48 [restartedMain] WARN  org.hibernate.orm.deprecation - HHH90000025: PostgreSQLDialect does not need to be specified explicitly using 'hibernate.dialect' (remove the property setting and it will be selected by default)
2025-06-20 06:49:48 [restartedMain] WARN  o.s.s.c.a.a.c.InitializeUserDetailsBeanManagerConfigurer$InitializeUserDetailsManagerConfigurer - Global AuthenticationManager configured with an AuthenticationProvider bean. UserDetailsService beans will not be used for username/password login. Consider removing the AuthenticationProvider bean. Alternatively, consider using the UserDetailsService in a manually instantiated DaoAuthenticationProvider.
2025-06-20 06:49:49 [restartedMain] INFO  c.p.s.impl.PersonelTypeServiceImpl - Checking for default personnel types...
2025-06-20 06:49:49 [restartedMain] INFO  c.p.s.impl.PersonelTypeServiceImpl - Personnel types already exist in the database.
2025-06-20 06:49:49 [restartedMain] WARN  o.s.b.a.o.j.JpaBaseConfiguration$JpaWebConfiguration - spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
2025-06-20 06:49:50 [restartedMain] INFO  o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = personneltrackingsystem-admin-1
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-20 06:49:50 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 06:49:50 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 06:49:50 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750391390147
2025-06-20 06:49:50 [kafka-admin-client-thread | personneltrackingsystem-admin-1] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for personneltrackingsystem-admin-1 unregistered
2025-06-20 06:49:50 [kafka-admin-client-thread | personneltrackingsystem-admin-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-20 06:49:50 [kafka-admin-client-thread | personneltrackingsystem-admin-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-20 06:49:50 [kafka-admin-client-thread | personneltrackingsystem-admin-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-20 06:49:50 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-20 06:49:50 [restartedMain] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pts-group-reset-4
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pts-group-reset
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-06-20 06:49:50 [restartedMain] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 06:49:50 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 06:49:50 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 06:49:50 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750391390248
2025-06-20 06:49:50 [restartedMain] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Subscribed to topic(s): turnstile-request
2025-06-20 06:49:50 [restartedMain] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pts-group-reset-5
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pts-group-reset
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-06-20 06:49:50 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 06:49:50 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2025-06-20 06:49:50 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:49:50 [restartedMain] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 06:49:50 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Request joining group due to: need to re-join with the given member-id: consumer-pts-group-reset-4-3781a51a-6fc1-4b72-9c30-dac0e0c11ba7
2025-06-20 06:49:50 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:49:50 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=13, memberId='consumer-pts-group-reset-4-3781a51a-6fc1-4b72-9c30-dac0e0c11ba7', protocol='range'}
2025-06-20 06:49:50 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Finished assignment for group at generation 13: {consumer-pts-group-reset-4-3781a51a-6fc1-4b72-9c30-dac0e0c11ba7=Assignment(partitions=[turnstile-request-0])}
2025-06-20 06:49:50 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=13, memberId='consumer-pts-group-reset-4-3781a51a-6fc1-4b72-9c30-dac0e0c11ba7', protocol='range'}
2025-06-20 06:49:50 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[turnstile-request-0])
2025-06-20 06:49:50 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Adding newly assigned partitions: turnstile-request-0
2025-06-20 06:49:50 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition turnstile-request-0 to the committed offset FetchPosition{offset=4, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 06:49:50 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 06:49:50 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 06:49:50 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750391390372
2025-06-20 06:49:50 [restartedMain] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-5, groupId=pts-group-reset] Subscribed to topic(s): email-notification
2025-06-20 06:49:50 [restartedMain] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pts-group-reset-6
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pts-group-reset
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-06-20 06:49:50 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-pts-group-reset-5, groupId=pts-group-reset] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 06:49:50 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-5, groupId=pts-group-reset] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2025-06-20 06:49:50 [restartedMain] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 06:49:50 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-5, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:49:50 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 06:49:50 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-5, groupId=pts-group-reset] Request joining group due to: need to re-join with the given member-id: consumer-pts-group-reset-5-d628579d-0752-418b-87af-170ab1144e4f
2025-06-20 06:49:50 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-5, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:49:50 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 06:49:50 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750391390394
2025-06-20 06:49:50 [restartedMain] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-6, groupId=pts-group-reset] Subscribed to topic(s): turnstile-passage
2025-06-20 06:49:50 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-pts-group-reset-6, groupId=pts-group-reset] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 06:49:50 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-6, groupId=pts-group-reset] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2025-06-20 06:49:50 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-6, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:49:50 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-6, groupId=pts-group-reset] Request joining group due to: need to re-join with the given member-id: consumer-pts-group-reset-6-26439a15-9d49-4411-b12a-fac32b04a579
2025-06-20 06:49:50 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-6, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:49:50 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - Started PersonelTrackingSystemApplication in 5.049 seconds (process running for 210.454)
2025-06-20 06:49:53 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Request joining group due to: group is already rebalancing
2025-06-20 06:49:53 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Revoke previously assigned partitions turnstile-request-0
2025-06-20 06:49:53 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:49:53 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-5, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=14, memberId='consumer-pts-group-reset-5-d628579d-0752-418b-87af-170ab1144e4f', protocol='range'}
2025-06-20 06:49:53 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-6, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=14, memberId='consumer-pts-group-reset-6-26439a15-9d49-4411-b12a-fac32b04a579', protocol='range'}
2025-06-20 06:49:53 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=14, memberId='consumer-pts-group-reset-4-3781a51a-6fc1-4b72-9c30-dac0e0c11ba7', protocol='range'}
2025-06-20 06:49:53 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Finished assignment for group at generation 14: {consumer-pts-group-reset-5-d628579d-0752-418b-87af-170ab1144e4f=Assignment(partitions=[email-notification-0]), consumer-pts-group-reset-4-3781a51a-6fc1-4b72-9c30-dac0e0c11ba7=Assignment(partitions=[turnstile-request-0]), consumer-pts-group-reset-6-26439a15-9d49-4411-b12a-fac32b04a579=Assignment(partitions=[turnstile-passage-0])}
2025-06-20 06:49:53 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-5, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=14, memberId='consumer-pts-group-reset-5-d628579d-0752-418b-87af-170ab1144e4f', protocol='range'}
2025-06-20 06:49:53 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=14, memberId='consumer-pts-group-reset-4-3781a51a-6fc1-4b72-9c30-dac0e0c11ba7', protocol='range'}
2025-06-20 06:49:53 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-6, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=14, memberId='consumer-pts-group-reset-6-26439a15-9d49-4411-b12a-fac32b04a579', protocol='range'}
2025-06-20 06:49:53 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-5, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[email-notification-0])
2025-06-20 06:49:53 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[turnstile-request-0])
2025-06-20 06:49:53 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-6, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[turnstile-passage-0])
2025-06-20 06:49:53 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-5, groupId=pts-group-reset] Adding newly assigned partitions: email-notification-0
2025-06-20 06:49:53 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Adding newly assigned partitions: turnstile-request-0
2025-06-20 06:49:53 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-6, groupId=pts-group-reset] Adding newly assigned partitions: turnstile-passage-0
2025-06-20 06:49:53 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition turnstile-request-0 to the committed offset FetchPosition{offset=4, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 06:49:53 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition email-notification-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 06:49:53 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition turnstile-passage-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 06:51:40 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - Starting PersonelTrackingSystemApplication using Java 21.0.7 with PID 14456 (C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes started by Lenovo in C:\Users\Lenovo\Desktop\personnel-tracking-system)
2025-06-20 06:51:40 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - No active profile set, falling back to 1 default profile: "default"
2025-06-20 06:51:43 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-20 06:51:43 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-20 06:51:43 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.30]
2025-06-20 06:51:43 [restartedMain] INFO  o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-20 06:51:43 [restartedMain] WARN  c.h.i.impl.HazelcastInstanceFactory - Hazelcast is starting in a Java modular environment (Java 9 and newer) but without proper access to required Java packages. Use additional Java arguments to provide Hazelcast access to Java internal API. The internal API access is used to get the best performance results. Arguments to be used:
 --add-modules java.se --add-exports java.base/jdk.internal.ref=ALL-UNNAMED --add-opens java.base/java.lang=ALL-UNNAMED --add-opens java.base/sun.nio.ch=ALL-UNNAMED --add-opens java.management/sun.management=ALL-UNNAMED --add-opens jdk.management/com.sun.management.internal=ALL-UNNAMED
2025-06-20 06:51:43 [restartedMain] INFO  c.hazelcast.instance.AddressPicker - [LOCAL] [pts-cluster] [5.4.0] Interfaces is disabled, trying to pick one address from TCP-IP config addresses: [127.0.0.1]
2025-06-20 06:51:43 [restartedMain] INFO  com.hazelcast.system.logo - [127.0.0.1]:5701 [pts-cluster] [5.4.0] 
	o    o     o     o---o   o--o o      o---o     o     o----o o--o--o
	|    |    / \       /         |     /         / \    |         |   
	o----o       o     o   o----o |    o             o   o----o    |   
	|    |  *     \   /           |     \       *     \       |    |   
	o    o *       o o---o   o--o o----o o---o *       o o----o    o   
2025-06-20 06:51:43 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Copyright (c) 2008-2024, Hazelcast, Inc. All Rights Reserved.
2025-06-20 06:51:43 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Hazelcast Platform 5.4.0 (20240415) starting at [127.0.0.1]:5701
2025-06-20 06:51:43 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Cluster name: pts-cluster
2025-06-20 06:51:43 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Integrity Checker is disabled. Fail-fast on corrupted executables will not be performed. For more information, see the documentation for Integrity Checker.
2025-06-20 06:51:43 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] The Jet engine is disabled.
To enable the Jet engine on the members, do one of the following:
  - Change member config using Java API: config.getJetConfig().setEnabled(true)
  - Change XML/YAML configuration property: Set hazelcast.jet.enabled to true
  - Add system property: -Dhz.jet.enabled=true (for Hazelcast embedded, works only when loading config via Config.load)
  - Add environment variable: HZ_JET_ENABLED=true (recommended when running container image. For Hazelcast embedded, works only when loading config via Config.load)
2025-06-20 06:51:44 [restartedMain] INFO  com.hazelcast.system.security - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Enable DEBUG/FINE log level for log category com.hazelcast.system.security  or use -Dhazelcast.security.recommendations system property to see security recommendations and the status of current config.
2025-06-20 06:51:44 [restartedMain] INFO  com.hazelcast.instance.impl.Node - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Using TCP/IP discovery
2025-06-20 06:51:44 [restartedMain] WARN  com.hazelcast.cp.CPSubsystem - [127.0.0.1]:5701 [pts-cluster] [5.4.0] CP Subsystem is not enabled. CP data structures will operate in UNSAFE mode! Please note that UNSAFE mode will not provide strong consistency guarantees.
2025-06-20 06:51:44 [restartedMain] INFO  c.h.internal.diagnostics.Diagnostics - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Diagnostics disabled. To enable add -Dhazelcast.diagnostics.enabled=true to the JVM arguments.
2025-06-20 06:51:44 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is STARTING
2025-06-20 06:51:44 [hz.personnel-tracking-system.cached.thread-3] INFO  c.h.i.cluster.impl.TcpIpJoiner - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5703 is added to the blacklist.
2025-06-20 06:51:44 [hz.personnel-tracking-system.cached.thread-2] INFO  c.h.i.cluster.impl.TcpIpJoiner - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5702 is added to the blacklist.
2025-06-20 06:51:45 [restartedMain] INFO  c.h.internal.cluster.ClusterService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] 

Members {size:1, ver:1} [
	Member [127.0.0.1]:5701 - ac0992cc-d4f9-4be8-b532-eb26efda4d12 this
]

2025-06-20 06:51:45 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is STARTED
2025-06-20 06:51:46 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-1 - Starting...
2025-06-20 06:51:46 [restartedMain] INFO  com.zaxxer.hikari.pool.HikariPool - HikariPool-1 - Added connection org.postgresql.jdbc.PgConnection@5df72be7
2025-06-20 06:51:46 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-1 - Start completed.
2025-06-20 06:51:46 [restartedMain] INFO  liquibase.changelog - Reading from dbpersonel.databasechangelog
2025-06-20 06:51:47 [restartedMain] INFO  liquibase.ui - Database is up to date, no changesets to execute
2025-06-20 06:51:47 [restartedMain] INFO  liquibase.changelog - Reading from dbpersonel.databasechangelog
2025-06-20 06:51:47 [restartedMain] INFO  liquibase.util - UPDATE SUMMARY
2025-06-20 06:51:47 [restartedMain] INFO  liquibase.util - Run:                          0
2025-06-20 06:51:47 [restartedMain] INFO  liquibase.util - Previously run:              28
2025-06-20 06:51:47 [restartedMain] INFO  liquibase.util - Filtered out:                 0
2025-06-20 06:51:47 [restartedMain] INFO  liquibase.util - -------------------------------
2025-06-20 06:51:47 [restartedMain] INFO  liquibase.util - Total change sets:           28
2025-06-20 06:51:47 [restartedMain] INFO  liquibase.util - Update summary generated
2025-06-20 06:51:47 [restartedMain] INFO  liquibase.command - Command execution complete
2025-06-20 06:51:47 [restartedMain] WARN  org.hibernate.orm.deprecation - HHH90000025: PostgreSQLDialect does not need to be specified explicitly using 'hibernate.dialect' (remove the property setting and it will be selected by default)
2025-06-20 06:51:49 [restartedMain] WARN  o.s.s.c.a.a.c.InitializeUserDetailsBeanManagerConfigurer$InitializeUserDetailsManagerConfigurer - Global AuthenticationManager configured with an AuthenticationProvider bean. UserDetailsService beans will not be used for username/password login. Consider removing the AuthenticationProvider bean. Alternatively, consider using the UserDetailsService in a manually instantiated DaoAuthenticationProvider.
2025-06-20 06:51:49 [restartedMain] INFO  c.p.s.impl.PersonelTypeServiceImpl - Checking for default personnel types...
2025-06-20 06:51:50 [restartedMain] INFO  c.p.s.impl.PersonelTypeServiceImpl - Personnel types already exist in the database.
2025-06-20 06:51:50 [restartedMain] WARN  o.s.b.a.o.j.JpaBaseConfiguration$JpaWebConfiguration - spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
2025-06-20 06:51:51 [restartedMain] INFO  o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = personneltrackingsystem-admin-0
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-20 06:51:51 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 06:51:51 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 06:51:51 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750391511566
2025-06-20 06:51:51 [kafka-admin-client-thread | personneltrackingsystem-admin-0] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for personneltrackingsystem-admin-0 unregistered
2025-06-20 06:51:51 [kafka-admin-client-thread | personneltrackingsystem-admin-0] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-20 06:51:51 [kafka-admin-client-thread | personneltrackingsystem-admin-0] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-20 06:51:51 [kafka-admin-client-thread | personneltrackingsystem-admin-0] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-20 06:51:51 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-20 06:51:52 [restartedMain] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pts-group-reset-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pts-group-reset
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-06-20 06:51:52 [restartedMain] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 06:51:52 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 06:51:52 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 06:51:52 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750391512130
2025-06-20 06:51:52 [restartedMain] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Subscribed to topic(s): turnstile-request
2025-06-20 06:51:52 [restartedMain] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pts-group-reset-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pts-group-reset
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-06-20 06:51:52 [restartedMain] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 06:51:52 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 06:51:52 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2025-06-20 06:51:52 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 06:51:52 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 06:51:52 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750391512158
2025-06-20 06:51:52 [restartedMain] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Subscribed to topic(s): email-notification
2025-06-20 06:51:52 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:51:52 [restartedMain] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pts-group-reset-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pts-group-reset
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-06-20 06:51:52 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 06:51:52 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2025-06-20 06:51:52 [restartedMain] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 06:51:52 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:51:52 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 06:51:52 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 06:51:52 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750391512188
2025-06-20 06:51:52 [restartedMain] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Subscribed to topic(s): turnstile-passage
2025-06-20 06:51:52 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Request joining group due to: need to re-join with the given member-id: consumer-pts-group-reset-2-21e3c7dd-cd32-419f-9068-305a8f330434
2025-06-20 06:51:52 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Request joining group due to: need to re-join with the given member-id: consumer-pts-group-reset-1-0be0b4bb-8ce7-4f6b-ac24-d847bca36a09
2025-06-20 06:51:52 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:51:52 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:51:52 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=16, memberId='consumer-pts-group-reset-1-0be0b4bb-8ce7-4f6b-ac24-d847bca36a09', protocol='range'}
2025-06-20 06:51:52 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=16, memberId='consumer-pts-group-reset-2-21e3c7dd-cd32-419f-9068-305a8f330434', protocol='range'}
2025-06-20 06:51:52 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 06:51:52 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2025-06-20 06:51:52 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:51:52 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Request joining group due to: need to re-join with the given member-id: consumer-pts-group-reset-3-a9f65151-fdbb-4b32-af6e-c9eb512a921d
2025-06-20 06:51:52 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:51:52 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] SyncGroup failed: The group began another rebalance. Need to re-join the group. Sent generation was Generation{generationId=16, memberId='consumer-pts-group-reset-2-21e3c7dd-cd32-419f-9068-305a8f330434', protocol='range'}
2025-06-20 06:51:52 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Request joining group due to: rebalance failed due to 'The group is rebalancing, so a rejoin is needed.' (RebalanceInProgressException)
2025-06-20 06:51:52 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:51:52 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Finished assignment for group at generation 16: {consumer-pts-group-reset-1-0be0b4bb-8ce7-4f6b-ac24-d847bca36a09=Assignment(partitions=[turnstile-request-0]), consumer-pts-group-reset-2-21e3c7dd-cd32-419f-9068-305a8f330434=Assignment(partitions=[email-notification-0])}
2025-06-20 06:51:52 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - Started PersonelTrackingSystemApplication in 11.765 seconds (process running for 12.386)
2025-06-20 06:51:52 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] SyncGroup failed: The group began another rebalance. Need to re-join the group. Sent generation was Generation{generationId=16, memberId='consumer-pts-group-reset-1-0be0b4bb-8ce7-4f6b-ac24-d847bca36a09', protocol='range'}
2025-06-20 06:51:52 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Request joining group due to: rebalance failed due to 'The group is rebalancing, so a rejoin is needed.' (RebalanceInProgressException)
2025-06-20 06:51:52 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:51:52 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=17, memberId='consumer-pts-group-reset-3-a9f65151-fdbb-4b32-af6e-c9eb512a921d', protocol='range'}
2025-06-20 06:51:52 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=17, memberId='consumer-pts-group-reset-2-21e3c7dd-cd32-419f-9068-305a8f330434', protocol='range'}
2025-06-20 06:51:52 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=17, memberId='consumer-pts-group-reset-1-0be0b4bb-8ce7-4f6b-ac24-d847bca36a09', protocol='range'}
2025-06-20 06:51:52 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Finished assignment for group at generation 17: {consumer-pts-group-reset-3-a9f65151-fdbb-4b32-af6e-c9eb512a921d=Assignment(partitions=[turnstile-passage-0]), consumer-pts-group-reset-1-0be0b4bb-8ce7-4f6b-ac24-d847bca36a09=Assignment(partitions=[turnstile-request-0]), consumer-pts-group-reset-2-21e3c7dd-cd32-419f-9068-305a8f330434=Assignment(partitions=[email-notification-0])}
2025-06-20 06:51:52 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=17, memberId='consumer-pts-group-reset-2-21e3c7dd-cd32-419f-9068-305a8f330434', protocol='range'}
2025-06-20 06:51:52 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=17, memberId='consumer-pts-group-reset-1-0be0b4bb-8ce7-4f6b-ac24-d847bca36a09', protocol='range'}
2025-06-20 06:51:52 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=17, memberId='consumer-pts-group-reset-3-a9f65151-fdbb-4b32-af6e-c9eb512a921d', protocol='range'}
2025-06-20 06:51:52 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[email-notification-0])
2025-06-20 06:51:52 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[turnstile-request-0])
2025-06-20 06:51:52 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[turnstile-passage-0])
2025-06-20 06:51:52 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Adding newly assigned partitions: email-notification-0
2025-06-20 06:51:52 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Adding newly assigned partitions: turnstile-request-0
2025-06-20 06:51:52 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Adding newly assigned partitions: turnstile-passage-0
2025-06-20 06:51:52 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition turnstile-request-0 to the committed offset FetchPosition{offset=4, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 06:51:52 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition turnstile-passage-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 06:51:52 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition email-notification-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 06:52:08 [http-nio-8080-exec-1] INFO  o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-06-20 06:52:08 [http-nio-8080-exec-1] INFO  c.p.c.impl.TurnstileControllerImpl - Turnstile passage request received: DtoTurnstilePassageFullRequest(wantedToEnterTurnstileId=1, personelId=1, operationType=OUT, operationTimeStr=18:23:35)
2025-06-20 06:52:08 [http-nio-8080-exec-1] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Last operation found: OUT (Personel: 1, Turnstile: 1)
2025-06-20 06:52:08 [http-nio-8080-exec-1] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Turnstile passage validation - Personel: 1, Turnstile: 1, Requested Operation: OUT, Last Operation: OUT
2025-06-20 06:52:08 [http-nio-8080-exec-1] WARN  c.p.s.i.TurnstileRegistrationLogServiceImpl - Only IN operation is allowed when the last operation is OUT (Personel: 1, Turnstile: 1)
2025-06-20 06:52:08 [http-nio-8080-exec-1] WARN  o.s.w.s.m.m.a.ExceptionHandlerExceptionResolver - Resolved [com.personneltrackingsystem.exception.ValidationException: turnstile.exit.requires.prior.entry]
2025-06-20 06:53:21 [http-nio-8080-exec-2] INFO  c.p.c.impl.TurnstileControllerImpl - Turnstile passage request received: DtoTurnstilePassageFullRequest(wantedToEnterTurnstileId=1, personelId=1, operationType=IN, operationTimeStr=09:23:35)
2025-06-20 06:53:21 [http-nio-8080-exec-2] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Last operation found: OUT (Personel: 1, Turnstile: 1)
2025-06-20 06:53:21 [http-nio-8080-exec-2] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Turnstile passage validation - Personel: 1, Turnstile: 1, Requested Operation: IN, Last Operation: OUT
2025-06-20 06:53:21 [http-nio-8080-exec-2] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Operation validated successfully: IN (Personel: 1, Turnstile: 1)
2025-06-20 06:53:21 [http-nio-8080-exec-2] INFO  c.p.s.k.i.KafkaProducerServiceImpl - Sending turnstile request event to Kafka: TurnstileRequestEvent(wantedToEnterTurnstileId=1, personelId=1, operationType=IN, operationTimeStr=09:23:35)
2025-06-20 06:53:21 [http-nio-8080-exec-2] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = personneltrackingsystem-producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 3
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.springframework.kafka.support.serializer.JsonSerializer

2025-06-20 06:53:21 [http-nio-8080-exec-2] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 06:53:21 [http-nio-8080-exec-2] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=personneltrackingsystem-producer-1] Instantiated an idempotent producer.
2025-06-20 06:53:21 [http-nio-8080-exec-2] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 06:53:21 [http-nio-8080-exec-2] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 06:53:21 [http-nio-8080-exec-2] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750391601988
2025-06-20 06:53:21 [kafka-producer-network-thread | personneltrackingsystem-producer-1] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=personneltrackingsystem-producer-1] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 06:53:21 [kafka-producer-network-thread | personneltrackingsystem-producer-1] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=personneltrackingsystem-producer-1] ProducerId set to 3 with epoch 0
2025-06-20 06:53:22 [kafka-producer-network-thread | personneltrackingsystem-producer-1] INFO  c.p.s.k.i.KafkaProducerServiceImpl - Turnstile request event sent successfully to topic: turnstile-request, partition: 0, offset: 4
2025-06-20 06:53:22 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.k.i.TurnstileRequestConsumerServiceImpl - Received turnstile request event: TurnstileRequestEvent(wantedToEnterTurnstileId=1, personelId=1, operationType=IN, operationTimeStr=09:23:35)
2025-06-20 06:53:22 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.c.i.PersonelCacheServiceImpl - Personnel found in cache with ID: 1
2025-06-20 06:53:22 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Saved turnstile registration log: Personel 1 - Turnstile 1 - Operation IN
2025-06-20 06:53:22 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.k.i.TurnstileRequestConsumerServiceImpl - Turnstile request event processed successfully
2025-06-20 06:53:27 [http-nio-8080-exec-3] INFO  c.p.c.impl.TurnstileControllerImpl - Turnstile passage request received: DtoTurnstilePassageFullRequest(wantedToEnterTurnstileId=1, personelId=1, operationType=IN, operationTimeStr=09:23:35)
2025-06-20 06:53:27 [http-nio-8080-exec-3] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Last operation found: OUT (Personel: 1, Turnstile: 1)
2025-06-20 06:53:27 [http-nio-8080-exec-3] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Turnstile passage validation - Personel: 1, Turnstile: 1, Requested Operation: IN, Last Operation: OUT
2025-06-20 06:53:27 [http-nio-8080-exec-3] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Operation validated successfully: IN (Personel: 1, Turnstile: 1)
2025-06-20 06:53:27 [http-nio-8080-exec-3] INFO  c.p.s.k.i.KafkaProducerServiceImpl - Sending turnstile request event to Kafka: TurnstileRequestEvent(wantedToEnterTurnstileId=1, personelId=1, operationType=IN, operationTimeStr=09:23:35)
2025-06-20 06:53:27 [kafka-producer-network-thread | personneltrackingsystem-producer-1] INFO  c.p.s.k.i.KafkaProducerServiceImpl - Turnstile request event sent successfully to topic: turnstile-request, partition: 0, offset: 5
2025-06-20 06:53:27 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.k.i.TurnstileRequestConsumerServiceImpl - Received turnstile request event: TurnstileRequestEvent(wantedToEnterTurnstileId=1, personelId=1, operationType=IN, operationTimeStr=09:23:35)
2025-06-20 06:53:27 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.c.i.PersonelCacheServiceImpl - Personnel found in cache with ID: 1
2025-06-20 06:53:27 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Saved turnstile registration log: Personel 1 - Turnstile 1 - Operation IN
2025-06-20 06:53:27 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.k.i.TurnstileRequestConsumerServiceImpl - Turnstile request event processed successfully
2025-06-20 06:55:07 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - Starting PersonelTrackingSystemApplication using Java 21.0.7 with PID 12076 (C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes started by Lenovo in C:\Users\Lenovo\Desktop\personnel-tracking-system)
2025-06-20 06:55:07 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - No active profile set, falling back to 1 default profile: "default"
2025-06-20 06:55:12 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-20 06:55:12 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-20 06:55:12 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.30]
2025-06-20 06:55:12 [restartedMain] INFO  o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-20 06:55:12 [restartedMain] WARN  c.h.i.impl.HazelcastInstanceFactory - Hazelcast is starting in a Java modular environment (Java 9 and newer) but without proper access to required Java packages. Use additional Java arguments to provide Hazelcast access to Java internal API. The internal API access is used to get the best performance results. Arguments to be used:
 --add-modules java.se --add-exports java.base/jdk.internal.ref=ALL-UNNAMED --add-opens java.base/java.lang=ALL-UNNAMED --add-opens java.base/sun.nio.ch=ALL-UNNAMED --add-opens java.management/sun.management=ALL-UNNAMED --add-opens jdk.management/com.sun.management.internal=ALL-UNNAMED
2025-06-20 06:55:12 [restartedMain] INFO  c.hazelcast.instance.AddressPicker - [LOCAL] [pts-cluster] [5.4.0] Interfaces is disabled, trying to pick one address from TCP-IP config addresses: [127.0.0.1]
2025-06-20 06:55:12 [restartedMain] INFO  com.hazelcast.system.logo - [127.0.0.1]:5701 [pts-cluster] [5.4.0] 
	o    o     o     o---o   o--o o      o---o     o     o----o o--o--o
	|    |    / \       /         |     /         / \    |         |   
	o----o       o     o   o----o |    o             o   o----o    |   
	|    |  *     \   /           |     \       *     \       |    |   
	o    o *       o o---o   o--o o----o o---o *       o o----o    o   
2025-06-20 06:55:12 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Copyright (c) 2008-2024, Hazelcast, Inc. All Rights Reserved.
2025-06-20 06:55:12 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Hazelcast Platform 5.4.0 (20240415) starting at [127.0.0.1]:5701
2025-06-20 06:55:12 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Cluster name: pts-cluster
2025-06-20 06:55:12 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Integrity Checker is disabled. Fail-fast on corrupted executables will not be performed. For more information, see the documentation for Integrity Checker.
2025-06-20 06:55:12 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] The Jet engine is disabled.
To enable the Jet engine on the members, do one of the following:
  - Change member config using Java API: config.getJetConfig().setEnabled(true)
  - Change XML/YAML configuration property: Set hazelcast.jet.enabled to true
  - Add system property: -Dhz.jet.enabled=true (for Hazelcast embedded, works only when loading config via Config.load)
  - Add environment variable: HZ_JET_ENABLED=true (recommended when running container image. For Hazelcast embedded, works only when loading config via Config.load)
2025-06-20 06:55:13 [restartedMain] INFO  com.hazelcast.system.security - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Enable DEBUG/FINE log level for log category com.hazelcast.system.security  or use -Dhazelcast.security.recommendations system property to see security recommendations and the status of current config.
2025-06-20 06:55:13 [restartedMain] INFO  com.hazelcast.instance.impl.Node - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Using TCP/IP discovery
2025-06-20 06:55:13 [restartedMain] WARN  com.hazelcast.cp.CPSubsystem - [127.0.0.1]:5701 [pts-cluster] [5.4.0] CP Subsystem is not enabled. CP data structures will operate in UNSAFE mode! Please note that UNSAFE mode will not provide strong consistency guarantees.
2025-06-20 06:55:13 [restartedMain] INFO  c.h.internal.diagnostics.Diagnostics - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Diagnostics disabled. To enable add -Dhazelcast.diagnostics.enabled=true to the JVM arguments.
2025-06-20 06:55:13 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is STARTING
2025-06-20 06:55:13 [hz.personnel-tracking-system.cached.thread-3] INFO  c.h.i.cluster.impl.TcpIpJoiner - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5703 is added to the blacklist.
2025-06-20 06:55:13 [hz.personnel-tracking-system.cached.thread-2] INFO  c.h.i.cluster.impl.TcpIpJoiner - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5702 is added to the blacklist.
2025-06-20 06:55:14 [restartedMain] INFO  c.h.internal.cluster.ClusterService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] 

Members {size:1, ver:1} [
	Member [127.0.0.1]:5701 - cb4e0fde-8b44-44e6-97df-2bf19187956a this
]

2025-06-20 06:55:14 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is STARTED
2025-06-20 06:55:14 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-1 - Starting...
2025-06-20 06:55:15 [restartedMain] INFO  com.zaxxer.hikari.pool.HikariPool - HikariPool-1 - Added connection org.postgresql.jdbc.PgConnection@52b6d9a2
2025-06-20 06:55:15 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-1 - Start completed.
2025-06-20 06:55:15 [restartedMain] INFO  liquibase.changelog - Reading from dbpersonel.databasechangelog
2025-06-20 06:55:15 [restartedMain] INFO  liquibase.ui - Database is up to date, no changesets to execute
2025-06-20 06:55:15 [restartedMain] INFO  liquibase.changelog - Reading from dbpersonel.databasechangelog
2025-06-20 06:55:15 [restartedMain] INFO  liquibase.util - UPDATE SUMMARY
2025-06-20 06:55:15 [restartedMain] INFO  liquibase.util - Run:                          0
2025-06-20 06:55:15 [restartedMain] INFO  liquibase.util - Previously run:              28
2025-06-20 06:55:15 [restartedMain] INFO  liquibase.util - Filtered out:                 0
2025-06-20 06:55:15 [restartedMain] INFO  liquibase.util - -------------------------------
2025-06-20 06:55:15 [restartedMain] INFO  liquibase.util - Total change sets:           28
2025-06-20 06:55:15 [restartedMain] INFO  liquibase.util - Update summary generated
2025-06-20 06:55:15 [restartedMain] INFO  liquibase.command - Command execution complete
2025-06-20 06:55:16 [restartedMain] WARN  org.hibernate.orm.deprecation - HHH90000025: PostgreSQLDialect does not need to be specified explicitly using 'hibernate.dialect' (remove the property setting and it will be selected by default)
2025-06-20 06:55:18 [restartedMain] WARN  o.s.s.c.a.a.c.InitializeUserDetailsBeanManagerConfigurer$InitializeUserDetailsManagerConfigurer - Global AuthenticationManager configured with an AuthenticationProvider bean. UserDetailsService beans will not be used for username/password login. Consider removing the AuthenticationProvider bean. Alternatively, consider using the UserDetailsService in a manually instantiated DaoAuthenticationProvider.
2025-06-20 06:55:19 [restartedMain] INFO  c.p.s.impl.PersonelTypeServiceImpl - Checking for default personnel types...
2025-06-20 06:55:19 [restartedMain] INFO  c.p.s.impl.PersonelTypeServiceImpl - Personnel types already exist in the database.
2025-06-20 06:55:19 [restartedMain] WARN  o.s.b.a.o.j.JpaBaseConfiguration$JpaWebConfiguration - spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
2025-06-20 06:55:20 [restartedMain] INFO  o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = personneltrackingsystem-admin-0
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-20 06:55:20 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 06:55:20 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 06:55:20 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750391720794
2025-06-20 06:55:21 [kafka-admin-client-thread | personneltrackingsystem-admin-0] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for personneltrackingsystem-admin-0 unregistered
2025-06-20 06:55:21 [kafka-admin-client-thread | personneltrackingsystem-admin-0] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-20 06:55:21 [kafka-admin-client-thread | personneltrackingsystem-admin-0] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-20 06:55:21 [kafka-admin-client-thread | personneltrackingsystem-admin-0] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-20 06:55:21 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-20 06:55:21 [restartedMain] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pts-group-reset-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pts-group-reset
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-06-20 06:55:21 [restartedMain] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 06:55:21 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 06:55:21 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 06:55:21 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750391721436
2025-06-20 06:55:21 [restartedMain] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Subscribed to topic(s): turnstile-request
2025-06-20 06:55:21 [restartedMain] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pts-group-reset-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pts-group-reset
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-06-20 06:55:21 [restartedMain] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 06:55:21 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 06:55:21 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2025-06-20 06:55:21 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 06:55:21 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 06:55:21 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:55:21 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750391721474
2025-06-20 06:55:21 [restartedMain] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Subscribed to topic(s): email-notification
2025-06-20 06:55:21 [restartedMain] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pts-group-reset-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pts-group-reset
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-06-20 06:55:21 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 06:55:21 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2025-06-20 06:55:21 [restartedMain] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 06:55:21 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:55:21 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Request joining group due to: need to re-join with the given member-id: consumer-pts-group-reset-2-33f0889e-9cc0-45d5-84b5-0b77093db35c
2025-06-20 06:55:21 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Request joining group due to: need to re-join with the given member-id: consumer-pts-group-reset-1-35454701-aa45-4174-a5d7-30a46660b867
2025-06-20 06:55:21 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:55:21 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:55:21 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=19, memberId='consumer-pts-group-reset-1-35454701-aa45-4174-a5d7-30a46660b867', protocol='range'}
2025-06-20 06:55:21 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=19, memberId='consumer-pts-group-reset-2-33f0889e-9cc0-45d5-84b5-0b77093db35c', protocol='range'}
2025-06-20 06:55:21 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 06:55:21 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 06:55:21 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750391721537
2025-06-20 06:55:21 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Finished assignment for group at generation 19: {consumer-pts-group-reset-1-35454701-aa45-4174-a5d7-30a46660b867=Assignment(partitions=[turnstile-request-0]), consumer-pts-group-reset-2-33f0889e-9cc0-45d5-84b5-0b77093db35c=Assignment(partitions=[email-notification-0])}
2025-06-20 06:55:21 [restartedMain] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Subscribed to topic(s): turnstile-passage
2025-06-20 06:55:21 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=19, memberId='consumer-pts-group-reset-1-35454701-aa45-4174-a5d7-30a46660b867', protocol='range'}
2025-06-20 06:55:21 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=19, memberId='consumer-pts-group-reset-2-33f0889e-9cc0-45d5-84b5-0b77093db35c', protocol='range'}
2025-06-20 06:55:21 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[email-notification-0])
2025-06-20 06:55:21 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[turnstile-request-0])
2025-06-20 06:55:21 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 06:55:21 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2025-06-20 06:55:21 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Adding newly assigned partitions: email-notification-0
2025-06-20 06:55:21 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Adding newly assigned partitions: turnstile-request-0
2025-06-20 06:55:21 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:55:21 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Request joining group due to: need to re-join with the given member-id: consumer-pts-group-reset-3-407dd5c5-23ac-4583-94b4-39dc169316bc
2025-06-20 06:55:21 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:55:21 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition email-notification-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 06:55:21 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition turnstile-request-0 to the committed offset FetchPosition{offset=6, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 06:55:21 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - Started PersonelTrackingSystemApplication in 15.649 seconds (process running for 16.784)
2025-06-20 06:55:24 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Request joining group due to: group is already rebalancing
2025-06-20 06:55:24 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Request joining group due to: group is already rebalancing
2025-06-20 06:55:24 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Revoke previously assigned partitions turnstile-request-0
2025-06-20 06:55:24 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Revoke previously assigned partitions email-notification-0
2025-06-20 06:55:24 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:55:24 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:55:24 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=20, memberId='consumer-pts-group-reset-3-407dd5c5-23ac-4583-94b4-39dc169316bc', protocol='range'}
2025-06-20 06:55:24 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=20, memberId='consumer-pts-group-reset-1-35454701-aa45-4174-a5d7-30a46660b867', protocol='range'}
2025-06-20 06:55:24 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=20, memberId='consumer-pts-group-reset-2-33f0889e-9cc0-45d5-84b5-0b77093db35c', protocol='range'}
2025-06-20 06:55:24 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Finished assignment for group at generation 20: {consumer-pts-group-reset-1-35454701-aa45-4174-a5d7-30a46660b867=Assignment(partitions=[turnstile-request-0]), consumer-pts-group-reset-2-33f0889e-9cc0-45d5-84b5-0b77093db35c=Assignment(partitions=[email-notification-0]), consumer-pts-group-reset-3-407dd5c5-23ac-4583-94b4-39dc169316bc=Assignment(partitions=[turnstile-passage-0])}
2025-06-20 06:55:24 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=20, memberId='consumer-pts-group-reset-2-33f0889e-9cc0-45d5-84b5-0b77093db35c', protocol='range'}
2025-06-20 06:55:24 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=20, memberId='consumer-pts-group-reset-1-35454701-aa45-4174-a5d7-30a46660b867', protocol='range'}
2025-06-20 06:55:24 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=20, memberId='consumer-pts-group-reset-3-407dd5c5-23ac-4583-94b4-39dc169316bc', protocol='range'}
2025-06-20 06:55:24 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[email-notification-0])
2025-06-20 06:55:24 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[turnstile-request-0])
2025-06-20 06:55:24 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[turnstile-passage-0])
2025-06-20 06:55:24 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Adding newly assigned partitions: email-notification-0
2025-06-20 06:55:24 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Adding newly assigned partitions: turnstile-request-0
2025-06-20 06:55:24 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Adding newly assigned partitions: turnstile-passage-0
2025-06-20 06:55:24 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition turnstile-request-0 to the committed offset FetchPosition{offset=6, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 06:55:24 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition email-notification-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 06:55:24 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition turnstile-passage-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 06:55:25 [http-nio-8080-exec-1] INFO  o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-06-20 06:55:26 [http-nio-8080-exec-1] INFO  c.p.c.impl.TurnstileControllerImpl - Turnstile passage request received: DtoTurnstilePassageFullRequest(wantedToEnterTurnstileId=1, personelId=1, operationType=IN, operationTimeStr=09:23:35)
2025-06-20 06:55:26 [http-nio-8080-exec-1] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Last operation found: OUT (Personel: 1, Turnstile: 1)
2025-06-20 06:55:26 [http-nio-8080-exec-1] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Turnstile passage validation - Personel: 1, Turnstile: 1, Requested Operation: IN, Last Operation: OUT
2025-06-20 06:55:26 [http-nio-8080-exec-1] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Operation validated successfully: IN (Personel: 1, Turnstile: 1)
2025-06-20 06:55:26 [http-nio-8080-exec-1] INFO  c.p.s.k.i.KafkaProducerServiceImpl - Sending turnstile request event to Kafka: TurnstileRequestEvent(wantedToEnterTurnstileId=1, personelId=1, operationType=IN, operationTimeStr=09:23:35)
2025-06-20 06:55:26 [http-nio-8080-exec-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = personneltrackingsystem-producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 3
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.springframework.kafka.support.serializer.JsonSerializer

2025-06-20 06:55:26 [http-nio-8080-exec-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 06:55:26 [http-nio-8080-exec-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=personneltrackingsystem-producer-1] Instantiated an idempotent producer.
2025-06-20 06:55:26 [http-nio-8080-exec-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 06:55:26 [http-nio-8080-exec-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 06:55:26 [http-nio-8080-exec-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750391726153
2025-06-20 06:55:26 [kafka-producer-network-thread | personneltrackingsystem-producer-1] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=personneltrackingsystem-producer-1] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 06:55:26 [kafka-producer-network-thread | personneltrackingsystem-producer-1] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=personneltrackingsystem-producer-1] ProducerId set to 4 with epoch 0
2025-06-20 06:55:26 [kafka-producer-network-thread | personneltrackingsystem-producer-1] INFO  c.p.s.k.i.KafkaProducerServiceImpl - Turnstile request event sent successfully to topic: turnstile-request, partition: 0, offset: 6
2025-06-20 06:55:26 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.k.i.TurnstileRequestConsumerServiceImpl - Received turnstile request event: TurnstileRequestEvent(wantedToEnterTurnstileId=1, personelId=1, operationType=IN, operationTimeStr=09:23:35)
2025-06-20 06:55:26 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.c.i.PersonelCacheServiceImpl - Personnel found in cache with ID: 1
2025-06-20 06:55:26 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Saved turnstile registration log: Personel 1 - Turnstile 1 - Operation IN
2025-06-20 06:55:26 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.k.i.TurnstileRequestConsumerServiceImpl - Turnstile request event processed successfully
2025-06-20 06:55:29 [http-nio-8080-exec-5] INFO  c.p.c.impl.TurnstileControllerImpl - Turnstile passage request received: DtoTurnstilePassageFullRequest(wantedToEnterTurnstileId=1, personelId=1, operationType=IN, operationTimeStr=09:23:35)
2025-06-20 06:55:29 [http-nio-8080-exec-5] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Last operation found: OUT (Personel: 1, Turnstile: 1)
2025-06-20 06:55:29 [http-nio-8080-exec-5] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Turnstile passage validation - Personel: 1, Turnstile: 1, Requested Operation: IN, Last Operation: OUT
2025-06-20 06:55:29 [http-nio-8080-exec-5] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Operation validated successfully: IN (Personel: 1, Turnstile: 1)
2025-06-20 06:55:29 [http-nio-8080-exec-5] INFO  c.p.s.k.i.KafkaProducerServiceImpl - Sending turnstile request event to Kafka: TurnstileRequestEvent(wantedToEnterTurnstileId=1, personelId=1, operationType=IN, operationTimeStr=09:23:35)
2025-06-20 06:55:29 [kafka-producer-network-thread | personneltrackingsystem-producer-1] INFO  c.p.s.k.i.KafkaProducerServiceImpl - Turnstile request event sent successfully to topic: turnstile-request, partition: 0, offset: 7
2025-06-20 06:55:29 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.k.i.TurnstileRequestConsumerServiceImpl - Received turnstile request event: TurnstileRequestEvent(wantedToEnterTurnstileId=1, personelId=1, operationType=IN, operationTimeStr=09:23:35)
2025-06-20 06:55:29 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.c.i.PersonelCacheServiceImpl - Personnel found in cache with ID: 1
2025-06-20 06:55:29 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Saved turnstile registration log: Personel 1 - Turnstile 1 - Operation IN
2025-06-20 06:55:29 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.k.i.TurnstileRequestConsumerServiceImpl - Turnstile request event processed successfully
2025-06-20 06:56:27 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - Starting PersonelTrackingSystemApplication using Java 21.0.7 with PID 12020 (C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes started by Lenovo in C:\Users\Lenovo\Desktop\personnel-tracking-system)
2025-06-20 06:56:27 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - No active profile set, falling back to 1 default profile: "default"
2025-06-20 06:56:30 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-20 06:56:30 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-20 06:56:30 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.30]
2025-06-20 06:56:30 [restartedMain] INFO  o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-20 06:56:30 [restartedMain] WARN  c.h.i.impl.HazelcastInstanceFactory - Hazelcast is starting in a Java modular environment (Java 9 and newer) but without proper access to required Java packages. Use additional Java arguments to provide Hazelcast access to Java internal API. The internal API access is used to get the best performance results. Arguments to be used:
 --add-modules java.se --add-exports java.base/jdk.internal.ref=ALL-UNNAMED --add-opens java.base/java.lang=ALL-UNNAMED --add-opens java.base/sun.nio.ch=ALL-UNNAMED --add-opens java.management/sun.management=ALL-UNNAMED --add-opens jdk.management/com.sun.management.internal=ALL-UNNAMED
2025-06-20 06:56:30 [restartedMain] INFO  c.hazelcast.instance.AddressPicker - [LOCAL] [pts-cluster] [5.4.0] Interfaces is disabled, trying to pick one address from TCP-IP config addresses: [127.0.0.1]
2025-06-20 06:56:30 [restartedMain] INFO  com.hazelcast.system.logo - [127.0.0.1]:5701 [pts-cluster] [5.4.0] 
	o    o     o     o---o   o--o o      o---o     o     o----o o--o--o
	|    |    / \       /         |     /         / \    |         |   
	o----o       o     o   o----o |    o             o   o----o    |   
	|    |  *     \   /           |     \       *     \       |    |   
	o    o *       o o---o   o--o o----o o---o *       o o----o    o   
2025-06-20 06:56:30 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Copyright (c) 2008-2024, Hazelcast, Inc. All Rights Reserved.
2025-06-20 06:56:30 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Hazelcast Platform 5.4.0 (20240415) starting at [127.0.0.1]:5701
2025-06-20 06:56:30 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Cluster name: pts-cluster
2025-06-20 06:56:30 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Integrity Checker is disabled. Fail-fast on corrupted executables will not be performed. For more information, see the documentation for Integrity Checker.
2025-06-20 06:56:30 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] The Jet engine is disabled.
To enable the Jet engine on the members, do one of the following:
  - Change member config using Java API: config.getJetConfig().setEnabled(true)
  - Change XML/YAML configuration property: Set hazelcast.jet.enabled to true
  - Add system property: -Dhz.jet.enabled=true (for Hazelcast embedded, works only when loading config via Config.load)
  - Add environment variable: HZ_JET_ENABLED=true (recommended when running container image. For Hazelcast embedded, works only when loading config via Config.load)
2025-06-20 06:56:31 [restartedMain] INFO  com.hazelcast.system.security - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Enable DEBUG/FINE log level for log category com.hazelcast.system.security  or use -Dhazelcast.security.recommendations system property to see security recommendations and the status of current config.
2025-06-20 06:56:31 [restartedMain] INFO  com.hazelcast.instance.impl.Node - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Using TCP/IP discovery
2025-06-20 06:56:31 [restartedMain] WARN  com.hazelcast.cp.CPSubsystem - [127.0.0.1]:5701 [pts-cluster] [5.4.0] CP Subsystem is not enabled. CP data structures will operate in UNSAFE mode! Please note that UNSAFE mode will not provide strong consistency guarantees.
2025-06-20 06:56:31 [restartedMain] INFO  c.h.internal.diagnostics.Diagnostics - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Diagnostics disabled. To enable add -Dhazelcast.diagnostics.enabled=true to the JVM arguments.
2025-06-20 06:56:31 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is STARTING
2025-06-20 06:56:31 [hz.personnel-tracking-system.cached.thread-2] INFO  c.h.i.cluster.impl.TcpIpJoiner - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5702 is added to the blacklist.
2025-06-20 06:56:31 [hz.personnel-tracking-system.cached.thread-3] INFO  c.h.i.cluster.impl.TcpIpJoiner - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5703 is added to the blacklist.
2025-06-20 06:56:32 [restartedMain] INFO  c.h.internal.cluster.ClusterService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] 

Members {size:1, ver:1} [
	Member [127.0.0.1]:5701 - 2028fe50-5541-48e9-b489-09088be08b33 this
]

2025-06-20 06:56:32 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is STARTED
2025-06-20 06:56:33 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-1 - Starting...
2025-06-20 06:56:33 [restartedMain] INFO  com.zaxxer.hikari.pool.HikariPool - HikariPool-1 - Added connection org.postgresql.jdbc.PgConnection@65d34acd
2025-06-20 06:56:33 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-1 - Start completed.
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Creating database history table with name: dbpersonel.databasechangelog
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Reading from dbpersonel.databasechangelog
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.lockservice - Successfully acquired change log lock
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.command - Using deploymentId: 0391794322
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Reading from dbpersonel.databasechangelog
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::1::liquibase
2025-06-20 06:56:34 [restartedMain] WARN  liquibase.executor - "dbpersonel" emas zaten mevcut, atlanyor
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::1::liquibase ran successfully in 13ms
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::2::liquibase
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::2::liquibase ran successfully in 115ms
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::3::liquibase
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Table personel_type created
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::3::liquibase ran successfully in 13ms
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::4::liquibase
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Table building created
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::4::liquibase ran successfully in 6ms
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::5::liquibase
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Table floor created
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::5::liquibase ran successfully in 7ms
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::6::liquibase
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Table personel created
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::6::liquibase ran successfully in 6ms
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::7::liquibase
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Table unit created
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::7::liquibase ran successfully in 10ms
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::8::liquibase
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Table personel_unit created
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::8::liquibase ran successfully in 9ms
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::9::liquibase
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Table gate created
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::9::liquibase ran successfully in 9ms
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::10::liquibase
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Table turnstile created
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::10::liquibase ran successfully in 6ms
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::11::liquibase
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Table turnstile_registration_log created
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::11::liquibase ran successfully in 5ms
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::12::liquibase
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Table working_hours created
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::12::liquibase ran successfully in 12ms
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::13::liquibase
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Table salary created
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::13::liquibase ran successfully in 12ms
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::14::liquibase
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Table user created
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::14::liquibase ran successfully in 8ms
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::15::liquibase
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Table permission created
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::15::liquibase ran successfully in 6ms
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::16::liquibase
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Table role_permission created
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::16::liquibase ran successfully in 7ms
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::17::liquibase
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::17::liquibase ran successfully in 55ms
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::1::liquibase
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - New row inserted into personel_type
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - New row inserted into personel_type
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - New row inserted into personel_type
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::1::liquibase ran successfully in 7ms
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::2::liquibase
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - New row inserted into building
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - New row inserted into building
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - New row inserted into building
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::2::liquibase ran successfully in 6ms
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::3::liquibase
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Data loaded from 'db/changelog/data/floor-data.csv' into table 'floor'
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::3::liquibase ran successfully in 51ms
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::4::liquibase
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - New row inserted into personel
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - New row inserted into personel
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - New row inserted into personel
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::4::liquibase ran successfully in 14ms
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::5::liquibase
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - New row inserted into unit
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - New row inserted into unit
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - New row inserted into unit
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::5::liquibase ran successfully in 13ms
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::6::liquibase
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - New row inserted into personel_unit
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - New row inserted into personel_unit
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - New row inserted into personel_unit
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::6::liquibase ran successfully in 14ms
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::7::liquibase
2025-06-20 06:56:34 [restartedMain] INFO  liquibase.changelog - Data loaded from 'db/changelog/data/gate-data.csv' into table 'gate'
2025-06-20 06:56:35 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::7::liquibase ran successfully in 15ms
2025-06-20 06:56:35 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::8::liquibase
2025-06-20 06:56:35 [restartedMain] INFO  liquibase.changelog - Data loaded from 'db/changelog/data/turnstile-data.csv' into table 'turnstile'
2025-06-20 06:56:35 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::8::liquibase ran successfully in 14ms
2025-06-20 06:56:35 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::9::liquibase
2025-06-20 06:56:35 [restartedMain] INFO  liquibase.changelog - New row inserted into working_hours
2025-06-20 06:56:35 [restartedMain] INFO  liquibase.changelog - New row inserted into working_hours
2025-06-20 06:56:35 [restartedMain] INFO  liquibase.changelog - New row inserted into working_hours
2025-06-20 06:56:35 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::9::liquibase ran successfully in 9ms
2025-06-20 06:56:35 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::10::liquibase
2025-06-20 06:56:35 [restartedMain] INFO  liquibase.changelog - New row inserted into permission
2025-06-20 06:56:35 [restartedMain] INFO  liquibase.changelog - New row inserted into permission
2025-06-20 06:56:35 [restartedMain] INFO  liquibase.changelog - New row inserted into permission
2025-06-20 06:56:35 [restartedMain] INFO  liquibase.changelog - New row inserted into permission
2025-06-20 06:56:35 [restartedMain] INFO  liquibase.changelog - New row inserted into permission
2025-06-20 06:56:35 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::10::liquibase ran successfully in 15ms
2025-06-20 06:56:35 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::11::liquibase
2025-06-20 06:56:35 [restartedMain] INFO  liquibase.changelog - New row inserted into role_permission
2025-06-20 06:56:35 [restartedMain] INFO  liquibase.changelog - New row inserted into role_permission
2025-06-20 06:56:35 [restartedMain] INFO  liquibase.changelog - New row inserted into role_permission
2025-06-20 06:56:35 [restartedMain] INFO  liquibase.changelog - New row inserted into role_permission
2025-06-20 06:56:35 [restartedMain] INFO  liquibase.changelog - New row inserted into role_permission
2025-06-20 06:56:35 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::11::liquibase ran successfully in 12ms
2025-06-20 06:56:35 [restartedMain] INFO  liquibase.util - UPDATE SUMMARY
2025-06-20 06:56:35 [restartedMain] INFO  liquibase.util - Run:                         28
2025-06-20 06:56:35 [restartedMain] INFO  liquibase.util - Previously run:               0
2025-06-20 06:56:35 [restartedMain] INFO  liquibase.util - Filtered out:                 0
2025-06-20 06:56:35 [restartedMain] INFO  liquibase.util - -------------------------------
2025-06-20 06:56:35 [restartedMain] INFO  liquibase.util - Total change sets:           28
2025-06-20 06:56:35 [restartedMain] INFO  liquibase.util - Update summary generated
2025-06-20 06:56:35 [restartedMain] INFO  liquibase.command - Update command completed successfully.
2025-06-20 06:56:35 [restartedMain] INFO  liquibase.ui - Liquibase: Update has been successful. Rows affected: 124
2025-06-20 06:56:35 [restartedMain] INFO  liquibase.lockservice - Successfully released change log lock
2025-06-20 06:56:35 [restartedMain] INFO  liquibase.command - Command execution complete
2025-06-20 06:56:35 [restartedMain] WARN  org.hibernate.orm.deprecation - HHH90000025: PostgreSQLDialect does not need to be specified explicitly using 'hibernate.dialect' (remove the property setting and it will be selected by default)
2025-06-20 06:56:37 [restartedMain] WARN  o.s.s.c.a.a.c.InitializeUserDetailsBeanManagerConfigurer$InitializeUserDetailsManagerConfigurer - Global AuthenticationManager configured with an AuthenticationProvider bean. UserDetailsService beans will not be used for username/password login. Consider removing the AuthenticationProvider bean. Alternatively, consider using the UserDetailsService in a manually instantiated DaoAuthenticationProvider.
2025-06-20 06:56:38 [restartedMain] INFO  c.p.s.impl.PersonelTypeServiceImpl - Checking for default personnel types...
2025-06-20 06:56:38 [restartedMain] INFO  c.p.s.impl.PersonelTypeServiceImpl - Personnel types already exist in the database.
2025-06-20 06:56:38 [restartedMain] WARN  o.s.b.a.o.j.JpaBaseConfiguration$JpaWebConfiguration - spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
2025-06-20 06:56:39 [restartedMain] INFO  o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = personneltrackingsystem-admin-0
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-20 06:56:39 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 06:56:39 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 06:56:39 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750391799641
2025-06-20 06:56:40 [kafka-admin-client-thread | personneltrackingsystem-admin-0] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for personneltrackingsystem-admin-0 unregistered
2025-06-20 06:56:40 [kafka-admin-client-thread | personneltrackingsystem-admin-0] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-20 06:56:40 [kafka-admin-client-thread | personneltrackingsystem-admin-0] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-20 06:56:40 [kafka-admin-client-thread | personneltrackingsystem-admin-0] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-20 06:56:40 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-20 06:56:40 [restartedMain] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pts-group-reset-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pts-group-reset
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-06-20 06:56:40 [restartedMain] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 06:56:40 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 06:56:40 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 06:56:40 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750391800279
2025-06-20 06:56:40 [restartedMain] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Subscribed to topic(s): turnstile-request
2025-06-20 06:56:40 [restartedMain] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pts-group-reset-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pts-group-reset
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-06-20 06:56:40 [restartedMain] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 06:56:40 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 06:56:40 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 06:56:40 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2025-06-20 06:56:40 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 06:56:40 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750391800309
2025-06-20 06:56:40 [restartedMain] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Subscribed to topic(s): email-notification
2025-06-20 06:56:40 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:56:40 [restartedMain] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pts-group-reset-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pts-group-reset
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-06-20 06:56:40 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 06:56:40 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2025-06-20 06:56:40 [restartedMain] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 06:56:40 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:56:40 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 06:56:40 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 06:56:40 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750391800342
2025-06-20 06:56:40 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Request joining group due to: need to re-join with the given member-id: consumer-pts-group-reset-1-1d6afcb3-d23c-40c4-9068-30d253091c74
2025-06-20 06:56:40 [restartedMain] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Subscribed to topic(s): turnstile-passage
2025-06-20 06:56:40 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:56:40 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Request joining group due to: need to re-join with the given member-id: consumer-pts-group-reset-2-c67c663d-bfd6-41db-ab7c-6f90f30682e5
2025-06-20 06:56:40 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:56:40 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=22, memberId='consumer-pts-group-reset-1-1d6afcb3-d23c-40c4-9068-30d253091c74', protocol='range'}
2025-06-20 06:56:40 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Finished assignment for group at generation 22: {consumer-pts-group-reset-1-1d6afcb3-d23c-40c4-9068-30d253091c74=Assignment(partitions=[turnstile-request-0])}
2025-06-20 06:56:40 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] SyncGroup failed: The group began another rebalance. Need to re-join the group. Sent generation was Generation{generationId=22, memberId='consumer-pts-group-reset-1-1d6afcb3-d23c-40c4-9068-30d253091c74', protocol='range'}
2025-06-20 06:56:40 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Request joining group due to: rebalance failed due to 'The group is rebalancing, so a rejoin is needed.' (RebalanceInProgressException)
2025-06-20 06:56:40 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 06:56:40 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:56:40 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2025-06-20 06:56:40 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=23, memberId='consumer-pts-group-reset-1-1d6afcb3-d23c-40c4-9068-30d253091c74', protocol='range'}
2025-06-20 06:56:40 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=23, memberId='consumer-pts-group-reset-2-c67c663d-bfd6-41db-ab7c-6f90f30682e5', protocol='range'}
2025-06-20 06:56:40 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:56:40 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Finished assignment for group at generation 23: {consumer-pts-group-reset-2-c67c663d-bfd6-41db-ab7c-6f90f30682e5=Assignment(partitions=[email-notification-0]), consumer-pts-group-reset-1-1d6afcb3-d23c-40c4-9068-30d253091c74=Assignment(partitions=[turnstile-request-0])}
2025-06-20 06:56:40 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Request joining group due to: need to re-join with the given member-id: consumer-pts-group-reset-3-d5ef4029-9316-4226-96d7-bbc446a74fa5
2025-06-20 06:56:40 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:56:40 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=23, memberId='consumer-pts-group-reset-2-c67c663d-bfd6-41db-ab7c-6f90f30682e5', protocol='range'}
2025-06-20 06:56:40 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=23, memberId='consumer-pts-group-reset-1-1d6afcb3-d23c-40c4-9068-30d253091c74', protocol='range'}
2025-06-20 06:56:40 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[turnstile-request-0])
2025-06-20 06:56:40 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[email-notification-0])
2025-06-20 06:56:40 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Adding newly assigned partitions: turnstile-request-0
2025-06-20 06:56:40 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Adding newly assigned partitions: email-notification-0
2025-06-20 06:56:40 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - Started PersonelTrackingSystemApplication in 13.27 seconds (process running for 13.844)
2025-06-20 06:56:40 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition turnstile-request-0 to the committed offset FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 06:56:40 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition email-notification-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 06:56:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Request joining group due to: group is already rebalancing
2025-06-20 06:56:43 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Request joining group due to: group is already rebalancing
2025-06-20 06:56:43 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Revoke previously assigned partitions turnstile-request-0
2025-06-20 06:56:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Revoke previously assigned partitions email-notification-0
2025-06-20 06:56:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:56:43 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] (Re-)joining group
2025-06-20 06:56:43 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=24, memberId='consumer-pts-group-reset-3-d5ef4029-9316-4226-96d7-bbc446a74fa5', protocol='range'}
2025-06-20 06:56:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=24, memberId='consumer-pts-group-reset-2-c67c663d-bfd6-41db-ab7c-6f90f30682e5', protocol='range'}
2025-06-20 06:56:43 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=24, memberId='consumer-pts-group-reset-1-1d6afcb3-d23c-40c4-9068-30d253091c74', protocol='range'}
2025-06-20 06:56:43 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Finished assignment for group at generation 24: {consumer-pts-group-reset-2-c67c663d-bfd6-41db-ab7c-6f90f30682e5=Assignment(partitions=[email-notification-0]), consumer-pts-group-reset-1-1d6afcb3-d23c-40c4-9068-30d253091c74=Assignment(partitions=[turnstile-request-0]), consumer-pts-group-reset-3-d5ef4029-9316-4226-96d7-bbc446a74fa5=Assignment(partitions=[turnstile-passage-0])}
2025-06-20 06:56:43 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=24, memberId='consumer-pts-group-reset-3-d5ef4029-9316-4226-96d7-bbc446a74fa5', protocol='range'}
2025-06-20 06:56:43 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=24, memberId='consumer-pts-group-reset-1-1d6afcb3-d23c-40c4-9068-30d253091c74', protocol='range'}
2025-06-20 06:56:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=24, memberId='consumer-pts-group-reset-2-c67c663d-bfd6-41db-ab7c-6f90f30682e5', protocol='range'}
2025-06-20 06:56:43 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[turnstile-passage-0])
2025-06-20 06:56:43 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[turnstile-request-0])
2025-06-20 06:56:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[email-notification-0])
2025-06-20 06:56:43 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Adding newly assigned partitions: turnstile-passage-0
2025-06-20 06:56:43 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Adding newly assigned partitions: turnstile-request-0
2025-06-20 06:56:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Adding newly assigned partitions: email-notification-0
2025-06-20 06:56:43 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition turnstile-passage-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 06:56:43 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition turnstile-request-0 to the committed offset FetchPosition{offset=8, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 06:56:43 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition email-notification-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 06:56:50 [http-nio-8080-exec-1] INFO  o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-06-20 06:56:52 [http-nio-8080-exec-10] INFO  o.s.api.AbstractOpenApiResource - Init duration for springdoc-openapi is: 1239 ms
2025-06-20 06:57:18 [http-nio-8080-exec-6] INFO  c.p.c.impl.TurnstileControllerImpl - Turnstile passage request received: DtoTurnstilePassageFullRequest(wantedToEnterTurnstileId=1, personelId=1, operationType=IN, operationTimeStr=09:17:35)
2025-06-20 06:57:18 [http-nio-8080-exec-6] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - No previous operation found for personel 1 on turnstile 1
2025-06-20 06:57:18 [http-nio-8080-exec-6] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Turnstile passage validation - Personel: 1, Turnstile: 1, Requested Operation: IN, Last Operation: null
2025-06-20 06:57:18 [http-nio-8080-exec-6] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Operation validated successfully: IN (Personel: 1, Turnstile: 1)
2025-06-20 06:57:18 [http-nio-8080-exec-6] INFO  c.p.s.k.i.KafkaProducerServiceImpl - Sending turnstile request event to Kafka: TurnstileRequestEvent(wantedToEnterTurnstileId=1, personelId=1, operationType=IN, operationTimeStr=09:17:35)
2025-06-20 06:57:18 [http-nio-8080-exec-6] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = personneltrackingsystem-producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 3
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.springframework.kafka.support.serializer.JsonSerializer

2025-06-20 06:57:18 [http-nio-8080-exec-6] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 06:57:18 [http-nio-8080-exec-6] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=personneltrackingsystem-producer-1] Instantiated an idempotent producer.
2025-06-20 06:57:18 [http-nio-8080-exec-6] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 06:57:18 [http-nio-8080-exec-6] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 06:57:18 [http-nio-8080-exec-6] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750391838573
2025-06-20 06:57:18 [kafka-producer-network-thread | personneltrackingsystem-producer-1] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=personneltrackingsystem-producer-1] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 06:57:18 [kafka-producer-network-thread | personneltrackingsystem-producer-1] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=personneltrackingsystem-producer-1] ProducerId set to 5 with epoch 0
2025-06-20 06:57:18 [kafka-producer-network-thread | personneltrackingsystem-producer-1] INFO  c.p.s.k.i.KafkaProducerServiceImpl - Turnstile request event sent successfully to topic: turnstile-request, partition: 0, offset: 8
2025-06-20 06:57:18 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.k.i.TurnstileRequestConsumerServiceImpl - Received turnstile request event: TurnstileRequestEvent(wantedToEnterTurnstileId=1, personelId=1, operationType=IN, operationTimeStr=09:17:35)
2025-06-20 06:57:19 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.c.i.PersonelCacheServiceImpl - Personnel found in cache with ID: 1
2025-06-20 06:57:19 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Saved turnstile registration log: Personel 1 - Turnstile 1 - Operation IN
2025-06-20 06:57:19 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.k.i.TurnstileRequestConsumerServiceImpl - Turnstile request event processed successfully
2025-06-20 06:57:30 [http-nio-8080-exec-10] INFO  c.p.c.impl.TurnstileControllerImpl - Turnstile passage request received: DtoTurnstilePassageFullRequest(wantedToEnterTurnstileId=1, personelId=1, operationType=IN, operationTimeStr=09:17:35)
2025-06-20 06:57:30 [http-nio-8080-exec-10] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Last operation found: IN (Personel: 1, Turnstile: 1)
2025-06-20 06:57:30 [http-nio-8080-exec-10] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Turnstile passage validation - Personel: 1, Turnstile: 1, Requested Operation: IN, Last Operation: IN
2025-06-20 06:57:30 [http-nio-8080-exec-10] WARN  c.p.s.i.TurnstileRegistrationLogServiceImpl - Only OUT operation is allowed when the last operation is IN (Personel: 1, Turnstile: 1)
2025-06-20 06:57:30 [http-nio-8080-exec-10] WARN  o.s.w.s.m.m.a.ExceptionHandlerExceptionResolver - Resolved [com.personneltrackingsystem.exception.ValidationException: turnstile.entry.requires.prior.exit]
2025-06-20 06:58:26 [http-nio-8080-exec-3] INFO  c.p.c.impl.TurnstileControllerImpl - Turnstile passage request received: DtoTurnstilePassageFullRequest(wantedToEnterTurnstileId=1, personelId=1, operationType=IN, operationTimeStr=09:17:35)
2025-06-20 06:58:26 [http-nio-8080-exec-3] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Last operation found: IN (Personel: 1, Turnstile: 1)
2025-06-20 06:58:26 [http-nio-8080-exec-3] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Turnstile passage validation - Personel: 1, Turnstile: 1, Requested Operation: IN, Last Operation: IN
2025-06-20 06:58:26 [http-nio-8080-exec-3] WARN  c.p.s.i.TurnstileRegistrationLogServiceImpl - Only OUT operation is allowed when the last operation is IN (Personel: 1, Turnstile: 1)
2025-06-20 06:58:26 [http-nio-8080-exec-3] WARN  o.s.w.s.m.m.a.ExceptionHandlerExceptionResolver - Resolved [com.personneltrackingsystem.exception.ValidationException: turnstile.entry.requires.prior.exit]
2025-06-20 07:01:27 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Revoke previously assigned partitions turnstile-request-0
2025-06-20 07:01:27 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Revoke previously assigned partitions turnstile-passage-0
2025-06-20 07:01:27 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Revoke previously assigned partitions email-notification-0
2025-06-20 07:01:27 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Member consumer-pts-group-reset-1-1d6afcb3-d23c-40c4-9068-30d253091c74 sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2025-06-20 07:01:27 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Member consumer-pts-group-reset-3-d5ef4029-9316-4226-96d7-bbc446a74fa5 sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2025-06-20 07:01:27 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Member consumer-pts-group-reset-2-c67c663d-bfd6-41db-ab7c-6f90f30682e5 sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2025-06-20 07:01:27 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-20 07:01:27 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-20 07:01:27 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-20 07:01:27 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Request joining group due to: consumer pro-actively leaving the group
2025-06-20 07:01:27 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Request joining group due to: consumer pro-actively leaving the group
2025-06-20 07:01:27 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Request joining group due to: consumer pro-actively leaving the group
2025-06-20 07:01:27 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Unsubscribed all topics or patterns and assigned partitions
2025-06-20 07:01:27 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Unsubscribed all topics or patterns and assigned partitions
2025-06-20 07:01:27 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Unsubscribed all topics or patterns and assigned partitions
2025-06-20 07:01:27 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-20 07:01:27 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-20 07:01:27 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-20 07:01:27 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Request joining group due to: consumer pro-actively leaving the group
2025-06-20 07:01:27 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Request joining group due to: consumer pro-actively leaving the group
2025-06-20 07:01:27 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Request joining group due to: consumer pro-actively leaving the group
2025-06-20 07:01:27 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-20 07:01:27 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-20 07:01:27 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-20 07:01:27 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-20 07:01:27 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-20 07:01:27 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-20 07:01:27 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-20 07:01:27 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-20 07:01:27 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-pts-group-reset-1 unregistered
2025-06-20 07:01:27 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-pts-group-reset-2 unregistered
2025-06-20 07:01:27 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-20 07:01:27 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-20 07:01:27 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-20 07:01:27 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-20 07:01:27 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-pts-group-reset-3 unregistered
2025-06-20 07:01:27 [Thread-5] INFO  o.a.coyote.http11.Http11NioProtocol - Stopping ProtocolHandler ["http-nio-8080"]
2025-06-20 07:01:27 [Thread-5] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=personneltrackingsystem-producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2025-06-20 07:01:27 [Thread-5] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-20 07:01:27 [Thread-5] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-20 07:01:27 [Thread-5] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-20 07:01:27 [Thread-5] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-20 07:01:27 [Thread-5] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.producer for personneltrackingsystem-producer-1 unregistered
2025-06-20 07:01:27 [Thread-5] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-1 - Shutdown initiated...
2025-06-20 07:01:27 [Thread-5] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-1 - Shutdown completed.
2025-06-20 07:01:27 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - Starting PersonelTrackingSystemApplication using Java 21.0.7 with PID 12020 (C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes started by Lenovo in C:\Users\Lenovo\Desktop\personnel-tracking-system)
2025-06-20 07:01:27 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - No active profile set, falling back to 1 default profile: "default"
2025-06-20 07:01:28 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-20 07:01:28 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-20 07:01:28 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.30]
2025-06-20 07:01:28 [restartedMain] INFO  o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-20 07:01:28 [restartedMain] INFO  c.hazelcast.instance.AddressPicker - [LOCAL] [pts-cluster] [5.4.0] Interfaces is disabled, trying to pick one address from TCP-IP config addresses: [127.0.0.1]
2025-06-20 07:01:28 [restartedMain] INFO  com.hazelcast.system.logo - [127.0.0.1]:5701 [pts-cluster] [5.4.0] 
	o    o     o     o---o   o--o o      o---o     o     o----o o--o--o
	|    |    / \       /         |     /         / \    |         |   
	o----o       o     o   o----o |    o             o   o----o    |   
	|    |  *     \   /           |     \       *     \       |    |   
	o    o *       o o---o   o--o o----o o---o *       o o----o    o   
2025-06-20 07:01:28 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Copyright (c) 2008-2024, Hazelcast, Inc. All Rights Reserved.
2025-06-20 07:01:28 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Hazelcast Platform 5.4.0 (20240415) starting at [127.0.0.1]:5701
2025-06-20 07:01:28 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Cluster name: pts-cluster
2025-06-20 07:01:28 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Integrity Checker is disabled. Fail-fast on corrupted executables will not be performed. For more information, see the documentation for Integrity Checker.
2025-06-20 07:01:28 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] The Jet engine is disabled.
To enable the Jet engine on the members, do one of the following:
  - Change member config using Java API: config.getJetConfig().setEnabled(true)
  - Change XML/YAML configuration property: Set hazelcast.jet.enabled to true
  - Add system property: -Dhz.jet.enabled=true (for Hazelcast embedded, works only when loading config via Config.load)
  - Add environment variable: HZ_JET_ENABLED=true (recommended when running container image. For Hazelcast embedded, works only when loading config via Config.load)
2025-06-20 07:01:28 [restartedMain] INFO  com.hazelcast.system.security - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Enable DEBUG/FINE log level for log category com.hazelcast.system.security  or use -Dhazelcast.security.recommendations system property to see security recommendations and the status of current config.
2025-06-20 07:01:28 [restartedMain] INFO  com.hazelcast.instance.impl.Node - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Using TCP/IP discovery
2025-06-20 07:01:28 [restartedMain] WARN  com.hazelcast.cp.CPSubsystem - [127.0.0.1]:5701 [pts-cluster] [5.4.0] CP Subsystem is not enabled. CP data structures will operate in UNSAFE mode! Please note that UNSAFE mode will not provide strong consistency guarantees.
2025-06-20 07:01:28 [restartedMain] INFO  c.h.internal.diagnostics.Diagnostics - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Diagnostics disabled. To enable add -Dhazelcast.diagnostics.enabled=true to the JVM arguments.
2025-06-20 07:01:28 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is STARTING
2025-06-20 07:01:28 [hz.personnel-tracking-system.cached.thread-1] INFO  c.h.i.cluster.impl.TcpIpJoiner - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5703 is added to the blacklist.
2025-06-20 07:01:28 [hz.personnel-tracking-system.cached.thread-3] INFO  c.h.i.cluster.impl.TcpIpJoiner - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5702 is added to the blacklist.
2025-06-20 07:01:29 [restartedMain] INFO  c.h.internal.cluster.ClusterService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] 

Members {size:1, ver:1} [
	Member [127.0.0.1]:5701 - c202abea-3c74-48aa-aba3-ebc947052522 this
]

2025-06-20 07:01:29 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is STARTED
2025-06-20 07:01:29 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-2 - Starting...
2025-06-20 07:01:29 [restartedMain] INFO  com.zaxxer.hikari.pool.HikariPool - HikariPool-2 - Added connection org.postgresql.jdbc.PgConnection@5d2faeea
2025-06-20 07:01:29 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-2 - Start completed.
2025-06-20 07:01:30 [restartedMain] INFO  liquibase.changelog - Reading from dbpersonel.databasechangelog
2025-06-20 07:01:30 [restartedMain] INFO  liquibase.ui - Database is up to date, no changesets to execute
2025-06-20 07:01:30 [restartedMain] INFO  liquibase.changelog - Reading from dbpersonel.databasechangelog
2025-06-20 07:01:30 [restartedMain] INFO  liquibase.util - UPDATE SUMMARY
2025-06-20 07:01:30 [restartedMain] INFO  liquibase.util - Run:                          0
2025-06-20 07:01:30 [restartedMain] INFO  liquibase.util - Previously run:              28
2025-06-20 07:01:30 [restartedMain] INFO  liquibase.util - Filtered out:                 0
2025-06-20 07:01:30 [restartedMain] INFO  liquibase.util - -------------------------------
2025-06-20 07:01:30 [restartedMain] INFO  liquibase.util - Total change sets:           28
2025-06-20 07:01:30 [restartedMain] INFO  liquibase.util - Update summary generated
2025-06-20 07:01:30 [restartedMain] INFO  liquibase.command - Command execution complete
2025-06-20 07:01:30 [restartedMain] WARN  org.hibernate.orm.deprecation - HHH90000025: PostgreSQLDialect does not need to be specified explicitly using 'hibernate.dialect' (remove the property setting and it will be selected by default)
2025-06-20 07:01:30 [restartedMain] WARN  o.s.s.c.a.a.c.InitializeUserDetailsBeanManagerConfigurer$InitializeUserDetailsManagerConfigurer - Global AuthenticationManager configured with an AuthenticationProvider bean. UserDetailsService beans will not be used for username/password login. Consider removing the AuthenticationProvider bean. Alternatively, consider using the UserDetailsService in a manually instantiated DaoAuthenticationProvider.
2025-06-20 07:01:30 [restartedMain] INFO  c.p.s.impl.PersonelTypeServiceImpl - Checking for default personnel types...
2025-06-20 07:01:30 [restartedMain] INFO  c.p.s.impl.PersonelTypeServiceImpl - Personnel types already exist in the database.
2025-06-20 07:01:31 [restartedMain] WARN  o.s.b.a.o.j.JpaBaseConfiguration$JpaWebConfiguration - spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
2025-06-20 07:01:31 [restartedMain] INFO  o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = personneltrackingsystem-admin-1
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-20 07:01:31 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 07:01:31 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 07:01:31 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750392091354
2025-06-20 07:01:31 [kafka-admin-client-thread | personneltrackingsystem-admin-1] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for personneltrackingsystem-admin-1 unregistered
2025-06-20 07:01:31 [kafka-admin-client-thread | personneltrackingsystem-admin-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-20 07:01:31 [kafka-admin-client-thread | personneltrackingsystem-admin-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-20 07:01:31 [kafka-admin-client-thread | personneltrackingsystem-admin-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-20 07:01:31 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-20 07:01:31 [restartedMain] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pts-group-reset-4
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pts-group-reset
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-06-20 07:01:31 [restartedMain] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 07:01:31 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 07:01:31 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 07:01:31 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750392091390
2025-06-20 07:01:31 [restartedMain] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Subscribed to topic(s): turnstile-request
2025-06-20 07:01:31 [restartedMain] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pts-group-reset-5
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pts-group-reset
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-06-20 07:01:31 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 07:01:31 [restartedMain] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 07:01:31 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2025-06-20 07:01:31 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:01:31 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Request joining group due to: need to re-join with the given member-id: consumer-pts-group-reset-4-850a6c81-6c5a-4d11-9da7-fa90d5d6e8c2
2025-06-20 07:01:31 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:01:31 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 07:01:31 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 07:01:31 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750392091414
2025-06-20 07:01:31 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=26, memberId='consumer-pts-group-reset-4-850a6c81-6c5a-4d11-9da7-fa90d5d6e8c2', protocol='range'}
2025-06-20 07:01:31 [restartedMain] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-5, groupId=pts-group-reset] Subscribed to topic(s): email-notification
2025-06-20 07:01:31 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Finished assignment for group at generation 26: {consumer-pts-group-reset-4-850a6c81-6c5a-4d11-9da7-fa90d5d6e8c2=Assignment(partitions=[turnstile-request-0])}
2025-06-20 07:01:31 [restartedMain] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pts-group-reset-6
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pts-group-reset
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-06-20 07:01:31 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=26, memberId='consumer-pts-group-reset-4-850a6c81-6c5a-4d11-9da7-fa90d5d6e8c2', protocol='range'}
2025-06-20 07:01:31 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-pts-group-reset-5, groupId=pts-group-reset] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 07:01:31 [restartedMain] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 07:01:31 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[turnstile-request-0])
2025-06-20 07:01:31 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-5, groupId=pts-group-reset] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2025-06-20 07:01:31 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Adding newly assigned partitions: turnstile-request-0
2025-06-20 07:01:31 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-5, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:01:31 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition turnstile-request-0 to the committed offset FetchPosition{offset=9, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 07:01:31 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-5, groupId=pts-group-reset] Request joining group due to: need to re-join with the given member-id: consumer-pts-group-reset-5-4e1caa2c-bb81-490b-ba4c-00d9ccb2a34f
2025-06-20 07:01:31 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-5, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:01:31 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 07:01:31 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 07:01:31 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750392091440
2025-06-20 07:01:31 [restartedMain] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-6, groupId=pts-group-reset] Subscribed to topic(s): turnstile-passage
2025-06-20 07:01:31 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-pts-group-reset-6, groupId=pts-group-reset] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 07:01:31 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-6, groupId=pts-group-reset] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2025-06-20 07:01:31 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-6, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:01:31 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - Started PersonelTrackingSystemApplication in 3.696 seconds (process running for 304.874)
2025-06-20 07:01:31 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-6, groupId=pts-group-reset] Request joining group due to: need to re-join with the given member-id: consumer-pts-group-reset-6-b678996a-9ee2-4db3-9e99-9011adb52aa6
2025-06-20 07:01:31 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-6, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:01:34 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Request joining group due to: group is already rebalancing
2025-06-20 07:01:34 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Revoke previously assigned partitions turnstile-request-0
2025-06-20 07:01:34 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:01:34 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-6, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=27, memberId='consumer-pts-group-reset-6-b678996a-9ee2-4db3-9e99-9011adb52aa6', protocol='range'}
2025-06-20 07:01:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-5, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=27, memberId='consumer-pts-group-reset-5-4e1caa2c-bb81-490b-ba4c-00d9ccb2a34f', protocol='range'}
2025-06-20 07:01:34 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=27, memberId='consumer-pts-group-reset-4-850a6c81-6c5a-4d11-9da7-fa90d5d6e8c2', protocol='range'}
2025-06-20 07:01:34 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Finished assignment for group at generation 27: {consumer-pts-group-reset-4-850a6c81-6c5a-4d11-9da7-fa90d5d6e8c2=Assignment(partitions=[turnstile-request-0]), consumer-pts-group-reset-6-b678996a-9ee2-4db3-9e99-9011adb52aa6=Assignment(partitions=[turnstile-passage-0]), consumer-pts-group-reset-5-4e1caa2c-bb81-490b-ba4c-00d9ccb2a34f=Assignment(partitions=[email-notification-0])}
2025-06-20 07:01:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-5, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=27, memberId='consumer-pts-group-reset-5-4e1caa2c-bb81-490b-ba4c-00d9ccb2a34f', protocol='range'}
2025-06-20 07:01:34 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-6, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=27, memberId='consumer-pts-group-reset-6-b678996a-9ee2-4db3-9e99-9011adb52aa6', protocol='range'}
2025-06-20 07:01:34 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=27, memberId='consumer-pts-group-reset-4-850a6c81-6c5a-4d11-9da7-fa90d5d6e8c2', protocol='range'}
2025-06-20 07:01:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-5, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[email-notification-0])
2025-06-20 07:01:34 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-6, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[turnstile-passage-0])
2025-06-20 07:01:34 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[turnstile-request-0])
2025-06-20 07:01:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-5, groupId=pts-group-reset] Adding newly assigned partitions: email-notification-0
2025-06-20 07:01:34 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-6, groupId=pts-group-reset] Adding newly assigned partitions: turnstile-passage-0
2025-06-20 07:01:34 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Adding newly assigned partitions: turnstile-request-0
2025-06-20 07:01:34 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition turnstile-passage-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 07:01:34 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition email-notification-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 07:01:34 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition turnstile-request-0 to the committed offset FetchPosition{offset=9, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 07:02:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-5, groupId=pts-group-reset] Revoke previously assigned partitions email-notification-0
2025-06-20 07:02:17 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-6, groupId=pts-group-reset] Revoke previously assigned partitions turnstile-passage-0
2025-06-20 07:02:17 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Revoke previously assigned partitions turnstile-request-0
2025-06-20 07:02:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-5, groupId=pts-group-reset] Member consumer-pts-group-reset-5-4e1caa2c-bb81-490b-ba4c-00d9ccb2a34f sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2025-06-20 07:02:17 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-6, groupId=pts-group-reset] Member consumer-pts-group-reset-6-b678996a-9ee2-4db3-9e99-9011adb52aa6 sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2025-06-20 07:02:17 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Member consumer-pts-group-reset-4-850a6c81-6c5a-4d11-9da7-fa90d5d6e8c2 sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2025-06-20 07:02:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-5, groupId=pts-group-reset] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-20 07:02:17 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-6, groupId=pts-group-reset] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-20 07:02:17 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-20 07:02:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-5, groupId=pts-group-reset] Request joining group due to: consumer pro-actively leaving the group
2025-06-20 07:02:17 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-6, groupId=pts-group-reset] Request joining group due to: consumer pro-actively leaving the group
2025-06-20 07:02:17 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Request joining group due to: consumer pro-actively leaving the group
2025-06-20 07:02:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-5, groupId=pts-group-reset] Unsubscribed all topics or patterns and assigned partitions
2025-06-20 07:02:17 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-6, groupId=pts-group-reset] Unsubscribed all topics or patterns and assigned partitions
2025-06-20 07:02:17 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Unsubscribed all topics or patterns and assigned partitions
2025-06-20 07:02:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-5, groupId=pts-group-reset] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-20 07:02:17 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-6, groupId=pts-group-reset] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-20 07:02:17 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-20 07:02:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-5, groupId=pts-group-reset] Request joining group due to: consumer pro-actively leaving the group
2025-06-20 07:02:17 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-6, groupId=pts-group-reset] Request joining group due to: consumer pro-actively leaving the group
2025-06-20 07:02:17 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Request joining group due to: consumer pro-actively leaving the group
2025-06-20 07:02:17 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-20 07:02:17 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-20 07:02:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-20 07:02:17 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-20 07:02:17 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-20 07:02:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-20 07:02:17 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-20 07:02:17 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-20 07:02:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-20 07:02:17 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-20 07:02:17 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-20 07:02:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-20 07:02:17 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-pts-group-reset-4 unregistered
2025-06-20 07:02:17 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-pts-group-reset-6 unregistered
2025-06-20 07:02:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-pts-group-reset-5 unregistered
2025-06-20 07:02:17 [Thread-7] INFO  o.a.coyote.http11.Http11NioProtocol - Stopping ProtocolHandler ["http-nio-8080"]
2025-06-20 07:02:18 [Thread-7] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-2 - Shutdown initiated...
2025-06-20 07:02:18 [Thread-7] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-2 - Shutdown completed.
2025-06-20 07:02:18 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - Starting PersonelTrackingSystemApplication using Java 21.0.7 with PID 12020 (C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes started by Lenovo in C:\Users\Lenovo\Desktop\personnel-tracking-system)
2025-06-20 07:02:18 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - No active profile set, falling back to 1 default profile: "default"
2025-06-20 07:02:18 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-20 07:02:18 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-20 07:02:18 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.30]
2025-06-20 07:02:18 [restartedMain] INFO  o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-20 07:02:18 [restartedMain] INFO  c.hazelcast.instance.AddressPicker - [LOCAL] [pts-cluster] [5.4.0] Interfaces is disabled, trying to pick one address from TCP-IP config addresses: [127.0.0.1]
2025-06-20 07:02:18 [restartedMain] INFO  com.hazelcast.system.logo - [127.0.0.1]:5701 [pts-cluster] [5.4.0] 
	o    o     o     o---o   o--o o      o---o     o     o----o o--o--o
	|    |    / \       /         |     /         / \    |         |   
	o----o       o     o   o----o |    o             o   o----o    |   
	|    |  *     \   /           |     \       *     \       |    |   
	o    o *       o o---o   o--o o----o o---o *       o o----o    o   
2025-06-20 07:02:18 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Copyright (c) 2008-2024, Hazelcast, Inc. All Rights Reserved.
2025-06-20 07:02:18 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Hazelcast Platform 5.4.0 (20240415) starting at [127.0.0.1]:5701
2025-06-20 07:02:18 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Cluster name: pts-cluster
2025-06-20 07:02:18 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Integrity Checker is disabled. Fail-fast on corrupted executables will not be performed. For more information, see the documentation for Integrity Checker.
2025-06-20 07:02:18 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] The Jet engine is disabled.
To enable the Jet engine on the members, do one of the following:
  - Change member config using Java API: config.getJetConfig().setEnabled(true)
  - Change XML/YAML configuration property: Set hazelcast.jet.enabled to true
  - Add system property: -Dhz.jet.enabled=true (for Hazelcast embedded, works only when loading config via Config.load)
  - Add environment variable: HZ_JET_ENABLED=true (recommended when running container image. For Hazelcast embedded, works only when loading config via Config.load)
2025-06-20 07:02:18 [restartedMain] INFO  com.hazelcast.system.security - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Enable DEBUG/FINE log level for log category com.hazelcast.system.security  or use -Dhazelcast.security.recommendations system property to see security recommendations and the status of current config.
2025-06-20 07:02:18 [restartedMain] INFO  com.hazelcast.instance.impl.Node - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Using TCP/IP discovery
2025-06-20 07:02:18 [restartedMain] WARN  com.hazelcast.cp.CPSubsystem - [127.0.0.1]:5701 [pts-cluster] [5.4.0] CP Subsystem is not enabled. CP data structures will operate in UNSAFE mode! Please note that UNSAFE mode will not provide strong consistency guarantees.
2025-06-20 07:02:18 [restartedMain] INFO  c.h.internal.diagnostics.Diagnostics - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Diagnostics disabled. To enable add -Dhazelcast.diagnostics.enabled=true to the JVM arguments.
2025-06-20 07:02:18 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is STARTING
2025-06-20 07:02:18 [hz.personnel-tracking-system.cached.thread-1] INFO  c.h.i.cluster.impl.TcpIpJoiner - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5703 is added to the blacklist.
2025-06-20 07:02:18 [hz.personnel-tracking-system.cached.thread-3] INFO  c.h.i.cluster.impl.TcpIpJoiner - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5702 is added to the blacklist.
2025-06-20 07:02:19 [restartedMain] INFO  c.h.internal.cluster.ClusterService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] 

Members {size:1, ver:1} [
	Member [127.0.0.1]:5701 - 9a59843a-da57-411a-b9f4-dce869b6f782 this
]

2025-06-20 07:02:19 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is STARTED
2025-06-20 07:02:19 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-3 - Starting...
2025-06-20 07:02:19 [restartedMain] INFO  com.zaxxer.hikari.pool.HikariPool - HikariPool-3 - Added connection org.postgresql.jdbc.PgConnection@73e88bcd
2025-06-20 07:02:19 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-3 - Start completed.
2025-06-20 07:02:20 [restartedMain] INFO  liquibase.changelog - Reading from dbpersonel.databasechangelog
2025-06-20 07:02:20 [restartedMain] INFO  liquibase.ui - Database is up to date, no changesets to execute
2025-06-20 07:02:20 [restartedMain] INFO  liquibase.changelog - Reading from dbpersonel.databasechangelog
2025-06-20 07:02:20 [restartedMain] INFO  liquibase.util - UPDATE SUMMARY
2025-06-20 07:02:20 [restartedMain] INFO  liquibase.util - Run:                          0
2025-06-20 07:02:20 [restartedMain] INFO  liquibase.util - Previously run:              28
2025-06-20 07:02:20 [restartedMain] INFO  liquibase.util - Filtered out:                 0
2025-06-20 07:02:20 [restartedMain] INFO  liquibase.util - -------------------------------
2025-06-20 07:02:20 [restartedMain] INFO  liquibase.util - Total change sets:           28
2025-06-20 07:02:20 [restartedMain] INFO  liquibase.util - Update summary generated
2025-06-20 07:02:20 [restartedMain] INFO  liquibase.command - Command execution complete
2025-06-20 07:02:20 [restartedMain] WARN  org.hibernate.orm.deprecation - HHH90000025: PostgreSQLDialect does not need to be specified explicitly using 'hibernate.dialect' (remove the property setting and it will be selected by default)
2025-06-20 07:02:20 [restartedMain] WARN  o.s.s.c.a.a.c.InitializeUserDetailsBeanManagerConfigurer$InitializeUserDetailsManagerConfigurer - Global AuthenticationManager configured with an AuthenticationProvider bean. UserDetailsService beans will not be used for username/password login. Consider removing the AuthenticationProvider bean. Alternatively, consider using the UserDetailsService in a manually instantiated DaoAuthenticationProvider.
2025-06-20 07:02:20 [restartedMain] INFO  c.p.s.impl.PersonelTypeServiceImpl - Checking for default personnel types...
2025-06-20 07:02:20 [restartedMain] INFO  c.p.s.impl.PersonelTypeServiceImpl - Personnel types already exist in the database.
2025-06-20 07:02:21 [restartedMain] WARN  o.s.b.a.o.j.JpaBaseConfiguration$JpaWebConfiguration - spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
2025-06-20 07:02:21 [restartedMain] INFO  o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = personneltrackingsystem-admin-2
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-20 07:02:21 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 07:02:21 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 07:02:21 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750392141366
2025-06-20 07:02:21 [kafka-admin-client-thread | personneltrackingsystem-admin-2] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for personneltrackingsystem-admin-2 unregistered
2025-06-20 07:02:21 [kafka-admin-client-thread | personneltrackingsystem-admin-2] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-20 07:02:21 [kafka-admin-client-thread | personneltrackingsystem-admin-2] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-20 07:02:21 [kafka-admin-client-thread | personneltrackingsystem-admin-2] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-20 07:02:21 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-20 07:02:21 [restartedMain] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pts-group-reset-7
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pts-group-reset
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-06-20 07:02:21 [restartedMain] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 07:02:21 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 07:02:21 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 07:02:21 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750392141391
2025-06-20 07:02:21 [restartedMain] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-7, groupId=pts-group-reset] Subscribed to topic(s): turnstile-request
2025-06-20 07:02:21 [restartedMain] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pts-group-reset-8
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pts-group-reset
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-06-20 07:02:21 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-pts-group-reset-7, groupId=pts-group-reset] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 07:02:21 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-7, groupId=pts-group-reset] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2025-06-20 07:02:21 [restartedMain] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 07:02:21 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-7, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:02:21 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-7, groupId=pts-group-reset] Request joining group due to: need to re-join with the given member-id: consumer-pts-group-reset-7-a5a435cb-ab49-46cb-bf45-26fdffca5479
2025-06-20 07:02:21 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-7, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:02:21 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-7, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=29, memberId='consumer-pts-group-reset-7-a5a435cb-ab49-46cb-bf45-26fdffca5479', protocol='range'}
2025-06-20 07:02:21 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 07:02:21 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 07:02:21 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-7, groupId=pts-group-reset] Finished assignment for group at generation 29: {consumer-pts-group-reset-7-a5a435cb-ab49-46cb-bf45-26fdffca5479=Assignment(partitions=[turnstile-request-0])}
2025-06-20 07:02:21 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750392141411
2025-06-20 07:02:21 [restartedMain] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-8, groupId=pts-group-reset] Subscribed to topic(s): email-notification
2025-06-20 07:02:21 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-7, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=29, memberId='consumer-pts-group-reset-7-a5a435cb-ab49-46cb-bf45-26fdffca5479', protocol='range'}
2025-06-20 07:02:21 [restartedMain] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pts-group-reset-9
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pts-group-reset
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-06-20 07:02:21 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-7, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[turnstile-request-0])
2025-06-20 07:02:21 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-pts-group-reset-8, groupId=pts-group-reset] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 07:02:21 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-7, groupId=pts-group-reset] Adding newly assigned partitions: turnstile-request-0
2025-06-20 07:02:21 [restartedMain] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 07:02:21 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-8, groupId=pts-group-reset] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2025-06-20 07:02:21 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-8, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:02:21 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition turnstile-request-0 to the committed offset FetchPosition{offset=9, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 07:02:21 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-8, groupId=pts-group-reset] Request joining group due to: need to re-join with the given member-id: consumer-pts-group-reset-8-70e41da5-c654-490c-add2-0581982387ae
2025-06-20 07:02:21 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-8, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:02:21 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 07:02:21 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 07:02:21 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750392141433
2025-06-20 07:02:21 [restartedMain] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-9, groupId=pts-group-reset] Subscribed to topic(s): turnstile-passage
2025-06-20 07:02:21 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-pts-group-reset-9, groupId=pts-group-reset] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 07:02:21 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-9, groupId=pts-group-reset] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2025-06-20 07:02:21 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-9, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:02:21 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - Started PersonelTrackingSystemApplication in 3.345 seconds (process running for 354.864)
2025-06-20 07:02:21 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-9, groupId=pts-group-reset] Request joining group due to: need to re-join with the given member-id: consumer-pts-group-reset-9-b8f466cb-61ac-486b-8654-e3324196cc16
2025-06-20 07:02:21 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-9, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:02:24 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-7, groupId=pts-group-reset] Request joining group due to: group is already rebalancing
2025-06-20 07:02:24 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-7, groupId=pts-group-reset] Revoke previously assigned partitions turnstile-request-0
2025-06-20 07:02:24 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-7, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:02:24 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-8, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=30, memberId='consumer-pts-group-reset-8-70e41da5-c654-490c-add2-0581982387ae', protocol='range'}
2025-06-20 07:02:24 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-9, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=30, memberId='consumer-pts-group-reset-9-b8f466cb-61ac-486b-8654-e3324196cc16', protocol='range'}
2025-06-20 07:02:24 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-7, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=30, memberId='consumer-pts-group-reset-7-a5a435cb-ab49-46cb-bf45-26fdffca5479', protocol='range'}
2025-06-20 07:02:24 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-7, groupId=pts-group-reset] Finished assignment for group at generation 30: {consumer-pts-group-reset-8-70e41da5-c654-490c-add2-0581982387ae=Assignment(partitions=[email-notification-0]), consumer-pts-group-reset-9-b8f466cb-61ac-486b-8654-e3324196cc16=Assignment(partitions=[turnstile-passage-0]), consumer-pts-group-reset-7-a5a435cb-ab49-46cb-bf45-26fdffca5479=Assignment(partitions=[turnstile-request-0])}
2025-06-20 07:02:24 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-7, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=30, memberId='consumer-pts-group-reset-7-a5a435cb-ab49-46cb-bf45-26fdffca5479', protocol='range'}
2025-06-20 07:02:24 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-8, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=30, memberId='consumer-pts-group-reset-8-70e41da5-c654-490c-add2-0581982387ae', protocol='range'}
2025-06-20 07:02:24 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-9, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=30, memberId='consumer-pts-group-reset-9-b8f466cb-61ac-486b-8654-e3324196cc16', protocol='range'}
2025-06-20 07:02:24 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-7, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[turnstile-request-0])
2025-06-20 07:02:24 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-8, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[email-notification-0])
2025-06-20 07:02:24 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-9, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[turnstile-passage-0])
2025-06-20 07:02:24 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-7, groupId=pts-group-reset] Adding newly assigned partitions: turnstile-request-0
2025-06-20 07:02:24 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-8, groupId=pts-group-reset] Adding newly assigned partitions: email-notification-0
2025-06-20 07:02:24 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-9, groupId=pts-group-reset] Adding newly assigned partitions: turnstile-passage-0
2025-06-20 07:02:24 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition email-notification-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 07:02:24 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition turnstile-request-0 to the committed offset FetchPosition{offset=9, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 07:02:24 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition turnstile-passage-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 07:02:57 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-7, groupId=pts-group-reset] Revoke previously assigned partitions turnstile-request-0
2025-06-20 07:02:57 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-8, groupId=pts-group-reset] Revoke previously assigned partitions email-notification-0
2025-06-20 07:02:57 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-9, groupId=pts-group-reset] Revoke previously assigned partitions turnstile-passage-0
2025-06-20 07:02:57 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-7, groupId=pts-group-reset] Member consumer-pts-group-reset-7-a5a435cb-ab49-46cb-bf45-26fdffca5479 sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2025-06-20 07:02:57 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-8, groupId=pts-group-reset] Member consumer-pts-group-reset-8-70e41da5-c654-490c-add2-0581982387ae sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2025-06-20 07:02:57 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-9, groupId=pts-group-reset] Member consumer-pts-group-reset-9-b8f466cb-61ac-486b-8654-e3324196cc16 sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2025-06-20 07:02:57 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-7, groupId=pts-group-reset] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-20 07:02:57 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-8, groupId=pts-group-reset] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-20 07:02:57 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-9, groupId=pts-group-reset] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-20 07:02:57 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-7, groupId=pts-group-reset] Request joining group due to: consumer pro-actively leaving the group
2025-06-20 07:02:57 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-8, groupId=pts-group-reset] Request joining group due to: consumer pro-actively leaving the group
2025-06-20 07:02:57 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-9, groupId=pts-group-reset] Request joining group due to: consumer pro-actively leaving the group
2025-06-20 07:02:57 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-7, groupId=pts-group-reset] Unsubscribed all topics or patterns and assigned partitions
2025-06-20 07:02:57 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-8, groupId=pts-group-reset] Unsubscribed all topics or patterns and assigned partitions
2025-06-20 07:02:57 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-7, groupId=pts-group-reset] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-20 07:02:57 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-9, groupId=pts-group-reset] Unsubscribed all topics or patterns and assigned partitions
2025-06-20 07:02:57 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-8, groupId=pts-group-reset] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-20 07:02:57 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-7, groupId=pts-group-reset] Request joining group due to: consumer pro-actively leaving the group
2025-06-20 07:02:57 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-9, groupId=pts-group-reset] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-20 07:02:57 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-8, groupId=pts-group-reset] Request joining group due to: consumer pro-actively leaving the group
2025-06-20 07:02:57 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-9, groupId=pts-group-reset] Request joining group due to: consumer pro-actively leaving the group
2025-06-20 07:02:58 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-20 07:02:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-20 07:02:58 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-20 07:02:58 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-20 07:02:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-20 07:02:58 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-20 07:02:58 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-20 07:02:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-20 07:02:58 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-20 07:02:58 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-20 07:02:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-20 07:02:58 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-20 07:02:58 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-pts-group-reset-7 unregistered
2025-06-20 07:02:58 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-pts-group-reset-9 unregistered
2025-06-20 07:02:58 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-pts-group-reset-8 unregistered
2025-06-20 07:02:58 [Thread-11] INFO  o.a.coyote.http11.Http11NioProtocol - Stopping ProtocolHandler ["http-nio-8080"]
2025-06-20 07:02:58 [Thread-11] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-3 - Shutdown initiated...
2025-06-20 07:02:58 [Thread-11] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-3 - Shutdown completed.
2025-06-20 07:02:58 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - Starting PersonelTrackingSystemApplication using Java 21.0.7 with PID 12020 (C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes started by Lenovo in C:\Users\Lenovo\Desktop\personnel-tracking-system)
2025-06-20 07:02:58 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - No active profile set, falling back to 1 default profile: "default"
2025-06-20 07:02:58 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-20 07:02:58 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-20 07:02:58 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.30]
2025-06-20 07:02:58 [restartedMain] INFO  o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-20 07:02:58 [restartedMain] INFO  c.hazelcast.instance.AddressPicker - [LOCAL] [pts-cluster] [5.4.0] Interfaces is disabled, trying to pick one address from TCP-IP config addresses: [127.0.0.1]
2025-06-20 07:02:58 [restartedMain] INFO  com.hazelcast.system.logo - [127.0.0.1]:5701 [pts-cluster] [5.4.0] 
	o    o     o     o---o   o--o o      o---o     o     o----o o--o--o
	|    |    / \       /         |     /         / \    |         |   
	o----o       o     o   o----o |    o             o   o----o    |   
	|    |  *     \   /           |     \       *     \       |    |   
	o    o *       o o---o   o--o o----o o---o *       o o----o    o   
2025-06-20 07:02:58 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Copyright (c) 2008-2024, Hazelcast, Inc. All Rights Reserved.
2025-06-20 07:02:58 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Hazelcast Platform 5.4.0 (20240415) starting at [127.0.0.1]:5701
2025-06-20 07:02:58 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Cluster name: pts-cluster
2025-06-20 07:02:58 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Integrity Checker is disabled. Fail-fast on corrupted executables will not be performed. For more information, see the documentation for Integrity Checker.
2025-06-20 07:02:58 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] The Jet engine is disabled.
To enable the Jet engine on the members, do one of the following:
  - Change member config using Java API: config.getJetConfig().setEnabled(true)
  - Change XML/YAML configuration property: Set hazelcast.jet.enabled to true
  - Add system property: -Dhz.jet.enabled=true (for Hazelcast embedded, works only when loading config via Config.load)
  - Add environment variable: HZ_JET_ENABLED=true (recommended when running container image. For Hazelcast embedded, works only when loading config via Config.load)
2025-06-20 07:02:58 [restartedMain] INFO  com.hazelcast.system.security - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Enable DEBUG/FINE log level for log category com.hazelcast.system.security  or use -Dhazelcast.security.recommendations system property to see security recommendations and the status of current config.
2025-06-20 07:02:58 [restartedMain] INFO  com.hazelcast.instance.impl.Node - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Using TCP/IP discovery
2025-06-20 07:02:58 [restartedMain] WARN  com.hazelcast.cp.CPSubsystem - [127.0.0.1]:5701 [pts-cluster] [5.4.0] CP Subsystem is not enabled. CP data structures will operate in UNSAFE mode! Please note that UNSAFE mode will not provide strong consistency guarantees.
2025-06-20 07:02:59 [restartedMain] INFO  c.h.internal.diagnostics.Diagnostics - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Diagnostics disabled. To enable add -Dhazelcast.diagnostics.enabled=true to the JVM arguments.
2025-06-20 07:02:59 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is STARTING
2025-06-20 07:02:59 [hz.personnel-tracking-system.cached.thread-2] INFO  c.h.i.cluster.impl.TcpIpJoiner - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5703 is added to the blacklist.
2025-06-20 07:02:59 [hz.personnel-tracking-system.cached.thread-3] INFO  c.h.i.cluster.impl.TcpIpJoiner - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5702 is added to the blacklist.
2025-06-20 07:03:00 [restartedMain] INFO  c.h.internal.cluster.ClusterService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] 

Members {size:1, ver:1} [
	Member [127.0.0.1]:5701 - 2e161313-64cc-4be7-892b-bdb02e47aa09 this
]

2025-06-20 07:03:00 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is STARTED
2025-06-20 07:03:00 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-4 - Starting...
2025-06-20 07:03:00 [restartedMain] INFO  com.zaxxer.hikari.pool.HikariPool - HikariPool-4 - Added connection org.postgresql.jdbc.PgConnection@593c847d
2025-06-20 07:03:00 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-4 - Start completed.
2025-06-20 07:03:00 [restartedMain] INFO  liquibase.changelog - Reading from dbpersonel.databasechangelog
2025-06-20 07:03:00 [restartedMain] INFO  liquibase.ui - Database is up to date, no changesets to execute
2025-06-20 07:03:00 [restartedMain] INFO  liquibase.changelog - Reading from dbpersonel.databasechangelog
2025-06-20 07:03:00 [restartedMain] INFO  liquibase.util - UPDATE SUMMARY
2025-06-20 07:03:00 [restartedMain] INFO  liquibase.util - Run:                          0
2025-06-20 07:03:00 [restartedMain] INFO  liquibase.util - Previously run:              28
2025-06-20 07:03:00 [restartedMain] INFO  liquibase.util - Filtered out:                 0
2025-06-20 07:03:00 [restartedMain] INFO  liquibase.util - -------------------------------
2025-06-20 07:03:00 [restartedMain] INFO  liquibase.util - Total change sets:           28
2025-06-20 07:03:00 [restartedMain] INFO  liquibase.util - Update summary generated
2025-06-20 07:03:00 [restartedMain] INFO  liquibase.command - Command execution complete
2025-06-20 07:03:00 [restartedMain] WARN  org.hibernate.orm.deprecation - HHH90000025: PostgreSQLDialect does not need to be specified explicitly using 'hibernate.dialect' (remove the property setting and it will be selected by default)
2025-06-20 07:03:00 [restartedMain] WARN  o.s.s.c.a.a.c.InitializeUserDetailsBeanManagerConfigurer$InitializeUserDetailsManagerConfigurer - Global AuthenticationManager configured with an AuthenticationProvider bean. UserDetailsService beans will not be used for username/password login. Consider removing the AuthenticationProvider bean. Alternatively, consider using the UserDetailsService in a manually instantiated DaoAuthenticationProvider.
2025-06-20 07:03:00 [restartedMain] INFO  c.p.s.impl.PersonelTypeServiceImpl - Checking for default personnel types...
2025-06-20 07:03:00 [restartedMain] INFO  c.p.s.impl.PersonelTypeServiceImpl - Personnel types already exist in the database.
2025-06-20 07:03:00 [restartedMain] WARN  o.s.b.a.o.j.JpaBaseConfiguration$JpaWebConfiguration - spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
2025-06-20 07:03:05 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - Starting PersonelTrackingSystemApplication using Java 21.0.7 with PID 13488 (C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes started by Lenovo in C:\Users\Lenovo\Desktop\personnel-tracking-system)
2025-06-20 07:03:05 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - No active profile set, falling back to 1 default profile: "default"
2025-06-20 07:03:08 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-20 07:03:08 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-20 07:03:08 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.30]
2025-06-20 07:03:08 [restartedMain] INFO  o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-20 07:03:08 [restartedMain] WARN  c.h.i.impl.HazelcastInstanceFactory - Hazelcast is starting in a Java modular environment (Java 9 and newer) but without proper access to required Java packages. Use additional Java arguments to provide Hazelcast access to Java internal API. The internal API access is used to get the best performance results. Arguments to be used:
 --add-modules java.se --add-exports java.base/jdk.internal.ref=ALL-UNNAMED --add-opens java.base/java.lang=ALL-UNNAMED --add-opens java.base/sun.nio.ch=ALL-UNNAMED --add-opens java.management/sun.management=ALL-UNNAMED --add-opens jdk.management/com.sun.management.internal=ALL-UNNAMED
2025-06-20 07:03:08 [restartedMain] INFO  c.hazelcast.instance.AddressPicker - [LOCAL] [pts-cluster] [5.4.0] Interfaces is disabled, trying to pick one address from TCP-IP config addresses: [127.0.0.1]
2025-06-20 07:03:08 [restartedMain] INFO  com.hazelcast.system.logo - [127.0.0.1]:5701 [pts-cluster] [5.4.0] 
	o    o     o     o---o   o--o o      o---o     o     o----o o--o--o
	|    |    / \       /         |     /         / \    |         |   
	o----o       o     o   o----o |    o             o   o----o    |   
	|    |  *     \   /           |     \       *     \       |    |   
	o    o *       o o---o   o--o o----o o---o *       o o----o    o   
2025-06-20 07:03:08 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Copyright (c) 2008-2024, Hazelcast, Inc. All Rights Reserved.
2025-06-20 07:03:08 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Hazelcast Platform 5.4.0 (20240415) starting at [127.0.0.1]:5701
2025-06-20 07:03:08 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Cluster name: pts-cluster
2025-06-20 07:03:08 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Integrity Checker is disabled. Fail-fast on corrupted executables will not be performed. For more information, see the documentation for Integrity Checker.
2025-06-20 07:03:08 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] The Jet engine is disabled.
To enable the Jet engine on the members, do one of the following:
  - Change member config using Java API: config.getJetConfig().setEnabled(true)
  - Change XML/YAML configuration property: Set hazelcast.jet.enabled to true
  - Add system property: -Dhz.jet.enabled=true (for Hazelcast embedded, works only when loading config via Config.load)
  - Add environment variable: HZ_JET_ENABLED=true (recommended when running container image. For Hazelcast embedded, works only when loading config via Config.load)
2025-06-20 07:03:09 [restartedMain] INFO  com.hazelcast.system.security - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Enable DEBUG/FINE log level for log category com.hazelcast.system.security  or use -Dhazelcast.security.recommendations system property to see security recommendations and the status of current config.
2025-06-20 07:03:09 [restartedMain] INFO  com.hazelcast.instance.impl.Node - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Using TCP/IP discovery
2025-06-20 07:03:09 [restartedMain] WARN  com.hazelcast.cp.CPSubsystem - [127.0.0.1]:5701 [pts-cluster] [5.4.0] CP Subsystem is not enabled. CP data structures will operate in UNSAFE mode! Please note that UNSAFE mode will not provide strong consistency guarantees.
2025-06-20 07:03:09 [restartedMain] INFO  c.h.internal.diagnostics.Diagnostics - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Diagnostics disabled. To enable add -Dhazelcast.diagnostics.enabled=true to the JVM arguments.
2025-06-20 07:03:09 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is STARTING
2025-06-20 07:03:09 [hz.personnel-tracking-system.cached.thread-3] INFO  c.h.i.cluster.impl.TcpIpJoiner - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5702 is added to the blacklist.
2025-06-20 07:03:09 [hz.personnel-tracking-system.cached.thread-2] INFO  c.h.i.cluster.impl.TcpIpJoiner - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5703 is added to the blacklist.
2025-06-20 07:03:10 [restartedMain] INFO  c.h.internal.cluster.ClusterService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] 

Members {size:1, ver:1} [
	Member [127.0.0.1]:5701 - 26dce811-9e52-43eb-a300-6e0909052a80 this
]

2025-06-20 07:03:10 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is STARTED
2025-06-20 07:03:10 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-1 - Starting...
2025-06-20 07:03:11 [restartedMain] INFO  com.zaxxer.hikari.pool.HikariPool - HikariPool-1 - Added connection org.postgresql.jdbc.PgConnection@10c8fd12
2025-06-20 07:03:11 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-1 - Start completed.
2025-06-20 07:03:11 [hz.personnel-tracking-system.HealthMonitor] INFO  c.h.i.diagnostics.HealthMonitor - [127.0.0.1]:5701 [pts-cluster] [5.4.0] The HealthMonitor has detected a high load on the system. For more detailed information,
enable Diagnostics by adding the property -Dhazelcast.diagnostics.enabled=true
2025-06-20 07:03:11 [hz.personnel-tracking-system.HealthMonitor] INFO  c.h.i.diagnostics.HealthMonitor - [127.0.0.1]:5701 [pts-cluster] [5.4.0] processors=8, physical.memory.total=7,8G, physical.memory.free=981,5M, swap.space.total=0, swap.space.free=0, heap.memory.used=40,6M, heap.memory.free=30,3M, heap.memory.total=75,0M, heap.memory.max=2,0G, heap.memory.used/total=54,16%, heap.memory.used/max=2,02%, minor.gc.count=9, minor.gc.time=35ms, major.gc.count=0, major.gc.time=0ms, unknown.gc.count=4, unknown.gc.time=3ms, load.process=0,00%, load.system=100,00%, load.systemAverage=n/a thread.count=56, thread.peakCount=56, cluster.timeDiff=0, event.q.size=0, executor.q.async.size=0, executor.q.client.size=0, executor.q.client.query.size=0, executor.q.client.blocking.size=0, executor.q.query.size=0, executor.q.scheduled.size=0, executor.q.io.size=0, executor.q.system.size=0, executor.q.operations.size=0, executor.q.priorityOperation.size=0, operations.completed.count=1, executor.q.mapLoad.size=0, executor.q.mapLoadAllKeys.size=0, executor.q.cluster.size=0, executor.q.response.size=0, operations.running.count=0, operations.pending.invocations.percentage=0,00%, operations.pending.invocations.count=0, proxy.count=0, clientEndpoint.count=0, connection.active.count=0, client.connection.count=0, connection.count=0
2025-06-20 07:03:11 [restartedMain] INFO  liquibase.changelog - Reading from dbpersonel.databasechangelog
2025-06-20 07:03:12 [restartedMain] INFO  liquibase.ui - Database is up to date, no changesets to execute
2025-06-20 07:03:12 [restartedMain] INFO  liquibase.changelog - Reading from dbpersonel.databasechangelog
2025-06-20 07:03:12 [restartedMain] INFO  liquibase.util - UPDATE SUMMARY
2025-06-20 07:03:12 [restartedMain] INFO  liquibase.util - Run:                          0
2025-06-20 07:03:12 [restartedMain] INFO  liquibase.util - Previously run:              28
2025-06-20 07:03:12 [restartedMain] INFO  liquibase.util - Filtered out:                 0
2025-06-20 07:03:12 [restartedMain] INFO  liquibase.util - -------------------------------
2025-06-20 07:03:12 [restartedMain] INFO  liquibase.util - Total change sets:           28
2025-06-20 07:03:12 [restartedMain] INFO  liquibase.util - Update summary generated
2025-06-20 07:03:12 [restartedMain] INFO  liquibase.command - Command execution complete
2025-06-20 07:03:12 [restartedMain] WARN  org.hibernate.orm.deprecation - HHH90000025: PostgreSQLDialect does not need to be specified explicitly using 'hibernate.dialect' (remove the property setting and it will be selected by default)
2025-06-20 07:03:14 [restartedMain] WARN  o.s.s.c.a.a.c.InitializeUserDetailsBeanManagerConfigurer$InitializeUserDetailsManagerConfigurer - Global AuthenticationManager configured with an AuthenticationProvider bean. UserDetailsService beans will not be used for username/password login. Consider removing the AuthenticationProvider bean. Alternatively, consider using the UserDetailsService in a manually instantiated DaoAuthenticationProvider.
2025-06-20 07:03:15 [restartedMain] INFO  c.p.s.impl.PersonelTypeServiceImpl - Checking for default personnel types...
2025-06-20 07:03:15 [restartedMain] INFO  c.p.s.impl.PersonelTypeServiceImpl - Personnel types already exist in the database.
2025-06-20 07:03:16 [restartedMain] WARN  o.s.b.a.o.j.JpaBaseConfiguration$JpaWebConfiguration - spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
2025-06-20 07:03:17 [restartedMain] INFO  o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = personneltrackingsystem-admin-0
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-20 07:03:17 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 07:03:17 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 07:03:17 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750392197192
2025-06-20 07:03:17 [kafka-admin-client-thread | personneltrackingsystem-admin-0] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for personneltrackingsystem-admin-0 unregistered
2025-06-20 07:03:17 [kafka-admin-client-thread | personneltrackingsystem-admin-0] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-20 07:03:17 [kafka-admin-client-thread | personneltrackingsystem-admin-0] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-20 07:03:17 [kafka-admin-client-thread | personneltrackingsystem-admin-0] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-20 07:03:17 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-20 07:03:17 [restartedMain] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pts-group-reset-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pts-group-reset
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-06-20 07:03:17 [restartedMain] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 07:03:17 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 07:03:17 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 07:03:17 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750392197746
2025-06-20 07:03:17 [restartedMain] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Subscribed to topic(s): turnstile-request
2025-06-20 07:03:17 [restartedMain] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pts-group-reset-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pts-group-reset
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-06-20 07:03:17 [restartedMain] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 07:03:17 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 07:03:17 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2025-06-20 07:03:17 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 07:03:17 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 07:03:17 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:03:17 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750392197782
2025-06-20 07:03:17 [restartedMain] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Subscribed to topic(s): email-notification
2025-06-20 07:03:17 [restartedMain] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pts-group-reset-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pts-group-reset
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-06-20 07:03:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 07:03:17 [restartedMain] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 07:03:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2025-06-20 07:03:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:03:17 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Request joining group due to: need to re-join with the given member-id: consumer-pts-group-reset-1-8647a791-822d-4762-b14e-3de837f0f9c0
2025-06-20 07:03:17 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:03:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Request joining group due to: need to re-join with the given member-id: consumer-pts-group-reset-2-8197e6d1-08ea-4d89-9719-ce666f833278
2025-06-20 07:03:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:03:17 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=32, memberId='consumer-pts-group-reset-1-8647a791-822d-4762-b14e-3de837f0f9c0', protocol='range'}
2025-06-20 07:03:17 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 07:03:17 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 07:03:17 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750392197841
2025-06-20 07:03:17 [restartedMain] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Subscribed to topic(s): turnstile-passage
2025-06-20 07:03:17 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Finished assignment for group at generation 32: {consumer-pts-group-reset-1-8647a791-822d-4762-b14e-3de837f0f9c0=Assignment(partitions=[turnstile-request-0])}
2025-06-20 07:03:17 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 07:03:17 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2025-06-20 07:03:17 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:03:17 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] SyncGroup failed: The group began another rebalance. Need to re-join the group. Sent generation was Generation{generationId=32, memberId='consumer-pts-group-reset-1-8647a791-822d-4762-b14e-3de837f0f9c0', protocol='range'}
2025-06-20 07:03:17 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Request joining group due to: rebalance failed due to 'The group is rebalancing, so a rejoin is needed.' (RebalanceInProgressException)
2025-06-20 07:03:17 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:03:17 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Request joining group due to: need to re-join with the given member-id: consumer-pts-group-reset-3-108d7b5b-ec14-4451-a859-51f7df517d97
2025-06-20 07:03:17 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:03:17 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=33, memberId='consumer-pts-group-reset-1-8647a791-822d-4762-b14e-3de837f0f9c0', protocol='range'}
2025-06-20 07:03:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=33, memberId='consumer-pts-group-reset-2-8197e6d1-08ea-4d89-9719-ce666f833278', protocol='range'}
2025-06-20 07:03:17 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=33, memberId='consumer-pts-group-reset-3-108d7b5b-ec14-4451-a859-51f7df517d97', protocol='range'}
2025-06-20 07:03:17 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - Started PersonelTrackingSystemApplication in 12.96 seconds (process running for 13.591)
2025-06-20 07:03:17 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Finished assignment for group at generation 33: {consumer-pts-group-reset-1-8647a791-822d-4762-b14e-3de837f0f9c0=Assignment(partitions=[turnstile-request-0]), consumer-pts-group-reset-3-108d7b5b-ec14-4451-a859-51f7df517d97=Assignment(partitions=[turnstile-passage-0]), consumer-pts-group-reset-2-8197e6d1-08ea-4d89-9719-ce666f833278=Assignment(partitions=[email-notification-0])}
2025-06-20 07:03:17 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=33, memberId='consumer-pts-group-reset-1-8647a791-822d-4762-b14e-3de837f0f9c0', protocol='range'}
2025-06-20 07:03:17 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=33, memberId='consumer-pts-group-reset-3-108d7b5b-ec14-4451-a859-51f7df517d97', protocol='range'}
2025-06-20 07:03:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=33, memberId='consumer-pts-group-reset-2-8197e6d1-08ea-4d89-9719-ce666f833278', protocol='range'}
2025-06-20 07:03:17 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[turnstile-request-0])
2025-06-20 07:03:17 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[turnstile-passage-0])
2025-06-20 07:03:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[email-notification-0])
2025-06-20 07:03:17 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Adding newly assigned partitions: turnstile-passage-0
2025-06-20 07:03:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Adding newly assigned partitions: email-notification-0
2025-06-20 07:03:17 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Adding newly assigned partitions: turnstile-request-0
2025-06-20 07:03:17 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition turnstile-request-0 to the committed offset FetchPosition{offset=9, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 07:03:17 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition email-notification-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 07:03:17 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition turnstile-passage-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 07:03:22 [http-nio-8080-exec-1] INFO  o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-06-20 07:03:22 [http-nio-8080-exec-1] INFO  c.p.c.impl.TurnstileControllerImpl - Turnstile passage request received: DtoTurnstilePassageFullRequest(wantedToEnterTurnstileId=1, personelId=1, operationType=IN, operationTimeStr=09:17:35)
2025-06-20 07:03:22 [http-nio-8080-exec-1] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Last operation found: IN (Personel: 1, Turnstile: 1)
2025-06-20 07:03:22 [http-nio-8080-exec-1] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Turnstile passage validation - Personel: 1, Turnstile: 1, Requested Operation: IN, Last Operation: IN
2025-06-20 07:03:22 [http-nio-8080-exec-1] WARN  c.p.s.i.TurnstileRegistrationLogServiceImpl - Only OUT operation is allowed when the last operation is IN (Personel: 1, Turnstile: 1)
2025-06-20 07:03:22 [http-nio-8080-exec-1] WARN  o.s.w.s.m.m.a.ExceptionHandlerExceptionResolver - Resolved [com.personneltrackingsystem.exception.ValidationException: turnstile.entry.requires.prior.exit]
2025-06-20 07:03:32 [http-nio-8080-exec-2] INFO  c.p.c.impl.TurnstileControllerImpl - Turnstile passage request received: DtoTurnstilePassageFullRequest(wantedToEnterTurnstileId=1, personelId=1, operationType=OUT, operationTimeStr=18:17:35)
2025-06-20 07:03:32 [http-nio-8080-exec-2] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Last operation found: IN (Personel: 1, Turnstile: 1)
2025-06-20 07:03:32 [http-nio-8080-exec-2] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Turnstile passage validation - Personel: 1, Turnstile: 1, Requested Operation: OUT, Last Operation: IN
2025-06-20 07:03:32 [http-nio-8080-exec-2] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Operation validated successfully: OUT (Personel: 1, Turnstile: 1)
2025-06-20 07:03:32 [http-nio-8080-exec-2] INFO  c.p.s.k.i.KafkaProducerServiceImpl - Sending turnstile request event to Kafka: TurnstileRequestEvent(wantedToEnterTurnstileId=1, personelId=1, operationType=OUT, operationTimeStr=18:17:35)
2025-06-20 07:03:32 [http-nio-8080-exec-2] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = personneltrackingsystem-producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 3
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.springframework.kafka.support.serializer.JsonSerializer

2025-06-20 07:03:32 [http-nio-8080-exec-2] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 07:03:32 [http-nio-8080-exec-2] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=personneltrackingsystem-producer-1] Instantiated an idempotent producer.
2025-06-20 07:03:32 [http-nio-8080-exec-2] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 07:03:32 [http-nio-8080-exec-2] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 07:03:32 [http-nio-8080-exec-2] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750392212821
2025-06-20 07:03:32 [kafka-producer-network-thread | personneltrackingsystem-producer-1] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=personneltrackingsystem-producer-1] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 07:03:32 [kafka-producer-network-thread | personneltrackingsystem-producer-1] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=personneltrackingsystem-producer-1] ProducerId set to 6 with epoch 0
2025-06-20 07:03:32 [kafka-producer-network-thread | personneltrackingsystem-producer-1] INFO  c.p.s.k.i.KafkaProducerServiceImpl - Turnstile request event sent successfully to topic: turnstile-request, partition: 0, offset: 9
2025-06-20 07:03:32 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.k.i.TurnstileRequestConsumerServiceImpl - Received turnstile request event: TurnstileRequestEvent(wantedToEnterTurnstileId=1, personelId=1, operationType=OUT, operationTimeStr=18:17:35)
2025-06-20 07:03:33 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.c.i.PersonelCacheServiceImpl - Personnel found in cache with ID: 1
2025-06-20 07:03:33 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Saved turnstile registration log: Personel 1 - Turnstile 1 - Operation OUT
2025-06-20 07:03:33 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.k.i.TurnstileRequestConsumerServiceImpl - Turnstile request event processed successfully
2025-06-20 07:03:44 [http-nio-8080-exec-4] INFO  c.p.c.impl.TurnstileControllerImpl - Turnstile passage request received: DtoTurnstilePassageFullRequest(wantedToEnterTurnstileId=1, personelId=1, operationType=IN, operationTimeStr=09:19:35)
2025-06-20 07:03:44 [http-nio-8080-exec-4] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Last operation found: OUT (Personel: 1, Turnstile: 1)
2025-06-20 07:03:44 [http-nio-8080-exec-4] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Turnstile passage validation - Personel: 1, Turnstile: 1, Requested Operation: IN, Last Operation: OUT
2025-06-20 07:03:44 [http-nio-8080-exec-4] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Operation validated successfully: IN (Personel: 1, Turnstile: 1)
2025-06-20 07:03:44 [http-nio-8080-exec-4] INFO  c.p.s.k.i.KafkaProducerServiceImpl - Sending turnstile request event to Kafka: TurnstileRequestEvent(wantedToEnterTurnstileId=1, personelId=1, operationType=IN, operationTimeStr=09:19:35)
2025-06-20 07:03:44 [kafka-producer-network-thread | personneltrackingsystem-producer-1] INFO  c.p.s.k.i.KafkaProducerServiceImpl - Turnstile request event sent successfully to topic: turnstile-request, partition: 0, offset: 10
2025-06-20 07:03:44 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.k.i.TurnstileRequestConsumerServiceImpl - Received turnstile request event: TurnstileRequestEvent(wantedToEnterTurnstileId=1, personelId=1, operationType=IN, operationTimeStr=09:19:35)
2025-06-20 07:03:44 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.c.i.PersonelCacheServiceImpl - Personnel found in cache with ID: 1
2025-06-20 07:03:44 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Saved turnstile registration log: Personel 1 - Turnstile 1 - Operation IN
2025-06-20 07:03:44 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.impl.TurnstileServiceImpl - Personnel Arif zcan is late, arrived at 09:19:35
2025-06-20 07:03:44 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] WARN  c.p.s.impl.TurnstileServiceImpl - Personnel Arif zcan is not associated with any units, cannot send late arrival notification
2025-06-20 07:03:44 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.k.i.TurnstileRequestConsumerServiceImpl - Turnstile request event processed successfully
2025-06-20 07:06:48 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Revoke previously assigned partitions email-notification-0
2025-06-20 07:06:48 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Revoke previously assigned partitions turnstile-passage-0
2025-06-20 07:06:48 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Revoke previously assigned partitions turnstile-request-0
2025-06-20 07:06:48 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Member consumer-pts-group-reset-2-8197e6d1-08ea-4d89-9719-ce666f833278 sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2025-06-20 07:06:48 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Member consumer-pts-group-reset-3-108d7b5b-ec14-4451-a859-51f7df517d97 sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2025-06-20 07:06:48 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Member consumer-pts-group-reset-1-8647a791-822d-4762-b14e-3de837f0f9c0 sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2025-06-20 07:06:48 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-20 07:06:48 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-20 07:06:48 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-20 07:06:48 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Request joining group due to: consumer pro-actively leaving the group
2025-06-20 07:06:48 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Request joining group due to: consumer pro-actively leaving the group
2025-06-20 07:06:48 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Request joining group due to: consumer pro-actively leaving the group
2025-06-20 07:06:48 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Unsubscribed all topics or patterns and assigned partitions
2025-06-20 07:06:48 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Unsubscribed all topics or patterns and assigned partitions
2025-06-20 07:06:48 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Unsubscribed all topics or patterns and assigned partitions
2025-06-20 07:06:48 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-20 07:06:48 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-20 07:06:48 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-20 07:06:48 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Request joining group due to: consumer pro-actively leaving the group
2025-06-20 07:06:48 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Request joining group due to: consumer pro-actively leaving the group
2025-06-20 07:06:48 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Request joining group due to: consumer pro-actively leaving the group
2025-06-20 07:06:48 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-20 07:06:48 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-20 07:06:48 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-20 07:06:48 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-20 07:06:48 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-pts-group-reset-1 unregistered
2025-06-20 07:06:48 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-20 07:06:48 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-20 07:06:48 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-20 07:06:48 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-20 07:06:48 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-20 07:06:48 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-20 07:06:48 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-20 07:06:48 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-20 07:06:48 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-pts-group-reset-2 unregistered
2025-06-20 07:06:48 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-pts-group-reset-3 unregistered
2025-06-20 07:06:48 [Thread-5] INFO  o.a.coyote.http11.Http11NioProtocol - Stopping ProtocolHandler ["http-nio-8080"]
2025-06-20 07:06:48 [Thread-5] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=personneltrackingsystem-producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2025-06-20 07:06:48 [Thread-5] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-20 07:06:48 [Thread-5] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-20 07:06:48 [Thread-5] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-20 07:06:48 [Thread-5] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-20 07:06:48 [Thread-5] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.producer for personneltrackingsystem-producer-1 unregistered
2025-06-20 07:06:48 [Thread-5] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-1 - Shutdown initiated...
2025-06-20 07:06:48 [Thread-5] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-1 - Shutdown completed.
2025-06-20 07:06:49 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - Starting PersonelTrackingSystemApplication using Java 21.0.7 with PID 13488 (C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes started by Lenovo in C:\Users\Lenovo\Desktop\personnel-tracking-system)
2025-06-20 07:06:49 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - No active profile set, falling back to 1 default profile: "default"
2025-06-20 07:06:49 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-20 07:06:49 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-20 07:06:49 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.30]
2025-06-20 07:06:49 [restartedMain] INFO  o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-20 07:06:49 [restartedMain] INFO  c.hazelcast.instance.AddressPicker - [LOCAL] [pts-cluster] [5.4.0] Interfaces is disabled, trying to pick one address from TCP-IP config addresses: [127.0.0.1]
2025-06-20 07:06:49 [restartedMain] INFO  com.hazelcast.system.logo - [127.0.0.1]:5701 [pts-cluster] [5.4.0] 
	o    o     o     o---o   o--o o      o---o     o     o----o o--o--o
	|    |    / \       /         |     /         / \    |         |   
	o----o       o     o   o----o |    o             o   o----o    |   
	|    |  *     \   /           |     \       *     \       |    |   
	o    o *       o o---o   o--o o----o o---o *       o o----o    o   
2025-06-20 07:06:49 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Copyright (c) 2008-2024, Hazelcast, Inc. All Rights Reserved.
2025-06-20 07:06:49 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Hazelcast Platform 5.4.0 (20240415) starting at [127.0.0.1]:5701
2025-06-20 07:06:49 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Cluster name: pts-cluster
2025-06-20 07:06:49 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Integrity Checker is disabled. Fail-fast on corrupted executables will not be performed. For more information, see the documentation for Integrity Checker.
2025-06-20 07:06:49 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] The Jet engine is disabled.
To enable the Jet engine on the members, do one of the following:
  - Change member config using Java API: config.getJetConfig().setEnabled(true)
  - Change XML/YAML configuration property: Set hazelcast.jet.enabled to true
  - Add system property: -Dhz.jet.enabled=true (for Hazelcast embedded, works only when loading config via Config.load)
  - Add environment variable: HZ_JET_ENABLED=true (recommended when running container image. For Hazelcast embedded, works only when loading config via Config.load)
2025-06-20 07:06:49 [restartedMain] INFO  com.hazelcast.system.security - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Enable DEBUG/FINE log level for log category com.hazelcast.system.security  or use -Dhazelcast.security.recommendations system property to see security recommendations and the status of current config.
2025-06-20 07:06:49 [restartedMain] INFO  com.hazelcast.instance.impl.Node - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Using TCP/IP discovery
2025-06-20 07:06:49 [restartedMain] WARN  com.hazelcast.cp.CPSubsystem - [127.0.0.1]:5701 [pts-cluster] [5.4.0] CP Subsystem is not enabled. CP data structures will operate in UNSAFE mode! Please note that UNSAFE mode will not provide strong consistency guarantees.
2025-06-20 07:06:49 [restartedMain] INFO  c.h.internal.diagnostics.Diagnostics - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Diagnostics disabled. To enable add -Dhazelcast.diagnostics.enabled=true to the JVM arguments.
2025-06-20 07:06:49 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is STARTING
2025-06-20 07:06:49 [hz.personnel-tracking-system.cached.thread-3] INFO  c.h.i.cluster.impl.TcpIpJoiner - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5702 is added to the blacklist.
2025-06-20 07:06:49 [hz.personnel-tracking-system.cached.thread-2] INFO  c.h.i.cluster.impl.TcpIpJoiner - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5703 is added to the blacklist.
2025-06-20 07:06:50 [restartedMain] INFO  c.h.internal.cluster.ClusterService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] 

Members {size:1, ver:1} [
	Member [127.0.0.1]:5701 - 533202a9-b30c-40da-8f7e-9123260ada00 this
]

2025-06-20 07:06:50 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is STARTED
2025-06-20 07:06:50 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-2 - Starting...
2025-06-20 07:06:51 [restartedMain] INFO  com.zaxxer.hikari.pool.HikariPool - HikariPool-2 - Added connection org.postgresql.jdbc.PgConnection@33e7dfe6
2025-06-20 07:06:51 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-2 - Start completed.
2025-06-20 07:06:51 [restartedMain] INFO  liquibase.changelog - Reading from dbpersonel.databasechangelog
2025-06-20 07:06:51 [restartedMain] INFO  liquibase.ui - Database is up to date, no changesets to execute
2025-06-20 07:06:51 [restartedMain] INFO  liquibase.changelog - Reading from dbpersonel.databasechangelog
2025-06-20 07:06:51 [restartedMain] INFO  liquibase.util - UPDATE SUMMARY
2025-06-20 07:06:51 [restartedMain] INFO  liquibase.util - Run:                          0
2025-06-20 07:06:51 [restartedMain] INFO  liquibase.util - Previously run:              28
2025-06-20 07:06:51 [restartedMain] INFO  liquibase.util - Filtered out:                 0
2025-06-20 07:06:51 [restartedMain] INFO  liquibase.util - -------------------------------
2025-06-20 07:06:51 [restartedMain] INFO  liquibase.util - Total change sets:           28
2025-06-20 07:06:51 [restartedMain] INFO  liquibase.util - Update summary generated
2025-06-20 07:06:51 [restartedMain] INFO  liquibase.command - Command execution complete
2025-06-20 07:06:51 [restartedMain] WARN  org.hibernate.orm.deprecation - HHH90000025: PostgreSQLDialect does not need to be specified explicitly using 'hibernate.dialect' (remove the property setting and it will be selected by default)
2025-06-20 07:06:52 [restartedMain] WARN  o.s.s.c.a.a.c.InitializeUserDetailsBeanManagerConfigurer$InitializeUserDetailsManagerConfigurer - Global AuthenticationManager configured with an AuthenticationProvider bean. UserDetailsService beans will not be used for username/password login. Consider removing the AuthenticationProvider bean. Alternatively, consider using the UserDetailsService in a manually instantiated DaoAuthenticationProvider.
2025-06-20 07:06:52 [restartedMain] INFO  c.p.s.impl.PersonelTypeServiceImpl - Checking for default personnel types...
2025-06-20 07:06:52 [restartedMain] INFO  c.p.s.impl.PersonelTypeServiceImpl - Personnel types already exist in the database.
2025-06-20 07:06:52 [restartedMain] WARN  o.s.b.a.o.j.JpaBaseConfiguration$JpaWebConfiguration - spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
2025-06-20 07:06:53 [restartedMain] INFO  o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = personneltrackingsystem-admin-1
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-20 07:06:53 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 07:06:53 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 07:06:53 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750392413263
2025-06-20 07:06:53 [kafka-admin-client-thread | personneltrackingsystem-admin-1] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for personneltrackingsystem-admin-1 unregistered
2025-06-20 07:06:53 [kafka-admin-client-thread | personneltrackingsystem-admin-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-20 07:06:53 [kafka-admin-client-thread | personneltrackingsystem-admin-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-20 07:06:53 [kafka-admin-client-thread | personneltrackingsystem-admin-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-20 07:06:53 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-20 07:06:53 [restartedMain] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pts-group-reset-4
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pts-group-reset
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-06-20 07:06:53 [restartedMain] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 07:06:53 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 07:06:53 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 07:06:53 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750392413304
2025-06-20 07:06:53 [restartedMain] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Subscribed to topic(s): turnstile-request
2025-06-20 07:06:53 [restartedMain] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pts-group-reset-5
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pts-group-reset
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-06-20 07:06:53 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 07:06:53 [restartedMain] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 07:06:53 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2025-06-20 07:06:53 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:06:53 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Request joining group due to: need to re-join with the given member-id: consumer-pts-group-reset-4-1d826d88-e8b0-45d5-9c18-758f43b41789
2025-06-20 07:06:53 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:06:53 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=35, memberId='consumer-pts-group-reset-4-1d826d88-e8b0-45d5-9c18-758f43b41789', protocol='range'}
2025-06-20 07:06:53 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 07:06:53 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Finished assignment for group at generation 35: {consumer-pts-group-reset-4-1d826d88-e8b0-45d5-9c18-758f43b41789=Assignment(partitions=[turnstile-request-0])}
2025-06-20 07:06:53 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 07:06:53 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750392413331
2025-06-20 07:06:53 [restartedMain] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-5, groupId=pts-group-reset] Subscribed to topic(s): email-notification
2025-06-20 07:06:53 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=35, memberId='consumer-pts-group-reset-4-1d826d88-e8b0-45d5-9c18-758f43b41789', protocol='range'}
2025-06-20 07:06:53 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[turnstile-request-0])
2025-06-20 07:06:53 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Adding newly assigned partitions: turnstile-request-0
2025-06-20 07:06:53 [restartedMain] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pts-group-reset-6
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pts-group-reset
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-06-20 07:06:53 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition turnstile-request-0 to the committed offset FetchPosition{offset=11, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 07:06:53 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-pts-group-reset-5, groupId=pts-group-reset] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 07:06:53 [restartedMain] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 07:06:53 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-5, groupId=pts-group-reset] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2025-06-20 07:06:53 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-5, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:06:53 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-5, groupId=pts-group-reset] Request joining group due to: need to re-join with the given member-id: consumer-pts-group-reset-5-2b916d67-3bb4-4108-b462-3bfa6c269caf
2025-06-20 07:06:53 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-5, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:06:53 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 07:06:53 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 07:06:53 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750392413358
2025-06-20 07:06:53 [restartedMain] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-6, groupId=pts-group-reset] Subscribed to topic(s): turnstile-passage
2025-06-20 07:06:53 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-pts-group-reset-6, groupId=pts-group-reset] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 07:06:53 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-6, groupId=pts-group-reset] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2025-06-20 07:06:53 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-6, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:06:53 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-6, groupId=pts-group-reset] Request joining group due to: need to re-join with the given member-id: consumer-pts-group-reset-6-8d0f0056-0064-4bb5-89de-e313276f69bc
2025-06-20 07:06:53 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - Started PersonelTrackingSystemApplication in 4.391 seconds (process running for 229.082)
2025-06-20 07:06:53 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-6, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:06:56 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Request joining group due to: group is already rebalancing
2025-06-20 07:06:56 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Revoke previously assigned partitions turnstile-request-0
2025-06-20 07:06:56 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:06:56 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-5, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=36, memberId='consumer-pts-group-reset-5-2b916d67-3bb4-4108-b462-3bfa6c269caf', protocol='range'}
2025-06-20 07:06:56 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-6, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=36, memberId='consumer-pts-group-reset-6-8d0f0056-0064-4bb5-89de-e313276f69bc', protocol='range'}
2025-06-20 07:06:56 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=36, memberId='consumer-pts-group-reset-4-1d826d88-e8b0-45d5-9c18-758f43b41789', protocol='range'}
2025-06-20 07:06:56 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Finished assignment for group at generation 36: {consumer-pts-group-reset-6-8d0f0056-0064-4bb5-89de-e313276f69bc=Assignment(partitions=[turnstile-passage-0]), consumer-pts-group-reset-5-2b916d67-3bb4-4108-b462-3bfa6c269caf=Assignment(partitions=[email-notification-0]), consumer-pts-group-reset-4-1d826d88-e8b0-45d5-9c18-758f43b41789=Assignment(partitions=[turnstile-request-0])}
2025-06-20 07:06:56 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=36, memberId='consumer-pts-group-reset-4-1d826d88-e8b0-45d5-9c18-758f43b41789', protocol='range'}
2025-06-20 07:06:56 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-6, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=36, memberId='consumer-pts-group-reset-6-8d0f0056-0064-4bb5-89de-e313276f69bc', protocol='range'}
2025-06-20 07:06:56 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-5, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=36, memberId='consumer-pts-group-reset-5-2b916d67-3bb4-4108-b462-3bfa6c269caf', protocol='range'}
2025-06-20 07:06:56 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[turnstile-request-0])
2025-06-20 07:06:56 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-6, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[turnstile-passage-0])
2025-06-20 07:06:56 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-5, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[email-notification-0])
2025-06-20 07:06:56 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Adding newly assigned partitions: turnstile-request-0
2025-06-20 07:06:56 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-6, groupId=pts-group-reset] Adding newly assigned partitions: turnstile-passage-0
2025-06-20 07:06:56 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-5, groupId=pts-group-reset] Adding newly assigned partitions: email-notification-0
2025-06-20 07:06:56 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition turnstile-passage-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 07:06:56 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition turnstile-request-0 to the committed offset FetchPosition{offset=11, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 07:06:56 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition email-notification-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 07:07:18 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Revoke previously assigned partitions turnstile-request-0
2025-06-20 07:07:18 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-5, groupId=pts-group-reset] Revoke previously assigned partitions email-notification-0
2025-06-20 07:07:18 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-6, groupId=pts-group-reset] Revoke previously assigned partitions turnstile-passage-0
2025-06-20 07:07:18 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Member consumer-pts-group-reset-4-1d826d88-e8b0-45d5-9c18-758f43b41789 sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2025-06-20 07:07:18 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-5, groupId=pts-group-reset] Member consumer-pts-group-reset-5-2b916d67-3bb4-4108-b462-3bfa6c269caf sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2025-06-20 07:07:18 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-6, groupId=pts-group-reset] Member consumer-pts-group-reset-6-8d0f0056-0064-4bb5-89de-e313276f69bc sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2025-06-20 07:07:18 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-20 07:07:18 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-5, groupId=pts-group-reset] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-20 07:07:18 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-6, groupId=pts-group-reset] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-20 07:07:18 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Request joining group due to: consumer pro-actively leaving the group
2025-06-20 07:07:18 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-5, groupId=pts-group-reset] Request joining group due to: consumer pro-actively leaving the group
2025-06-20 07:07:18 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-6, groupId=pts-group-reset] Request joining group due to: consumer pro-actively leaving the group
2025-06-20 07:07:18 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Unsubscribed all topics or patterns and assigned partitions
2025-06-20 07:07:18 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-5, groupId=pts-group-reset] Unsubscribed all topics or patterns and assigned partitions
2025-06-20 07:07:18 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-6, groupId=pts-group-reset] Unsubscribed all topics or patterns and assigned partitions
2025-06-20 07:07:18 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-20 07:07:18 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-5, groupId=pts-group-reset] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-20 07:07:18 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-6, groupId=pts-group-reset] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-20 07:07:18 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-4, groupId=pts-group-reset] Request joining group due to: consumer pro-actively leaving the group
2025-06-20 07:07:18 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-5, groupId=pts-group-reset] Request joining group due to: consumer pro-actively leaving the group
2025-06-20 07:07:18 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-6, groupId=pts-group-reset] Request joining group due to: consumer pro-actively leaving the group
2025-06-20 07:07:18 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-20 07:07:18 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-20 07:07:18 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-20 07:07:18 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-20 07:07:18 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-pts-group-reset-4 unregistered
2025-06-20 07:07:18 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-20 07:07:18 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-20 07:07:18 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-20 07:07:18 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-20 07:07:18 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-20 07:07:18 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-20 07:07:18 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-20 07:07:18 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-20 07:07:18 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-pts-group-reset-5 unregistered
2025-06-20 07:07:18 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-pts-group-reset-6 unregistered
2025-06-20 07:07:18 [Thread-7] INFO  o.a.coyote.http11.Http11NioProtocol - Stopping ProtocolHandler ["http-nio-8080"]
2025-06-20 07:07:18 [Thread-7] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-2 - Shutdown initiated...
2025-06-20 07:07:18 [Thread-7] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-2 - Shutdown completed.
2025-06-20 07:07:18 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - Starting PersonelTrackingSystemApplication using Java 21.0.7 with PID 13488 (C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes started by Lenovo in C:\Users\Lenovo\Desktop\personnel-tracking-system)
2025-06-20 07:07:18 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - No active profile set, falling back to 1 default profile: "default"
2025-06-20 07:07:19 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-20 07:07:19 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-20 07:07:19 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.30]
2025-06-20 07:07:19 [restartedMain] INFO  o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-20 07:07:19 [restartedMain] INFO  c.hazelcast.instance.AddressPicker - [LOCAL] [pts-cluster] [5.4.0] Interfaces is disabled, trying to pick one address from TCP-IP config addresses: [127.0.0.1]
2025-06-20 07:07:19 [restartedMain] INFO  com.hazelcast.system.logo - [127.0.0.1]:5701 [pts-cluster] [5.4.0] 
	o    o     o     o---o   o--o o      o---o     o     o----o o--o--o
	|    |    / \       /         |     /         / \    |         |   
	o----o       o     o   o----o |    o             o   o----o    |   
	|    |  *     \   /           |     \       *     \       |    |   
	o    o *       o o---o   o--o o----o o---o *       o o----o    o   
2025-06-20 07:07:19 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Copyright (c) 2008-2024, Hazelcast, Inc. All Rights Reserved.
2025-06-20 07:07:19 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Hazelcast Platform 5.4.0 (20240415) starting at [127.0.0.1]:5701
2025-06-20 07:07:19 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Cluster name: pts-cluster
2025-06-20 07:07:19 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Integrity Checker is disabled. Fail-fast on corrupted executables will not be performed. For more information, see the documentation for Integrity Checker.
2025-06-20 07:07:19 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] The Jet engine is disabled.
To enable the Jet engine on the members, do one of the following:
  - Change member config using Java API: config.getJetConfig().setEnabled(true)
  - Change XML/YAML configuration property: Set hazelcast.jet.enabled to true
  - Add system property: -Dhz.jet.enabled=true (for Hazelcast embedded, works only when loading config via Config.load)
  - Add environment variable: HZ_JET_ENABLED=true (recommended when running container image. For Hazelcast embedded, works only when loading config via Config.load)
2025-06-20 07:07:19 [restartedMain] INFO  com.hazelcast.system.security - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Enable DEBUG/FINE log level for log category com.hazelcast.system.security  or use -Dhazelcast.security.recommendations system property to see security recommendations and the status of current config.
2025-06-20 07:07:19 [restartedMain] INFO  com.hazelcast.instance.impl.Node - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Using TCP/IP discovery
2025-06-20 07:07:19 [restartedMain] WARN  com.hazelcast.cp.CPSubsystem - [127.0.0.1]:5701 [pts-cluster] [5.4.0] CP Subsystem is not enabled. CP data structures will operate in UNSAFE mode! Please note that UNSAFE mode will not provide strong consistency guarantees.
2025-06-20 07:07:19 [restartedMain] INFO  c.h.internal.diagnostics.Diagnostics - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Diagnostics disabled. To enable add -Dhazelcast.diagnostics.enabled=true to the JVM arguments.
2025-06-20 07:07:19 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is STARTING
2025-06-20 07:07:19 [hz.personnel-tracking-system.cached.thread-3] INFO  c.h.i.cluster.impl.TcpIpJoiner - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5703 is added to the blacklist.
2025-06-20 07:07:19 [hz.personnel-tracking-system.cached.thread-2] INFO  c.h.i.cluster.impl.TcpIpJoiner - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5702 is added to the blacklist.
2025-06-20 07:07:20 [restartedMain] INFO  c.h.internal.cluster.ClusterService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] 

Members {size:1, ver:1} [
	Member [127.0.0.1]:5701 - 7b4faea7-4f02-4c2d-a00a-23287233fb5d this
]

2025-06-20 07:07:20 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is STARTED
2025-06-20 07:07:20 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-3 - Starting...
2025-06-20 07:07:20 [restartedMain] INFO  com.zaxxer.hikari.pool.HikariPool - HikariPool-3 - Added connection org.postgresql.jdbc.PgConnection@3bf29672
2025-06-20 07:07:20 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-3 - Start completed.
2025-06-20 07:07:21 [restartedMain] INFO  liquibase.changelog - Reading from dbpersonel.databasechangelog
2025-06-20 07:07:21 [restartedMain] INFO  liquibase.ui - Database is up to date, no changesets to execute
2025-06-20 07:07:21 [restartedMain] INFO  liquibase.changelog - Reading from dbpersonel.databasechangelog
2025-06-20 07:07:21 [restartedMain] INFO  liquibase.util - UPDATE SUMMARY
2025-06-20 07:07:21 [restartedMain] INFO  liquibase.util - Run:                          0
2025-06-20 07:07:21 [restartedMain] INFO  liquibase.util - Previously run:              28
2025-06-20 07:07:21 [restartedMain] INFO  liquibase.util - Filtered out:                 0
2025-06-20 07:07:21 [restartedMain] INFO  liquibase.util - -------------------------------
2025-06-20 07:07:21 [restartedMain] INFO  liquibase.util - Total change sets:           28
2025-06-20 07:07:21 [restartedMain] INFO  liquibase.util - Update summary generated
2025-06-20 07:07:21 [restartedMain] INFO  liquibase.command - Command execution complete
2025-06-20 07:07:21 [restartedMain] WARN  org.hibernate.orm.deprecation - HHH90000025: PostgreSQLDialect does not need to be specified explicitly using 'hibernate.dialect' (remove the property setting and it will be selected by default)
2025-06-20 07:07:21 [restartedMain] WARN  o.s.s.c.a.a.c.InitializeUserDetailsBeanManagerConfigurer$InitializeUserDetailsManagerConfigurer - Global AuthenticationManager configured with an AuthenticationProvider bean. UserDetailsService beans will not be used for username/password login. Consider removing the AuthenticationProvider bean. Alternatively, consider using the UserDetailsService in a manually instantiated DaoAuthenticationProvider.
2025-06-20 07:07:22 [restartedMain] INFO  c.p.s.impl.PersonelTypeServiceImpl - Checking for default personnel types...
2025-06-20 07:07:22 [restartedMain] INFO  c.p.s.impl.PersonelTypeServiceImpl - Personnel types already exist in the database.
2025-06-20 07:07:22 [restartedMain] WARN  o.s.b.a.o.j.JpaBaseConfiguration$JpaWebConfiguration - spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
2025-06-20 07:07:22 [restartedMain] INFO  o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = personneltrackingsystem-admin-2
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-20 07:07:22 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 07:07:22 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 07:07:22 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750392442439
2025-06-20 07:07:22 [kafka-admin-client-thread | personneltrackingsystem-admin-2] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for personneltrackingsystem-admin-2 unregistered
2025-06-20 07:07:22 [kafka-admin-client-thread | personneltrackingsystem-admin-2] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-20 07:07:22 [kafka-admin-client-thread | personneltrackingsystem-admin-2] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-20 07:07:22 [kafka-admin-client-thread | personneltrackingsystem-admin-2] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-20 07:07:22 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-20 07:07:22 [restartedMain] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pts-group-reset-7
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pts-group-reset
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-06-20 07:07:22 [restartedMain] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 07:07:22 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 07:07:22 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 07:07:22 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750392442461
2025-06-20 07:07:22 [restartedMain] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-7, groupId=pts-group-reset] Subscribed to topic(s): turnstile-request
2025-06-20 07:07:22 [restartedMain] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pts-group-reset-8
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pts-group-reset
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-06-20 07:07:22 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-pts-group-reset-7, groupId=pts-group-reset] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 07:07:22 [restartedMain] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 07:07:22 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-7, groupId=pts-group-reset] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2025-06-20 07:07:22 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-7, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:07:22 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-7, groupId=pts-group-reset] Request joining group due to: need to re-join with the given member-id: consumer-pts-group-reset-7-205a19b7-5484-4b0c-975c-d80f88f5120d
2025-06-20 07:07:22 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 07:07:22 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 07:07:22 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-7, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:07:22 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750392442475
2025-06-20 07:07:22 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-7, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=38, memberId='consumer-pts-group-reset-7-205a19b7-5484-4b0c-975c-d80f88f5120d', protocol='range'}
2025-06-20 07:07:22 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-7, groupId=pts-group-reset] Finished assignment for group at generation 38: {consumer-pts-group-reset-7-205a19b7-5484-4b0c-975c-d80f88f5120d=Assignment(partitions=[turnstile-request-0])}
2025-06-20 07:07:22 [restartedMain] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-8, groupId=pts-group-reset] Subscribed to topic(s): email-notification
2025-06-20 07:07:22 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-7, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=38, memberId='consumer-pts-group-reset-7-205a19b7-5484-4b0c-975c-d80f88f5120d', protocol='range'}
2025-06-20 07:07:22 [restartedMain] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pts-group-reset-9
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pts-group-reset
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-06-20 07:07:22 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-7, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[turnstile-request-0])
2025-06-20 07:07:22 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-pts-group-reset-8, groupId=pts-group-reset] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 07:07:22 [restartedMain] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 07:07:22 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-7, groupId=pts-group-reset] Adding newly assigned partitions: turnstile-request-0
2025-06-20 07:07:22 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-8, groupId=pts-group-reset] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2025-06-20 07:07:22 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-8, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:07:22 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition turnstile-request-0 to the committed offset FetchPosition{offset=11, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 07:07:22 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-8, groupId=pts-group-reset] Request joining group due to: need to re-join with the given member-id: consumer-pts-group-reset-8-910b38ab-8401-42b1-b834-9de4b2b11548
2025-06-20 07:07:22 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 07:07:22 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 07:07:22 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-8, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:07:22 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750392442501
2025-06-20 07:07:22 [restartedMain] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-9, groupId=pts-group-reset] Subscribed to topic(s): turnstile-passage
2025-06-20 07:07:22 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-pts-group-reset-9, groupId=pts-group-reset] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 07:07:22 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-9, groupId=pts-group-reset] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2025-06-20 07:07:22 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-9, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:07:22 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-9, groupId=pts-group-reset] Request joining group due to: need to re-join with the given member-id: consumer-pts-group-reset-9-39ef1a75-1193-4c24-8c25-3635d5305250
2025-06-20 07:07:22 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-9, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:07:22 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - Started PersonelTrackingSystemApplication in 3.871 seconds (process running for 258.224)
2025-06-20 07:07:25 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-7, groupId=pts-group-reset] Request joining group due to: group is already rebalancing
2025-06-20 07:07:25 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-7, groupId=pts-group-reset] Revoke previously assigned partitions turnstile-request-0
2025-06-20 07:07:25 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-7, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:07:25 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-9, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=39, memberId='consumer-pts-group-reset-9-39ef1a75-1193-4c24-8c25-3635d5305250', protocol='range'}
2025-06-20 07:07:25 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-8, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=39, memberId='consumer-pts-group-reset-8-910b38ab-8401-42b1-b834-9de4b2b11548', protocol='range'}
2025-06-20 07:07:25 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-7, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=39, memberId='consumer-pts-group-reset-7-205a19b7-5484-4b0c-975c-d80f88f5120d', protocol='range'}
2025-06-20 07:07:25 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-7, groupId=pts-group-reset] Finished assignment for group at generation 39: {consumer-pts-group-reset-9-39ef1a75-1193-4c24-8c25-3635d5305250=Assignment(partitions=[turnstile-passage-0]), consumer-pts-group-reset-8-910b38ab-8401-42b1-b834-9de4b2b11548=Assignment(partitions=[email-notification-0]), consumer-pts-group-reset-7-205a19b7-5484-4b0c-975c-d80f88f5120d=Assignment(partitions=[turnstile-request-0])}
2025-06-20 07:07:25 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-8, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=39, memberId='consumer-pts-group-reset-8-910b38ab-8401-42b1-b834-9de4b2b11548', protocol='range'}
2025-06-20 07:07:25 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-9, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=39, memberId='consumer-pts-group-reset-9-39ef1a75-1193-4c24-8c25-3635d5305250', protocol='range'}
2025-06-20 07:07:25 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-7, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=39, memberId='consumer-pts-group-reset-7-205a19b7-5484-4b0c-975c-d80f88f5120d', protocol='range'}
2025-06-20 07:07:25 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-8, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[email-notification-0])
2025-06-20 07:07:25 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-9, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[turnstile-passage-0])
2025-06-20 07:07:25 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-7, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[turnstile-request-0])
2025-06-20 07:07:25 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-8, groupId=pts-group-reset] Adding newly assigned partitions: email-notification-0
2025-06-20 07:07:25 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-9, groupId=pts-group-reset] Adding newly assigned partitions: turnstile-passage-0
2025-06-20 07:07:25 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-7, groupId=pts-group-reset] Adding newly assigned partitions: turnstile-request-0
2025-06-20 07:07:25 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition email-notification-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 07:07:25 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition turnstile-passage-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 07:07:25 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition turnstile-request-0 to the committed offset FetchPosition{offset=11, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 07:08:54 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-9, groupId=pts-group-reset] Revoke previously assigned partitions turnstile-passage-0
2025-06-20 07:08:54 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-8, groupId=pts-group-reset] Revoke previously assigned partitions email-notification-0
2025-06-20 07:08:54 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-7, groupId=pts-group-reset] Revoke previously assigned partitions turnstile-request-0
2025-06-20 07:08:54 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-9, groupId=pts-group-reset] Member consumer-pts-group-reset-9-39ef1a75-1193-4c24-8c25-3635d5305250 sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2025-06-20 07:08:54 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-8, groupId=pts-group-reset] Member consumer-pts-group-reset-8-910b38ab-8401-42b1-b834-9de4b2b11548 sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2025-06-20 07:08:54 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-7, groupId=pts-group-reset] Member consumer-pts-group-reset-7-205a19b7-5484-4b0c-975c-d80f88f5120d sending LeaveGroup request to coordinator host.docker.internal:9092 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2025-06-20 07:08:54 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-9, groupId=pts-group-reset] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-20 07:08:54 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-8, groupId=pts-group-reset] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-20 07:08:54 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-7, groupId=pts-group-reset] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-20 07:08:54 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-9, groupId=pts-group-reset] Request joining group due to: consumer pro-actively leaving the group
2025-06-20 07:08:54 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-8, groupId=pts-group-reset] Request joining group due to: consumer pro-actively leaving the group
2025-06-20 07:08:54 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-7, groupId=pts-group-reset] Request joining group due to: consumer pro-actively leaving the group
2025-06-20 07:08:54 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-9, groupId=pts-group-reset] Unsubscribed all topics or patterns and assigned partitions
2025-06-20 07:08:54 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-8, groupId=pts-group-reset] Unsubscribed all topics or patterns and assigned partitions
2025-06-20 07:08:54 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-9, groupId=pts-group-reset] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-20 07:08:54 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-7, groupId=pts-group-reset] Unsubscribed all topics or patterns and assigned partitions
2025-06-20 07:08:54 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-8, groupId=pts-group-reset] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-20 07:08:54 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-9, groupId=pts-group-reset] Request joining group due to: consumer pro-actively leaving the group
2025-06-20 07:08:54 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-7, groupId=pts-group-reset] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-20 07:08:54 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-7, groupId=pts-group-reset] Request joining group due to: consumer pro-actively leaving the group
2025-06-20 07:08:54 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-8, groupId=pts-group-reset] Request joining group due to: consumer pro-actively leaving the group
2025-06-20 07:08:55 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-20 07:08:55 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-20 07:08:55 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-20 07:08:55 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-20 07:08:55 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-20 07:08:55 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-20 07:08:55 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-20 07:08:55 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-20 07:08:55 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter
2025-06-20 07:08:55 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-20 07:08:55 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-20 07:08:55 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-20 07:08:55 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-pts-group-reset-7 unregistered
2025-06-20 07:08:55 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-pts-group-reset-9 unregistered
2025-06-20 07:08:55 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-pts-group-reset-8 unregistered
2025-06-20 07:08:55 [Thread-11] INFO  o.a.coyote.http11.Http11NioProtocol - Stopping ProtocolHandler ["http-nio-8080"]
2025-06-20 07:08:55 [Thread-11] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-3 - Shutdown initiated...
2025-06-20 07:08:55 [Thread-11] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-3 - Shutdown completed.
2025-06-20 07:08:55 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - Starting PersonelTrackingSystemApplication using Java 21.0.7 with PID 13488 (C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes started by Lenovo in C:\Users\Lenovo\Desktop\personnel-tracking-system)
2025-06-20 07:08:55 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - No active profile set, falling back to 1 default profile: "default"
2025-06-20 07:08:55 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-20 07:08:55 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-20 07:08:55 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.30]
2025-06-20 07:08:55 [restartedMain] INFO  o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-20 07:08:55 [restartedMain] INFO  c.hazelcast.instance.AddressPicker - [LOCAL] [pts-cluster] [5.4.0] Interfaces is disabled, trying to pick one address from TCP-IP config addresses: [127.0.0.1]
2025-06-20 07:08:55 [restartedMain] INFO  com.hazelcast.system.logo - [127.0.0.1]:5701 [pts-cluster] [5.4.0] 
	o    o     o     o---o   o--o o      o---o     o     o----o o--o--o
	|    |    / \       /         |     /         / \    |         |   
	o----o       o     o   o----o |    o             o   o----o    |   
	|    |  *     \   /           |     \       *     \       |    |   
	o    o *       o o---o   o--o o----o o---o *       o o----o    o   
2025-06-20 07:08:55 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Copyright (c) 2008-2024, Hazelcast, Inc. All Rights Reserved.
2025-06-20 07:08:55 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Hazelcast Platform 5.4.0 (20240415) starting at [127.0.0.1]:5701
2025-06-20 07:08:55 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Cluster name: pts-cluster
2025-06-20 07:08:55 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Integrity Checker is disabled. Fail-fast on corrupted executables will not be performed. For more information, see the documentation for Integrity Checker.
2025-06-20 07:08:55 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] The Jet engine is disabled.
To enable the Jet engine on the members, do one of the following:
  - Change member config using Java API: config.getJetConfig().setEnabled(true)
  - Change XML/YAML configuration property: Set hazelcast.jet.enabled to true
  - Add system property: -Dhz.jet.enabled=true (for Hazelcast embedded, works only when loading config via Config.load)
  - Add environment variable: HZ_JET_ENABLED=true (recommended when running container image. For Hazelcast embedded, works only when loading config via Config.load)
2025-06-20 07:08:56 [restartedMain] INFO  com.hazelcast.system.security - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Enable DEBUG/FINE log level for log category com.hazelcast.system.security  or use -Dhazelcast.security.recommendations system property to see security recommendations and the status of current config.
2025-06-20 07:08:56 [restartedMain] INFO  com.hazelcast.instance.impl.Node - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Using TCP/IP discovery
2025-06-20 07:08:56 [restartedMain] WARN  com.hazelcast.cp.CPSubsystem - [127.0.0.1]:5701 [pts-cluster] [5.4.0] CP Subsystem is not enabled. CP data structures will operate in UNSAFE mode! Please note that UNSAFE mode will not provide strong consistency guarantees.
2025-06-20 07:08:56 [restartedMain] INFO  c.h.internal.diagnostics.Diagnostics - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Diagnostics disabled. To enable add -Dhazelcast.diagnostics.enabled=true to the JVM arguments.
2025-06-20 07:08:56 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is STARTING
2025-06-20 07:08:56 [hz.personnel-tracking-system.cached.thread-1] INFO  c.h.i.cluster.impl.TcpIpJoiner - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5703 is added to the blacklist.
2025-06-20 07:08:56 [hz.personnel-tracking-system.cached.thread-3] INFO  c.h.i.cluster.impl.TcpIpJoiner - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5702 is added to the blacklist.
2025-06-20 07:08:57 [restartedMain] INFO  c.h.internal.cluster.ClusterService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] 

Members {size:1, ver:1} [
	Member [127.0.0.1]:5701 - 43a2c572-3c93-4ae7-898c-010a9915d7c8 this
]

2025-06-20 07:08:57 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is STARTED
2025-06-20 07:08:57 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-4 - Starting...
2025-06-20 07:08:57 [restartedMain] INFO  com.zaxxer.hikari.pool.HikariPool - HikariPool-4 - Added connection org.postgresql.jdbc.PgConnection@26470bac
2025-06-20 07:08:57 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-4 - Start completed.
2025-06-20 07:08:57 [restartedMain] INFO  liquibase.changelog - Reading from dbpersonel.databasechangelog
2025-06-20 07:08:57 [restartedMain] INFO  liquibase.changelog - Change failed validation!
2025-06-20 07:08:57 [restartedMain] INFO  liquibase.command - Logging exception.
2025-06-20 07:08:57 [restartedMain] INFO  liquibase.ui - ERROR: Exception Details
2025-06-20 07:08:57 [restartedMain] INFO  liquibase.ui - ERROR: Exception Primary Class:  ValidationFailedException
2025-06-20 07:08:57 [restartedMain] INFO  liquibase.ui - ERROR: Exception Primary Reason:  Validation Failed:
     1 changesets check sum
          db/changelog/changes/002-insert-initial-data.yaml::5::liquibase was: 9:72d3a05465cfc3ddc522447fe86e8499 but is now: 9:1a87b07b88bb1e6079828c20a91ab632

2025-06-20 07:08:57 [restartedMain] INFO  liquibase.ui - ERROR: Exception Primary Source:  4.29.2
2025-06-20 07:08:57 [restartedMain] INFO  liquibase.command - Command execution complete
2025-06-20 07:08:57 [restartedMain] ERROR o.s.b.w.e.tomcat.TomcatStarter - Error starting Tomcat context. Exception: org.springframework.beans.factory.UnsatisfiedDependencyException. Message: Error creating bean with name 'jwtAuthenticationFilter' defined in file [C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes\com\personneltrackingsystem\filter\JwtAuthenticationFilter.class]: Unsatisfied dependency expressed through constructor parameter 1: Error creating bean with name 'customUserDetailsServiceImpl' defined in file [C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes\com\personneltrackingsystem\service\impl\CustomUserDetailsServiceImpl.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'userRepository' defined in com.personneltrackingsystem.repository.UserRepository defined in @EnableJpaRepositories declared on JpaRepositoriesRegistrar.EnableJpaRepositoriesConfiguration: Cannot resolve reference to bean 'jpaSharedEM_entityManagerFactory' while setting bean property 'entityManager'
2025-06-20 07:08:57 [restartedMain] INFO  o.a.catalina.core.StandardService - Stopping service [Tomcat]
2025-06-20 07:08:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.event-16] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:435)
 com.hazelcast.internal.util.executor.StripedExecutor$Worker.run(StripedExecutor.java:227)
2025-06-20 07:08:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.event-17] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:435)
 com.hazelcast.internal.util.executor.StripedExecutor$Worker.run(StripedExecutor.java:227)
2025-06-20 07:08:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.event-18] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:435)
 com.hazelcast.internal.util.executor.StripedExecutor$Worker.run(StripedExecutor.java:227)
2025-06-20 07:08:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.event-19] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:435)
 com.hazelcast.internal.util.executor.StripedExecutor$Worker.run(StripedExecutor.java:227)
2025-06-20 07:08:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.event-20] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:435)
 com.hazelcast.internal.util.executor.StripedExecutor$Worker.run(StripedExecutor.java:227)
2025-06-20 07:08:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.MetricsRegistry.thread-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1177)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 07:08:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.MetricsRegistry.thread-2] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:269)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1763)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 07:08:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.migration] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:269)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1763)
 java.base/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:460)
 com.hazelcast.internal.partition.impl.MigrationQueue.poll(MigrationQueue.java:48)
 com.hazelcast.internal.partition.impl.MigrationThread.doRun(MigrationThread.java:91)
 com.hazelcast.internal.partition.impl.MigrationThread.run(MigrationThread.java:66)
2025-06-20 07:08:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.InvocationMonitorThread] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:269)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1763)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 07:08:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.SlowOperationDetectorThread] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/java.lang.Thread.sleep0(Native Method)
 java.base/java.lang.Thread.sleep(Thread.java:558)
 java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
 com.hazelcast.spi.impl.operationexecutor.slowoperationdetector.SlowOperationDetector$DetectorThread.sleepInterval(SlowOperationDetector.java:280)
 com.hazelcast.spi.impl.operationexecutor.slowoperationdetector.SlowOperationDetector$DetectorThread.run(SlowOperationDetector.java:153)
2025-06-20 07:08:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-in-0] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:142)
 com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:292)
 com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249)
 com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111)
2025-06-20 07:08:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-in-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:142)
 com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:292)
 com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249)
 com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111)
2025-06-20 07:08:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-in-2] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:142)
 com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:292)
 com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249)
 com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111)
2025-06-20 07:08:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-out-0] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:142)
 com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:292)
 com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249)
 com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111)
2025-06-20 07:08:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-out-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:142)
 com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:292)
 com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249)
 com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111)
2025-06-20 07:08:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-out-2] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:142)
 com.hazelcast.internal.networking.nio.NioThread.selectLoop(NioThread.java:292)
 com.hazelcast.internal.networking.nio.NioThread.executeRun(NioThread.java:249)
 com.hazelcast.internal.util.executor.HazelcastManagedThread.run(HazelcastManagedThread.java:111)
2025-06-20 07:08:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.BalancerThread] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:269)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1763)
 java.base/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:460)
 com.hazelcast.internal.networking.nio.iobalancer.IOBalancerThread.run(IOBalancerThread.java:65)
2025-06-20 07:08:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.IO.thread-Acceptor] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.WEPoll.wait(Native Method)
 java.base/sun.nio.ch.WEPollSelectorImpl.doSelect(WEPollSelectorImpl.java:114)
 java.base/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:130)
 java.base/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:147)
 com.hazelcast.internal.server.tcp.TcpServerAcceptor$AcceptorIOThread.acceptLoop(TcpServerAcceptor.java:186)
 com.hazelcast.internal.server.tcp.TcpServerAcceptor$AcceptorIOThread.run(TcpServerAcceptor.java:172)
2025-06-20 07:08:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.TcpServer.thread-1] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1177)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 07:08:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.TcpServer.thread-2] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:269)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1763)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 07:08:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.TcpServer.thread-4] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1177)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 07:08:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.TcpServer.thread-3] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.park(LockSupport.java:371)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionNode.block(AbstractQueuedSynchronizer.java:519)
 java.base/java.util.concurrent.ForkJoinPool.unmanagedBlock(ForkJoinPool.java:3778)
 java.base/java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3723)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1712)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1177)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 07:08:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [hz.personnel-tracking-system.HealthMonitor] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/java.lang.Thread.sleep0(Native Method)
 java.base/java.lang.Thread.sleep(Thread.java:558)
 java.base/java.util.concurrent.TimeUnit.sleep(TimeUnit.java:446)
 com.hazelcast.internal.diagnostics.HealthMonitor$HealthMonitorThread.run(HealthMonitor.java:164)
2025-06-20 07:08:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [HikariPool-4 housekeeper] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/jdk.internal.misc.Unsafe.park(Native Method)
 java.base/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:269)
 java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1763)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182)
 java.base/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899)
 java.base/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1070)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 07:08:57 [restartedMain] WARN  o.a.c.loader.WebappClassLoaderBase - The web application [ROOT] appears to have started a thread named [HikariPool-4 connection adder] but has failed to stop it. This is very likely to create a memory leak. Stack trace of thread:
 java.base/sun.nio.ch.Net.poll(Native Method)
 java.base/sun.nio.ch.NioSocketImpl.park(NioSocketImpl.java:191)
 java.base/sun.nio.ch.NioSocketImpl.timedRead(NioSocketImpl.java:280)
 java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:304)
 java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:346)
 java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:796)
 java.base/java.net.Socket$SocketInputStream.read(Socket.java:1099)
 org.postgresql.core.VisibleBufferedInputStream.readMore(VisibleBufferedInputStream.java:162)
 org.postgresql.core.VisibleBufferedInputStream.ensureBytes(VisibleBufferedInputStream.java:129)
 org.postgresql.core.VisibleBufferedInputStream.ensureBytes(VisibleBufferedInputStream.java:114)
 org.postgresql.core.VisibleBufferedInputStream.read(VisibleBufferedInputStream.java:74)
 org.postgresql.core.PGStream.receiveChar(PGStream.java:467)
 org.postgresql.core.v3.ConnectionFactoryImpl.enableSSL(ConnectionFactoryImpl.java:594)
 org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:195)
 org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:262)
 org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)
 org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:273)
 org.postgresql.Driver.makeConnection(Driver.java:446)
 org.postgresql.Driver.connect(Driver.java:298)
 com.zaxxer.hikari.util.DriverDataSource.getConnection(DriverDataSource.java:137)
 com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:360)
 com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:202)
 com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:461)
 com.zaxxer.hikari.pool.HikariPool$PoolEntryCreator.call(HikariPool.java:724)
 com.zaxxer.hikari.pool.HikariPool$PoolEntryCreator.call(HikariPool.java:703)
 java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
 java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
 java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
 java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 07:08:57 [restartedMain] WARN  o.s.b.w.s.c.AnnotationConfigServletWebServerApplicationContext - Exception encountered during context initialization - cancelling refresh attempt: org.springframework.context.ApplicationContextException: Unable to start web server
2025-06-20 07:08:57 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-4 - Shutdown initiated...
2025-06-20 07:08:57 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-4 - Shutdown completed.
2025-06-20 07:08:57 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is SHUTTING_DOWN
2025-06-20 07:08:57 [restartedMain] INFO  com.hazelcast.instance.impl.Node - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Shutting down connection manager...
2025-06-20 07:08:57 [restartedMain] INFO  com.hazelcast.instance.impl.Node - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Shutting down node engine...
2025-06-20 07:08:57 [restartedMain] INFO  c.h.instance.impl.NodeExtension - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Destroying node NodeExtension.
2025-06-20 07:08:57 [restartedMain] INFO  com.hazelcast.instance.impl.Node - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Hazelcast Shutdown is completed in 15 ms.
2025-06-20 07:08:57 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is SHUTDOWN
2025-06-20 07:08:57 [restartedMain] ERROR o.s.boot.SpringApplication - Application run failed
org.springframework.context.ApplicationContextException: Unable to start web server
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.onRefresh(ServletWebServerApplicationContext.java:165)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:619)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:754)
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:456)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:335)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1363)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1352)
	at com.personneltrackingsystem.PersonelTrackingSystemApplication.main(PersonelTrackingSystemApplication.java:12)
	at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)
	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:50)
Caused by: org.springframework.boot.web.server.WebServerException: Unable to start embedded Tomcat
	at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.initialize(TomcatWebServer.java:147)
	at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.<init>(TomcatWebServer.java:107)
	at org.springframework.boot.web.embedded.tomcat.TomcatServletWebServerFactory.getTomcatWebServer(TomcatServletWebServerFactory.java:516)
	at org.springframework.boot.web.embedded.tomcat.TomcatServletWebServerFactory.getWebServer(TomcatServletWebServerFactory.java:222)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.createWebServer(ServletWebServerApplicationContext.java:188)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.onRefresh(ServletWebServerApplicationContext.java:162)
	... 11 common frames omitted
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'jwtAuthenticationFilter' defined in file [C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes\com\personneltrackingsystem\filter\JwtAuthenticationFilter.class]: Unsatisfied dependency expressed through constructor parameter 1: Error creating bean with name 'customUserDetailsServiceImpl' defined in file [C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes\com\personneltrackingsystem\service\impl\CustomUserDetailsServiceImpl.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'userRepository' defined in com.personneltrackingsystem.repository.UserRepository defined in @EnableJpaRepositories declared on JpaRepositoriesRegistrar.EnableJpaRepositoriesConfiguration: Cannot resolve reference to bean 'jpaSharedEM_entityManagerFactory' while setting bean property 'entityManager'
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:795)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:237)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1375)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1212)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:205)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.getOrderedBeansOfType(ServletContextInitializerBeans.java:211)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.addAsRegistrationBean(ServletContextInitializerBeans.java:174)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.addAsRegistrationBean(ServletContextInitializerBeans.java:169)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.addAdaptableBeans(ServletContextInitializerBeans.java:154)
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.<init>(ServletContextInitializerBeans.java:87)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.getServletContextInitializerBeans(ServletWebServerApplicationContext.java:266)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.selfInitialize(ServletWebServerApplicationContext.java:240)
	at org.springframework.boot.web.embedded.tomcat.TomcatStarter.onStartup(TomcatStarter.java:52)
	at org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:4412)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1203)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1193)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at org.apache.tomcat.util.threads.InlineExecutorService.execute(InlineExecutorService.java:75)
	at java.base/java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:145)
	at org.apache.catalina.core.ContainerBase.startInternal(ContainerBase.java:749)
	at org.apache.catalina.core.StandardHost.startInternal(StandardHost.java:772)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1203)
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1193)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at org.apache.tomcat.util.threads.InlineExecutorService.execute(InlineExecutorService.java:75)
	at java.base/java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:145)
	at org.apache.catalina.core.ContainerBase.startInternal(ContainerBase.java:749)
	at org.apache.catalina.core.StandardEngine.startInternal(StandardEngine.java:203)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.core.StandardService.startInternal(StandardService.java:415)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.core.StandardServer.startInternal(StandardServer.java:870)
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:164)
	at org.apache.catalina.startup.Tomcat.start(Tomcat.java:437)
	at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.initialize(TomcatWebServer.java:128)
	... 16 common frames omitted
Caused by: org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'customUserDetailsServiceImpl' defined in file [C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes\com\personneltrackingsystem\service\impl\CustomUserDetailsServiceImpl.class]: Unsatisfied dependency expressed through constructor parameter 0: Error creating bean with name 'userRepository' defined in com.personneltrackingsystem.repository.UserRepository defined in @EnableJpaRepositories declared on JpaRepositoriesRegistrar.EnableJpaRepositoriesConfiguration: Cannot resolve reference to bean 'jpaSharedEM_entityManagerFactory' while setting bean property 'entityManager'
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:795)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:237)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1375)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1212)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1443)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1353)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:904)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:782)
	... 57 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'userRepository' defined in com.personneltrackingsystem.repository.UserRepository defined in @EnableJpaRepositories declared on JpaRepositoriesRegistrar.EnableJpaRepositoriesConfiguration: Cannot resolve reference to bean 'jpaSharedEM_entityManagerFactory' while setting bean property 'entityManager'
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:377)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:135)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyPropertyValues(AbstractAutowireCapableBeanFactory.java:1705)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1454)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:599)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:254)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1443)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:1353)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveAutowiredArgument(ConstructorResolver.java:904)
	at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:782)
	... 71 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'jpaSharedEM_entityManagerFactory': Cannot resolve reference to bean 'entityManagerFactory' while setting constructor argument
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:377)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:135)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveConstructorArguments(ConstructorResolver.java:682)
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:509)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1355)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1185)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:562)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:365)
	... 85 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'liquibase' defined in class path resource [org/springframework/boot/autoconfigure/liquibase/LiquibaseAutoConfiguration$LiquibaseConfiguration.class]: liquibase.exception.CommandExecutionException: liquibase.exception.ValidationFailedException: Validation Failed:
     1 changesets check sum
          db/changelog/changes/002-insert-initial-data.yaml::5::liquibase was: 9:72d3a05465cfc3ddc522447fe86e8499 but is now: 9:1a87b07b88bb1e6079828c20a91ab632

	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1806)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:600)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:522)
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:337)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:335)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:313)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:365)
	... 97 common frames omitted
Caused by: liquibase.exception.UnexpectedLiquibaseException: liquibase.exception.CommandExecutionException: liquibase.exception.ValidationFailedException: Validation Failed:
     1 changesets check sum
          db/changelog/changes/002-insert-initial-data.yaml::5::liquibase was: 9:72d3a05465cfc3ddc522447fe86e8499 but is now: 9:1a87b07b88bb1e6079828c20a91ab632

	at liquibase.integration.spring.SpringLiquibase.afterPropertiesSet(SpringLiquibase.java:267)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1853)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1802)
	... 106 common frames omitted
Caused by: liquibase.exception.CommandExecutionException: liquibase.exception.ValidationFailedException: Validation Failed:
     1 changesets check sum
          db/changelog/changes/002-insert-initial-data.yaml::5::liquibase was: 9:72d3a05465cfc3ddc522447fe86e8499 but is now: 9:1a87b07b88bb1e6079828c20a91ab632

	at liquibase.command.CommandScope.execute(CommandScope.java:258)
	at liquibase.Liquibase.lambda$update$0(Liquibase.java:216)
	at liquibase.Scope.lambda$child$0(Scope.java:191)
	at liquibase.Scope.child(Scope.java:200)
	at liquibase.Scope.child(Scope.java:190)
	at liquibase.Scope.child(Scope.java:169)
	at liquibase.Liquibase.runInScope(Liquibase.java:1329)
	at liquibase.Liquibase.update(Liquibase.java:205)
	at liquibase.Liquibase.update(Liquibase.java:188)
	at liquibase.integration.spring.SpringLiquibase.performUpdate(SpringLiquibase.java:305)
	at liquibase.integration.spring.SpringLiquibase.lambda$afterPropertiesSet$0(SpringLiquibase.java:257)
	at liquibase.Scope.lambda$child$0(Scope.java:191)
	at liquibase.Scope.child(Scope.java:200)
	at liquibase.Scope.child(Scope.java:190)
	at liquibase.Scope.child(Scope.java:169)
	at liquibase.Scope.child(Scope.java:257)
	at liquibase.integration.spring.SpringLiquibase.afterPropertiesSet(SpringLiquibase.java:250)
	... 108 common frames omitted
Caused by: liquibase.exception.ValidationFailedException: Validation Failed:
     1 changesets check sum
          db/changelog/changes/002-insert-initial-data.yaml::5::liquibase was: 9:72d3a05465cfc3ddc522447fe86e8499 but is now: 9:1a87b07b88bb1e6079828c20a91ab632

	at liquibase.changelog.DatabaseChangeLog.validate(DatabaseChangeLog.java:398)
	at liquibase.command.core.helpers.DatabaseChangelogCommandStep.run(DatabaseChangelogCommandStep.java:92)
	at liquibase.command.CommandScope.execute(CommandScope.java:220)
	... 124 common frames omitted
2025-06-20 07:09:25 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - Starting PersonelTrackingSystemApplication using Java 21.0.7 with PID 17028 (C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes started by Lenovo in C:\Users\Lenovo\Desktop\personnel-tracking-system)
2025-06-20 07:09:25 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - No active profile set, falling back to 1 default profile: "default"
2025-06-20 07:09:28 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-20 07:09:28 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-20 07:09:28 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.30]
2025-06-20 07:09:28 [restartedMain] INFO  o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-20 07:09:28 [restartedMain] WARN  c.h.i.impl.HazelcastInstanceFactory - Hazelcast is starting in a Java modular environment (Java 9 and newer) but without proper access to required Java packages. Use additional Java arguments to provide Hazelcast access to Java internal API. The internal API access is used to get the best performance results. Arguments to be used:
 --add-modules java.se --add-exports java.base/jdk.internal.ref=ALL-UNNAMED --add-opens java.base/java.lang=ALL-UNNAMED --add-opens java.base/sun.nio.ch=ALL-UNNAMED --add-opens java.management/sun.management=ALL-UNNAMED --add-opens jdk.management/com.sun.management.internal=ALL-UNNAMED
2025-06-20 07:09:28 [restartedMain] INFO  c.hazelcast.instance.AddressPicker - [LOCAL] [pts-cluster] [5.4.0] Interfaces is disabled, trying to pick one address from TCP-IP config addresses: [127.0.0.1]
2025-06-20 07:09:28 [restartedMain] INFO  com.hazelcast.system.logo - [127.0.0.1]:5701 [pts-cluster] [5.4.0] 
	o    o     o     o---o   o--o o      o---o     o     o----o o--o--o
	|    |    / \       /         |     /         / \    |         |   
	o----o       o     o   o----o |    o             o   o----o    |   
	|    |  *     \   /           |     \       *     \       |    |   
	o    o *       o o---o   o--o o----o o---o *       o o----o    o   
2025-06-20 07:09:28 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Copyright (c) 2008-2024, Hazelcast, Inc. All Rights Reserved.
2025-06-20 07:09:28 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Hazelcast Platform 5.4.0 (20240415) starting at [127.0.0.1]:5701
2025-06-20 07:09:28 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Cluster name: pts-cluster
2025-06-20 07:09:28 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Integrity Checker is disabled. Fail-fast on corrupted executables will not be performed. For more information, see the documentation for Integrity Checker.
2025-06-20 07:09:28 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] The Jet engine is disabled.
To enable the Jet engine on the members, do one of the following:
  - Change member config using Java API: config.getJetConfig().setEnabled(true)
  - Change XML/YAML configuration property: Set hazelcast.jet.enabled to true
  - Add system property: -Dhz.jet.enabled=true (for Hazelcast embedded, works only when loading config via Config.load)
  - Add environment variable: HZ_JET_ENABLED=true (recommended when running container image. For Hazelcast embedded, works only when loading config via Config.load)
2025-06-20 07:09:29 [restartedMain] INFO  com.hazelcast.system.security - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Enable DEBUG/FINE log level for log category com.hazelcast.system.security  or use -Dhazelcast.security.recommendations system property to see security recommendations and the status of current config.
2025-06-20 07:09:29 [restartedMain] INFO  com.hazelcast.instance.impl.Node - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Using TCP/IP discovery
2025-06-20 07:09:29 [restartedMain] WARN  com.hazelcast.cp.CPSubsystem - [127.0.0.1]:5701 [pts-cluster] [5.4.0] CP Subsystem is not enabled. CP data structures will operate in UNSAFE mode! Please note that UNSAFE mode will not provide strong consistency guarantees.
2025-06-20 07:09:29 [restartedMain] INFO  c.h.internal.diagnostics.Diagnostics - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Diagnostics disabled. To enable add -Dhazelcast.diagnostics.enabled=true to the JVM arguments.
2025-06-20 07:09:29 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is STARTING
2025-06-20 07:09:29 [hz.personnel-tracking-system.cached.thread-3] INFO  c.h.i.cluster.impl.TcpIpJoiner - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5702 is added to the blacklist.
2025-06-20 07:09:29 [hz.personnel-tracking-system.cached.thread-2] INFO  c.h.i.cluster.impl.TcpIpJoiner - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5703 is added to the blacklist.
2025-06-20 07:09:30 [restartedMain] INFO  c.h.internal.cluster.ClusterService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] 

Members {size:1, ver:1} [
	Member [127.0.0.1]:5701 - 86ef8b7c-e36c-41f7-8d66-22968ee7df6e this
]

2025-06-20 07:09:30 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is STARTED
2025-06-20 07:09:30 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-1 - Starting...
2025-06-20 07:09:30 [restartedMain] INFO  com.zaxxer.hikari.pool.HikariPool - HikariPool-1 - Added connection org.postgresql.jdbc.PgConnection@10c8fd12
2025-06-20 07:09:30 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-1 - Start completed.
2025-06-20 07:09:31 [restartedMain] INFO  liquibase.changelog - Creating database history table with name: dbpersonel.databasechangelog
2025-06-20 07:09:31 [restartedMain] INFO  liquibase.changelog - Reading from dbpersonel.databasechangelog
2025-06-20 07:09:31 [restartedMain] INFO  liquibase.lockservice - Successfully acquired change log lock
2025-06-20 07:09:31 [restartedMain] INFO  liquibase.command - Using deploymentId: 0392571755
2025-06-20 07:09:31 [restartedMain] INFO  liquibase.changelog - Reading from dbpersonel.databasechangelog
2025-06-20 07:09:31 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::1::liquibase
2025-06-20 07:09:31 [restartedMain] WARN  liquibase.executor - "dbpersonel" emas zaten mevcut, atlanyor
2025-06-20 07:09:31 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:09:31 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::1::liquibase ran successfully in 19ms
2025-06-20 07:09:31 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::2::liquibase
2025-06-20 07:09:31 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:09:31 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:09:31 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:09:31 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:09:31 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:09:31 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:09:31 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:09:31 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:09:31 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:09:31 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:09:31 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:09:31 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:09:31 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:09:31 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:09:31 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:09:31 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:09:31 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:09:31 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::2::liquibase ran successfully in 141ms
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::3::liquibase
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - Table personel_type created
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::3::liquibase ran successfully in 14ms
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::4::liquibase
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - Table building created
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::4::liquibase ran successfully in 9ms
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::5::liquibase
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - Table floor created
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::5::liquibase ran successfully in 14ms
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::6::liquibase
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - Table personel created
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::6::liquibase ran successfully in 13ms
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::7::liquibase
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - Table unit created
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::7::liquibase ran successfully in 12ms
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::8::liquibase
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - Table personel_unit created
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::8::liquibase ran successfully in 8ms
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::9::liquibase
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - Table gate created
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::9::liquibase ran successfully in 7ms
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::10::liquibase
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - Table turnstile created
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::10::liquibase ran successfully in 7ms
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::11::liquibase
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - Table turnstile_registration_log created
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::11::liquibase ran successfully in 9ms
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::12::liquibase
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - Table working_hours created
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::12::liquibase ran successfully in 10ms
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::13::liquibase
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - Table salary created
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::13::liquibase ran successfully in 14ms
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::14::liquibase
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - Table user created
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::14::liquibase ran successfully in 11ms
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::15::liquibase
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - Table permission created
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::15::liquibase ran successfully in 14ms
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::16::liquibase
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - Table role_permission created
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::16::liquibase ran successfully in 10ms
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::17::liquibase
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::17::liquibase ran successfully in 65ms
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::1::liquibase
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - New row inserted into personel_type
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - New row inserted into personel_type
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - New row inserted into personel_type
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::1::liquibase ran successfully in 9ms
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::2::liquibase
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - New row inserted into building
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - New row inserted into building
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - New row inserted into building
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::2::liquibase ran successfully in 8ms
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::3::liquibase
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - Data loaded from 'db/changelog/data/floor-data.csv' into table 'floor'
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::3::liquibase ran successfully in 55ms
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::4::liquibase
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - New row inserted into personel
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - New row inserted into personel
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - New row inserted into personel
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::4::liquibase ran successfully in 10ms
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::5::liquibase
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - New row inserted into unit
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - New row inserted into unit
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - New row inserted into unit
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::5::liquibase ran successfully in 12ms
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::6::liquibase
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - New row inserted into personel_unit
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - New row inserted into personel_unit
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - New row inserted into personel_unit
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::6::liquibase ran successfully in 19ms
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::7::liquibase
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - Data loaded from 'db/changelog/data/gate-data.csv' into table 'gate'
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::7::liquibase ran successfully in 16ms
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::8::liquibase
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - Data loaded from 'db/changelog/data/turnstile-data.csv' into table 'turnstile'
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::8::liquibase ran successfully in 13ms
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::9::liquibase
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - New row inserted into working_hours
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - New row inserted into working_hours
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - New row inserted into working_hours
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::9::liquibase ran successfully in 10ms
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::10::liquibase
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - New row inserted into permission
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - New row inserted into permission
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - New row inserted into permission
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - New row inserted into permission
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - New row inserted into permission
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::10::liquibase ran successfully in 21ms
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::11::liquibase
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - New row inserted into role_permission
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - New row inserted into role_permission
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - New row inserted into role_permission
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - New row inserted into role_permission
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - New row inserted into role_permission
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::11::liquibase ran successfully in 15ms
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.util - UPDATE SUMMARY
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.util - Run:                         28
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.util - Previously run:               0
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.util - Filtered out:                 0
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.util - -------------------------------
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.util - Total change sets:           28
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.util - Update summary generated
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.command - Update command completed successfully.
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.ui - Liquibase: Update has been successful. Rows affected: 124
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.lockservice - Successfully released change log lock
2025-06-20 07:09:32 [restartedMain] INFO  liquibase.command - Command execution complete
2025-06-20 07:09:33 [restartedMain] WARN  org.hibernate.orm.deprecation - HHH90000025: PostgreSQLDialect does not need to be specified explicitly using 'hibernate.dialect' (remove the property setting and it will be selected by default)
2025-06-20 07:09:34 [restartedMain] WARN  o.s.s.c.a.a.c.InitializeUserDetailsBeanManagerConfigurer$InitializeUserDetailsManagerConfigurer - Global AuthenticationManager configured with an AuthenticationProvider bean. UserDetailsService beans will not be used for username/password login. Consider removing the AuthenticationProvider bean. Alternatively, consider using the UserDetailsService in a manually instantiated DaoAuthenticationProvider.
2025-06-20 07:09:35 [restartedMain] INFO  c.p.s.impl.PersonelTypeServiceImpl - Checking for default personnel types...
2025-06-20 07:09:35 [restartedMain] INFO  c.p.s.impl.PersonelTypeServiceImpl - Personnel types already exist in the database.
2025-06-20 07:09:36 [restartedMain] WARN  o.s.b.a.o.j.JpaBaseConfiguration$JpaWebConfiguration - spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
2025-06-20 07:09:37 [restartedMain] INFO  o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = personneltrackingsystem-admin-0
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-20 07:09:37 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 07:09:37 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 07:09:37 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750392577586
2025-06-20 07:09:37 [kafka-admin-client-thread | personneltrackingsystem-admin-0] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for personneltrackingsystem-admin-0 unregistered
2025-06-20 07:09:37 [kafka-admin-client-thread | personneltrackingsystem-admin-0] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-20 07:09:37 [kafka-admin-client-thread | personneltrackingsystem-admin-0] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-20 07:09:37 [kafka-admin-client-thread | personneltrackingsystem-admin-0] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-20 07:09:37 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-20 07:09:37 [restartedMain] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pts-group-reset-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pts-group-reset
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-06-20 07:09:37 [restartedMain] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 07:09:38 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 07:09:38 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 07:09:38 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750392578014
2025-06-20 07:09:38 [restartedMain] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Subscribed to topic(s): turnstile-request
2025-06-20 07:09:38 [restartedMain] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pts-group-reset-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pts-group-reset
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-06-20 07:09:38 [restartedMain] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 07:09:38 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 07:09:38 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 07:09:38 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 07:09:38 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750392578050
2025-06-20 07:09:38 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2025-06-20 07:09:38 [restartedMain] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Subscribed to topic(s): email-notification
2025-06-20 07:09:38 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:09:38 [restartedMain] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pts-group-reset-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pts-group-reset
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-06-20 07:09:38 [restartedMain] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 07:09:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 07:09:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2025-06-20 07:09:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:09:38 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 07:09:38 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 07:09:38 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750392578087
2025-06-20 07:09:38 [restartedMain] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Subscribed to topic(s): turnstile-passage
2025-06-20 07:09:38 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Request joining group due to: need to re-join with the given member-id: consumer-pts-group-reset-1-2b998a1e-d484-423d-bcf0-b0eae34cb3e1
2025-06-20 07:09:38 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:09:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Request joining group due to: need to re-join with the given member-id: consumer-pts-group-reset-2-9b97760f-9909-45a7-8940-a51e577d7e0c
2025-06-20 07:09:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:09:38 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=41, memberId='consumer-pts-group-reset-1-2b998a1e-d484-423d-bcf0-b0eae34cb3e1', protocol='range'}
2025-06-20 07:09:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=41, memberId='consumer-pts-group-reset-2-9b97760f-9909-45a7-8940-a51e577d7e0c', protocol='range'}
2025-06-20 07:09:38 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 07:09:38 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Finished assignment for group at generation 41: {consumer-pts-group-reset-1-2b998a1e-d484-423d-bcf0-b0eae34cb3e1=Assignment(partitions=[turnstile-request-0]), consumer-pts-group-reset-2-9b97760f-9909-45a7-8940-a51e577d7e0c=Assignment(partitions=[email-notification-0])}
2025-06-20 07:09:38 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - Started PersonelTrackingSystemApplication in 12.75 seconds (process running for 13.322)
2025-06-20 07:09:38 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2025-06-20 07:09:38 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:09:38 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=41, memberId='consumer-pts-group-reset-1-2b998a1e-d484-423d-bcf0-b0eae34cb3e1', protocol='range'}
2025-06-20 07:09:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=41, memberId='consumer-pts-group-reset-2-9b97760f-9909-45a7-8940-a51e577d7e0c', protocol='range'}
2025-06-20 07:09:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[email-notification-0])
2025-06-20 07:09:38 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[turnstile-request-0])
2025-06-20 07:09:38 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Request joining group due to: need to re-join with the given member-id: consumer-pts-group-reset-3-042812a9-522d-4588-b6f1-ffb60b5a1be8
2025-06-20 07:09:38 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:09:38 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Adding newly assigned partitions: turnstile-request-0
2025-06-20 07:09:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Adding newly assigned partitions: email-notification-0
2025-06-20 07:09:38 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition email-notification-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 07:09:38 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition turnstile-request-0 to the committed offset FetchPosition{offset=11, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 07:09:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Request joining group due to: group is already rebalancing
2025-06-20 07:09:41 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Request joining group due to: group is already rebalancing
2025-06-20 07:09:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Revoke previously assigned partitions email-notification-0
2025-06-20 07:09:41 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Revoke previously assigned partitions turnstile-request-0
2025-06-20 07:09:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:09:41 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:09:41 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=42, memberId='consumer-pts-group-reset-3-042812a9-522d-4588-b6f1-ffb60b5a1be8', protocol='range'}
2025-06-20 07:09:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=42, memberId='consumer-pts-group-reset-2-9b97760f-9909-45a7-8940-a51e577d7e0c', protocol='range'}
2025-06-20 07:09:41 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=42, memberId='consumer-pts-group-reset-1-2b998a1e-d484-423d-bcf0-b0eae34cb3e1', protocol='range'}
2025-06-20 07:09:41 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Finished assignment for group at generation 42: {consumer-pts-group-reset-3-042812a9-522d-4588-b6f1-ffb60b5a1be8=Assignment(partitions=[turnstile-passage-0]), consumer-pts-group-reset-1-2b998a1e-d484-423d-bcf0-b0eae34cb3e1=Assignment(partitions=[turnstile-request-0]), consumer-pts-group-reset-2-9b97760f-9909-45a7-8940-a51e577d7e0c=Assignment(partitions=[email-notification-0])}
2025-06-20 07:09:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=42, memberId='consumer-pts-group-reset-2-9b97760f-9909-45a7-8940-a51e577d7e0c', protocol='range'}
2025-06-20 07:09:41 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=42, memberId='consumer-pts-group-reset-1-2b998a1e-d484-423d-bcf0-b0eae34cb3e1', protocol='range'}
2025-06-20 07:09:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[email-notification-0])
2025-06-20 07:09:41 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=42, memberId='consumer-pts-group-reset-3-042812a9-522d-4588-b6f1-ffb60b5a1be8', protocol='range'}
2025-06-20 07:09:41 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[turnstile-request-0])
2025-06-20 07:09:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Adding newly assigned partitions: email-notification-0
2025-06-20 07:09:41 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Adding newly assigned partitions: turnstile-request-0
2025-06-20 07:09:41 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[turnstile-passage-0])
2025-06-20 07:09:41 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Adding newly assigned partitions: turnstile-passage-0
2025-06-20 07:09:41 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition turnstile-request-0 to the committed offset FetchPosition{offset=11, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 07:09:41 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition email-notification-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 07:09:41 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition turnstile-passage-0 to the committed offset FetchPosition{offset=2, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 07:09:43 [http-nio-8080-exec-1] INFO  o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-06-20 07:09:43 [http-nio-8080-exec-1] INFO  c.p.c.impl.TurnstileControllerImpl - Turnstile passage request received: DtoTurnstilePassageFullRequest(wantedToEnterTurnstileId=1, personelId=1, operationType=IN, operationTimeStr=09:19:35)
2025-06-20 07:09:43 [http-nio-8080-exec-1] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - No previous operation found for personel 1 on turnstile 1
2025-06-20 07:09:43 [http-nio-8080-exec-1] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Turnstile passage validation - Personel: 1, Turnstile: 1, Requested Operation: IN, Last Operation: null
2025-06-20 07:09:43 [http-nio-8080-exec-1] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Operation validated successfully: IN (Personel: 1, Turnstile: 1)
2025-06-20 07:09:43 [http-nio-8080-exec-1] INFO  c.p.s.k.i.KafkaProducerServiceImpl - Sending turnstile request event to Kafka: TurnstileRequestEvent(wantedToEnterTurnstileId=1, personelId=1, operationType=IN, operationTimeStr=09:19:35)
2025-06-20 07:09:43 [http-nio-8080-exec-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = personneltrackingsystem-producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 3
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.springframework.kafka.support.serializer.JsonSerializer

2025-06-20 07:09:43 [http-nio-8080-exec-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 07:09:43 [http-nio-8080-exec-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=personneltrackingsystem-producer-1] Instantiated an idempotent producer.
2025-06-20 07:09:43 [http-nio-8080-exec-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 07:09:43 [http-nio-8080-exec-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 07:09:43 [http-nio-8080-exec-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750392583962
2025-06-20 07:09:43 [kafka-producer-network-thread | personneltrackingsystem-producer-1] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=personneltrackingsystem-producer-1] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 07:09:43 [kafka-producer-network-thread | personneltrackingsystem-producer-1] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=personneltrackingsystem-producer-1] ProducerId set to 7 with epoch 0
2025-06-20 07:09:44 [kafka-producer-network-thread | personneltrackingsystem-producer-1] INFO  c.p.s.k.i.KafkaProducerServiceImpl - Turnstile request event sent successfully to topic: turnstile-request, partition: 0, offset: 11
2025-06-20 07:09:44 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.k.i.TurnstileRequestConsumerServiceImpl - Received turnstile request event: TurnstileRequestEvent(wantedToEnterTurnstileId=1, personelId=1, operationType=IN, operationTimeStr=09:19:35)
2025-06-20 07:09:44 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.c.i.PersonelCacheServiceImpl - Personnel found in cache with ID: 1
2025-06-20 07:09:44 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Saved turnstile registration log: Personel 1 - Turnstile 1 - Operation IN
2025-06-20 07:09:44 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.k.i.KafkaProducerServiceImpl - Sending turnstile passage event to Kafka: TurnstilePassageEvent(personelId=1, personelName=Arif zcan, personelEmail=zcanarif@gmail.com, turnstileId=1, turnstileName=Turnstile 1, passageTime=2025-06-20T09:19:35, operationType=IN, recipientEmail=arifozcan23@gmail.com, recipientName=Cem Baydogan, isAdminNotification=true, isLateArrival=true, minutesLate=19)
2025-06-20 07:09:44 [kafka-producer-network-thread | personneltrackingsystem-producer-1] INFO  c.p.s.k.i.KafkaProducerServiceImpl - Turnstile passage event sent successfully to topic: turnstile-passage, partition: 0, offset: 2
2025-06-20 07:09:44 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  c.p.s.k.i.TurnstilePassageConsumerServiceImpl - Received turnstile passage event: TurnstilePassageEvent(personelId=1, personelName=Arif zcan, personelEmail=zcanarif@gmail.com, turnstileId=1, turnstileName=Turnstile 1, passageTime=2025-06-20T09:19:35, operationType=IN, recipientEmail=arifozcan23@gmail.com, recipientName=Cem Baydogan, isAdminNotification=true, isLateArrival=true, minutesLate=19)
2025-06-20 07:09:44 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.k.i.TurnstileRequestConsumerServiceImpl - Turnstile request event processed successfully
2025-06-20 07:09:44 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  c.p.s.k.i.KafkaProducerServiceImpl - Sending email event to Kafka: EmailEvent(recipientEmail=arifozcan23@gmail.com, recipientName=Cem Baydogan, subject=Late Arrival Notification - Arif zcan, message=Dear Cem Baydogan,

This is to inform you that Arif zcan has arrived late to work today.

Details:
- Personnel Name: Arif zcan
- Arrival Time: 2025-06-20 09:19:35
- Minutes Late: 19 minutes
- Entrance: Turnstile 1


This is an automated message from the Personnel Tracking System.
, timestamp=2025-06-20T07:09:44.557085700)
2025-06-20 07:09:44 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  c.p.s.k.i.TurnstilePassageConsumerServiceImpl - Turnstile passage event processed successfully
2025-06-20 07:09:44 [kafka-producer-network-thread | personneltrackingsystem-producer-1] INFO  c.p.s.k.i.KafkaProducerServiceImpl - Email event sent successfully to topic: email-notification, partition: 0, offset: 2
2025-06-20 07:09:44 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  c.p.s.k.i.EmailConsumerServiceImpl - Received email event: EmailEvent(recipientEmail=arifozcan23@gmail.com, recipientName=Cem Baydogan, subject=Late Arrival Notification - Arif zcan, message=Dear Cem Baydogan,

This is to inform you that Arif zcan has arrived late to work today.

Details:
- Personnel Name: Arif zcan
- Arrival Time: 2025-06-20 09:19:35
- Minutes Late: 19 minutes
- Entrance: Turnstile 1


This is an automated message from the Personnel Tracking System.
, timestamp=2025-06-20T07:09:44.557085700)
2025-06-20 07:09:46 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  c.p.s.k.i.EmailConsumerServiceImpl - Email sent successfully to arifozcan23@gmail.com
2025-06-20 07:09:46 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  c.p.s.k.i.EmailConsumerServiceImpl - Email event processed successfully
2025-06-20 07:11:21 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - Starting PersonelTrackingSystemApplication using Java 21.0.7 with PID 2672 (C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes started by Lenovo in C:\Users\Lenovo\Desktop\personnel-tracking-system)
2025-06-20 07:11:21 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - No active profile set, falling back to 1 default profile: "default"
2025-06-20 07:11:24 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-20 07:11:24 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-20 07:11:24 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.30]
2025-06-20 07:11:24 [restartedMain] INFO  o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-20 07:11:24 [restartedMain] WARN  c.h.i.impl.HazelcastInstanceFactory - Hazelcast is starting in a Java modular environment (Java 9 and newer) but without proper access to required Java packages. Use additional Java arguments to provide Hazelcast access to Java internal API. The internal API access is used to get the best performance results. Arguments to be used:
 --add-modules java.se --add-exports java.base/jdk.internal.ref=ALL-UNNAMED --add-opens java.base/java.lang=ALL-UNNAMED --add-opens java.base/sun.nio.ch=ALL-UNNAMED --add-opens java.management/sun.management=ALL-UNNAMED --add-opens jdk.management/com.sun.management.internal=ALL-UNNAMED
2025-06-20 07:11:24 [restartedMain] INFO  c.hazelcast.instance.AddressPicker - [LOCAL] [pts-cluster] [5.4.0] Interfaces is disabled, trying to pick one address from TCP-IP config addresses: [127.0.0.1]
2025-06-20 07:11:24 [restartedMain] INFO  com.hazelcast.system.logo - [127.0.0.1]:5701 [pts-cluster] [5.4.0] 
	o    o     o     o---o   o--o o      o---o     o     o----o o--o--o
	|    |    / \       /         |     /         / \    |         |   
	o----o       o     o   o----o |    o             o   o----o    |   
	|    |  *     \   /           |     \       *     \       |    |   
	o    o *       o o---o   o--o o----o o---o *       o o----o    o   
2025-06-20 07:11:24 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Copyright (c) 2008-2024, Hazelcast, Inc. All Rights Reserved.
2025-06-20 07:11:24 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Hazelcast Platform 5.4.0 (20240415) starting at [127.0.0.1]:5701
2025-06-20 07:11:24 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Cluster name: pts-cluster
2025-06-20 07:11:24 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Integrity Checker is disabled. Fail-fast on corrupted executables will not be performed. For more information, see the documentation for Integrity Checker.
2025-06-20 07:11:24 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] The Jet engine is disabled.
To enable the Jet engine on the members, do one of the following:
  - Change member config using Java API: config.getJetConfig().setEnabled(true)
  - Change XML/YAML configuration property: Set hazelcast.jet.enabled to true
  - Add system property: -Dhz.jet.enabled=true (for Hazelcast embedded, works only when loading config via Config.load)
  - Add environment variable: HZ_JET_ENABLED=true (recommended when running container image. For Hazelcast embedded, works only when loading config via Config.load)
2025-06-20 07:11:24 [restartedMain] INFO  com.hazelcast.system.security - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Enable DEBUG/FINE log level for log category com.hazelcast.system.security  or use -Dhazelcast.security.recommendations system property to see security recommendations and the status of current config.
2025-06-20 07:11:25 [restartedMain] INFO  com.hazelcast.instance.impl.Node - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Using TCP/IP discovery
2025-06-20 07:11:25 [restartedMain] WARN  com.hazelcast.cp.CPSubsystem - [127.0.0.1]:5701 [pts-cluster] [5.4.0] CP Subsystem is not enabled. CP data structures will operate in UNSAFE mode! Please note that UNSAFE mode will not provide strong consistency guarantees.
2025-06-20 07:11:25 [restartedMain] INFO  c.h.internal.diagnostics.Diagnostics - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Diagnostics disabled. To enable add -Dhazelcast.diagnostics.enabled=true to the JVM arguments.
2025-06-20 07:11:25 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is STARTING
2025-06-20 07:11:25 [hz.personnel-tracking-system.cached.thread-3] INFO  c.h.i.cluster.impl.TcpIpJoiner - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5703 is added to the blacklist.
2025-06-20 07:11:25 [hz.personnel-tracking-system.cached.thread-2] INFO  c.h.i.cluster.impl.TcpIpJoiner - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5702 is added to the blacklist.
2025-06-20 07:11:26 [restartedMain] INFO  c.h.internal.cluster.ClusterService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] 

Members {size:1, ver:1} [
	Member [127.0.0.1]:5701 - c3caf3db-fb1f-40c6-9d45-7f6910fd992b this
]

2025-06-20 07:11:26 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is STARTED
2025-06-20 07:11:26 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-1 - Starting...
2025-06-20 07:11:26 [restartedMain] INFO  com.zaxxer.hikari.pool.HikariPool - HikariPool-1 - Added connection org.postgresql.jdbc.PgConnection@62992aac
2025-06-20 07:11:26 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-1 - Start completed.
2025-06-20 07:11:27 [restartedMain] INFO  liquibase.changelog - Creating database history table with name: dbpersonel.databasechangelog
2025-06-20 07:11:27 [restartedMain] INFO  liquibase.changelog - Reading from dbpersonel.databasechangelog
2025-06-20 07:11:27 [restartedMain] INFO  liquibase.lockservice - Successfully acquired change log lock
2025-06-20 07:11:27 [restartedMain] INFO  liquibase.command - Using deploymentId: 0392687667
2025-06-20 07:11:27 [restartedMain] INFO  liquibase.changelog - Reading from dbpersonel.databasechangelog
2025-06-20 07:11:27 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::1::liquibase
2025-06-20 07:11:27 [restartedMain] WARN  liquibase.executor - "dbpersonel" emas zaten mevcut, atlanyor
2025-06-20 07:11:27 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:11:27 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::1::liquibase ran successfully in 18ms
2025-06-20 07:11:27 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::2::liquibase
2025-06-20 07:11:27 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:11:27 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:11:27 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:11:27 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:11:27 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:11:27 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:11:27 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:11:27 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:11:27 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:11:27 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:11:27 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:11:27 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:11:27 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:11:27 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:11:27 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:11:27 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:11:27 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:11:27 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:11:27 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:11:27 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:11:27 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:11:27 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:11:27 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:11:27 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:11:27 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:11:27 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:11:27 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::2::liquibase ran successfully in 114ms
2025-06-20 07:11:27 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::3::liquibase
2025-06-20 07:11:27 [restartedMain] INFO  liquibase.changelog - Table personel_type created
2025-06-20 07:11:27 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::3::liquibase ran successfully in 10ms
2025-06-20 07:11:27 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::4::liquibase
2025-06-20 07:11:27 [restartedMain] INFO  liquibase.changelog - Table building created
2025-06-20 07:11:27 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::4::liquibase ran successfully in 6ms
2025-06-20 07:11:27 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::5::liquibase
2025-06-20 07:11:27 [restartedMain] INFO  liquibase.changelog - Table floor created
2025-06-20 07:11:27 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::5::liquibase ran successfully in 8ms
2025-06-20 07:11:27 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::6::liquibase
2025-06-20 07:11:27 [restartedMain] INFO  liquibase.changelog - Table personel created
2025-06-20 07:11:27 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::6::liquibase ran successfully in 7ms
2025-06-20 07:11:27 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::7::liquibase
2025-06-20 07:11:27 [restartedMain] INFO  liquibase.changelog - Table unit created
2025-06-20 07:11:27 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::7::liquibase ran successfully in 6ms
2025-06-20 07:11:27 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::8::liquibase
2025-06-20 07:11:27 [restartedMain] INFO  liquibase.changelog - Table personel_unit created
2025-06-20 07:11:27 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::8::liquibase ran successfully in 5ms
2025-06-20 07:11:27 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::9::liquibase
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - Table gate created
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::9::liquibase ran successfully in 6ms
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::10::liquibase
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - Table turnstile created
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::10::liquibase ran successfully in 5ms
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::11::liquibase
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - Table turnstile_registration_log created
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::11::liquibase ran successfully in 7ms
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::12::liquibase
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - Table working_hours created
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::12::liquibase ran successfully in 6ms
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::13::liquibase
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - Table salary created
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::13::liquibase ran successfully in 12ms
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::14::liquibase
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - Table user created
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::14::liquibase ran successfully in 9ms
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::15::liquibase
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - Table permission created
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::15::liquibase ran successfully in 15ms
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::16::liquibase
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - Table role_permission created
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::16::liquibase ran successfully in 7ms
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::17::liquibase
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::17::liquibase ran successfully in 69ms
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::1::liquibase
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - New row inserted into personel_type
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - New row inserted into personel_type
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - New row inserted into personel_type
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::1::liquibase ran successfully in 9ms
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::2::liquibase
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - New row inserted into building
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - New row inserted into building
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - New row inserted into building
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::2::liquibase ran successfully in 14ms
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::3::liquibase
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - Data loaded from 'db/changelog/data/floor-data.csv' into table 'floor'
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::3::liquibase ran successfully in 40ms
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::4::liquibase
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - New row inserted into personel
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - New row inserted into personel
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - New row inserted into personel
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::4::liquibase ran successfully in 10ms
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::5::liquibase
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - New row inserted into unit
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - New row inserted into unit
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - New row inserted into unit
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::5::liquibase ran successfully in 6ms
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::6::liquibase
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - New row inserted into personel_unit
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - New row inserted into personel_unit
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - New row inserted into personel_unit
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::6::liquibase ran successfully in 8ms
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::7::liquibase
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - Data loaded from 'db/changelog/data/gate-data.csv' into table 'gate'
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::7::liquibase ran successfully in 12ms
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::8::liquibase
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - Data loaded from 'db/changelog/data/turnstile-data.csv' into table 'turnstile'
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::8::liquibase ran successfully in 12ms
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::9::liquibase
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - New row inserted into working_hours
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - New row inserted into working_hours
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - New row inserted into working_hours
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::9::liquibase ran successfully in 10ms
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::10::liquibase
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - New row inserted into permission
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - New row inserted into permission
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - New row inserted into permission
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - New row inserted into permission
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - New row inserted into permission
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::10::liquibase ran successfully in 19ms
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::11::liquibase
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - New row inserted into role_permission
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - New row inserted into role_permission
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - New row inserted into role_permission
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - New row inserted into role_permission
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - New row inserted into role_permission
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::11::liquibase ran successfully in 12ms
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.util - UPDATE SUMMARY
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.util - Run:                         28
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.util - Previously run:               0
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.util - Filtered out:                 0
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.util - -------------------------------
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.util - Total change sets:           28
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.util - Update summary generated
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.command - Update command completed successfully.
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.ui - Liquibase: Update has been successful. Rows affected: 124
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.lockservice - Successfully released change log lock
2025-06-20 07:11:28 [restartedMain] INFO  liquibase.command - Command execution complete
2025-06-20 07:11:28 [restartedMain] WARN  org.hibernate.orm.deprecation - HHH90000025: PostgreSQLDialect does not need to be specified explicitly using 'hibernate.dialect' (remove the property setting and it will be selected by default)
2025-06-20 07:11:30 [restartedMain] WARN  o.s.s.c.a.a.c.InitializeUserDetailsBeanManagerConfigurer$InitializeUserDetailsManagerConfigurer - Global AuthenticationManager configured with an AuthenticationProvider bean. UserDetailsService beans will not be used for username/password login. Consider removing the AuthenticationProvider bean. Alternatively, consider using the UserDetailsService in a manually instantiated DaoAuthenticationProvider.
2025-06-20 07:11:31 [restartedMain] INFO  c.p.s.impl.PersonelTypeServiceImpl - Checking for default personnel types...
2025-06-20 07:11:31 [restartedMain] INFO  c.p.s.impl.PersonelTypeServiceImpl - Personnel types already exist in the database.
2025-06-20 07:11:31 [restartedMain] WARN  o.s.b.a.o.j.JpaBaseConfiguration$JpaWebConfiguration - spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
2025-06-20 07:11:32 [restartedMain] INFO  o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = personneltrackingsystem-admin-0
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-20 07:11:32 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 07:11:32 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 07:11:32 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750392692809
2025-06-20 07:11:33 [kafka-admin-client-thread | personneltrackingsystem-admin-0] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for personneltrackingsystem-admin-0 unregistered
2025-06-20 07:11:33 [kafka-admin-client-thread | personneltrackingsystem-admin-0] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-20 07:11:33 [kafka-admin-client-thread | personneltrackingsystem-admin-0] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-20 07:11:33 [kafka-admin-client-thread | personneltrackingsystem-admin-0] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-20 07:11:33 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-20 07:11:33 [restartedMain] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pts-group-reset-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pts-group-reset
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-06-20 07:11:33 [restartedMain] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 07:11:33 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 07:11:33 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 07:11:33 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750392693239
2025-06-20 07:11:33 [restartedMain] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Subscribed to topic(s): turnstile-request
2025-06-20 07:11:33 [restartedMain] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pts-group-reset-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pts-group-reset
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-06-20 07:11:33 [restartedMain] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 07:11:33 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 07:11:33 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 07:11:33 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2025-06-20 07:11:33 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 07:11:33 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750392693267
2025-06-20 07:11:33 [restartedMain] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Subscribed to topic(s): email-notification
2025-06-20 07:11:33 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:11:33 [restartedMain] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pts-group-reset-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pts-group-reset
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-06-20 07:11:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 07:11:33 [restartedMain] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 07:11:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2025-06-20 07:11:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:11:33 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 07:11:33 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Request joining group due to: need to re-join with the given member-id: consumer-pts-group-reset-1-8c3a796c-98a2-4d2e-be2a-3b078a688172
2025-06-20 07:11:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Request joining group due to: need to re-join with the given member-id: consumer-pts-group-reset-2-b603cc45-d4b7-4a1c-a064-83caf304abba
2025-06-20 07:11:33 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 07:11:33 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:11:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:11:33 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750392693293
2025-06-20 07:11:33 [restartedMain] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Subscribed to topic(s): turnstile-passage
2025-06-20 07:11:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=44, memberId='consumer-pts-group-reset-2-b603cc45-d4b7-4a1c-a064-83caf304abba', protocol='range'}
2025-06-20 07:11:33 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=44, memberId='consumer-pts-group-reset-1-8c3a796c-98a2-4d2e-be2a-3b078a688172', protocol='range'}
2025-06-20 07:11:33 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 07:11:33 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2025-06-20 07:11:33 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:11:33 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Finished assignment for group at generation 44: {consumer-pts-group-reset-2-b603cc45-d4b7-4a1c-a064-83caf304abba=Assignment(partitions=[email-notification-0]), consumer-pts-group-reset-1-8c3a796c-98a2-4d2e-be2a-3b078a688172=Assignment(partitions=[turnstile-request-0])}
2025-06-20 07:11:33 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Request joining group due to: need to re-join with the given member-id: consumer-pts-group-reset-3-6481dbb3-1716-49f0-9dec-4f62d17e2fa1
2025-06-20 07:11:33 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:11:33 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - Started PersonelTrackingSystemApplication in 12.635 seconds (process running for 13.109)
2025-06-20 07:11:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=44, memberId='consumer-pts-group-reset-2-b603cc45-d4b7-4a1c-a064-83caf304abba', protocol='range'}
2025-06-20 07:11:33 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=44, memberId='consumer-pts-group-reset-1-8c3a796c-98a2-4d2e-be2a-3b078a688172', protocol='range'}
2025-06-20 07:11:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[email-notification-0])
2025-06-20 07:11:33 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[turnstile-request-0])
2025-06-20 07:11:33 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Adding newly assigned partitions: turnstile-request-0
2025-06-20 07:11:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Adding newly assigned partitions: email-notification-0
2025-06-20 07:11:33 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition turnstile-request-0 to the committed offset FetchPosition{offset=12, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 07:11:33 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition email-notification-0 to the committed offset FetchPosition{offset=3, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 07:11:36 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Request joining group due to: group is already rebalancing
2025-06-20 07:11:36 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Request joining group due to: group is already rebalancing
2025-06-20 07:11:36 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Revoke previously assigned partitions email-notification-0
2025-06-20 07:11:36 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Revoke previously assigned partitions turnstile-request-0
2025-06-20 07:11:36 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:11:36 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:11:36 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=45, memberId='consumer-pts-group-reset-2-b603cc45-d4b7-4a1c-a064-83caf304abba', protocol='range'}
2025-06-20 07:11:36 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=45, memberId='consumer-pts-group-reset-3-6481dbb3-1716-49f0-9dec-4f62d17e2fa1', protocol='range'}
2025-06-20 07:11:36 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=45, memberId='consumer-pts-group-reset-1-8c3a796c-98a2-4d2e-be2a-3b078a688172', protocol='range'}
2025-06-20 07:11:36 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Finished assignment for group at generation 45: {consumer-pts-group-reset-2-b603cc45-d4b7-4a1c-a064-83caf304abba=Assignment(partitions=[email-notification-0]), consumer-pts-group-reset-1-8c3a796c-98a2-4d2e-be2a-3b078a688172=Assignment(partitions=[turnstile-request-0]), consumer-pts-group-reset-3-6481dbb3-1716-49f0-9dec-4f62d17e2fa1=Assignment(partitions=[turnstile-passage-0])}
2025-06-20 07:11:36 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=45, memberId='consumer-pts-group-reset-2-b603cc45-d4b7-4a1c-a064-83caf304abba', protocol='range'}
2025-06-20 07:11:36 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=45, memberId='consumer-pts-group-reset-1-8c3a796c-98a2-4d2e-be2a-3b078a688172', protocol='range'}
2025-06-20 07:11:36 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=45, memberId='consumer-pts-group-reset-3-6481dbb3-1716-49f0-9dec-4f62d17e2fa1', protocol='range'}
2025-06-20 07:11:36 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[email-notification-0])
2025-06-20 07:11:36 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[turnstile-request-0])
2025-06-20 07:11:36 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[turnstile-passage-0])
2025-06-20 07:11:36 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Adding newly assigned partitions: email-notification-0
2025-06-20 07:11:36 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Adding newly assigned partitions: turnstile-request-0
2025-06-20 07:11:36 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Adding newly assigned partitions: turnstile-passage-0
2025-06-20 07:11:36 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition turnstile-request-0 to the committed offset FetchPosition{offset=12, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 07:11:36 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition email-notification-0 to the committed offset FetchPosition{offset=3, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 07:11:36 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition turnstile-passage-0 to the committed offset FetchPosition{offset=3, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 07:12:03 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - Starting PersonelTrackingSystemApplication using Java 21.0.7 with PID 15096 (C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes started by Lenovo in C:\Users\Lenovo\Desktop\personnel-tracking-system)
2025-06-20 07:12:03 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - No active profile set, falling back to 1 default profile: "default"
2025-06-20 07:12:06 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-20 07:12:06 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-20 07:12:06 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.30]
2025-06-20 07:12:06 [restartedMain] INFO  o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-20 07:12:06 [restartedMain] WARN  c.h.i.impl.HazelcastInstanceFactory - Hazelcast is starting in a Java modular environment (Java 9 and newer) but without proper access to required Java packages. Use additional Java arguments to provide Hazelcast access to Java internal API. The internal API access is used to get the best performance results. Arguments to be used:
 --add-modules java.se --add-exports java.base/jdk.internal.ref=ALL-UNNAMED --add-opens java.base/java.lang=ALL-UNNAMED --add-opens java.base/sun.nio.ch=ALL-UNNAMED --add-opens java.management/sun.management=ALL-UNNAMED --add-opens jdk.management/com.sun.management.internal=ALL-UNNAMED
2025-06-20 07:12:06 [restartedMain] INFO  c.hazelcast.instance.AddressPicker - [LOCAL] [pts-cluster] [5.4.0] Interfaces is disabled, trying to pick one address from TCP-IP config addresses: [127.0.0.1]
2025-06-20 07:12:06 [restartedMain] INFO  com.hazelcast.system.logo - [127.0.0.1]:5701 [pts-cluster] [5.4.0] 
	o    o     o     o---o   o--o o      o---o     o     o----o o--o--o
	|    |    / \       /         |     /         / \    |         |   
	o----o       o     o   o----o |    o             o   o----o    |   
	|    |  *     \   /           |     \       *     \       |    |   
	o    o *       o o---o   o--o o----o o---o *       o o----o    o   
2025-06-20 07:12:06 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Copyright (c) 2008-2024, Hazelcast, Inc. All Rights Reserved.
2025-06-20 07:12:06 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Hazelcast Platform 5.4.0 (20240415) starting at [127.0.0.1]:5701
2025-06-20 07:12:06 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Cluster name: pts-cluster
2025-06-20 07:12:06 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Integrity Checker is disabled. Fail-fast on corrupted executables will not be performed. For more information, see the documentation for Integrity Checker.
2025-06-20 07:12:06 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] The Jet engine is disabled.
To enable the Jet engine on the members, do one of the following:
  - Change member config using Java API: config.getJetConfig().setEnabled(true)
  - Change XML/YAML configuration property: Set hazelcast.jet.enabled to true
  - Add system property: -Dhz.jet.enabled=true (for Hazelcast embedded, works only when loading config via Config.load)
  - Add environment variable: HZ_JET_ENABLED=true (recommended when running container image. For Hazelcast embedded, works only when loading config via Config.load)
2025-06-20 07:12:07 [restartedMain] INFO  com.hazelcast.system.security - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Enable DEBUG/FINE log level for log category com.hazelcast.system.security  or use -Dhazelcast.security.recommendations system property to see security recommendations and the status of current config.
2025-06-20 07:12:07 [restartedMain] INFO  com.hazelcast.instance.impl.Node - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Using TCP/IP discovery
2025-06-20 07:12:07 [restartedMain] WARN  com.hazelcast.cp.CPSubsystem - [127.0.0.1]:5701 [pts-cluster] [5.4.0] CP Subsystem is not enabled. CP data structures will operate in UNSAFE mode! Please note that UNSAFE mode will not provide strong consistency guarantees.
2025-06-20 07:12:07 [restartedMain] INFO  c.h.internal.diagnostics.Diagnostics - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Diagnostics disabled. To enable add -Dhazelcast.diagnostics.enabled=true to the JVM arguments.
2025-06-20 07:12:07 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is STARTING
2025-06-20 07:12:07 [hz.personnel-tracking-system.cached.thread-2] INFO  c.h.i.cluster.impl.TcpIpJoiner - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5703 is added to the blacklist.
2025-06-20 07:12:07 [hz.personnel-tracking-system.cached.thread-3] INFO  c.h.i.cluster.impl.TcpIpJoiner - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5702 is added to the blacklist.
2025-06-20 07:12:08 [restartedMain] INFO  c.h.internal.cluster.ClusterService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] 

Members {size:1, ver:1} [
	Member [127.0.0.1]:5701 - 4d176d78-a7ee-4c2a-9fa4-b99416d36c4b this
]

2025-06-20 07:12:08 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is STARTED
2025-06-20 07:12:08 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-1 - Starting...
2025-06-20 07:12:08 [restartedMain] INFO  com.zaxxer.hikari.pool.HikariPool - HikariPool-1 - Added connection org.postgresql.jdbc.PgConnection@347c68ec
2025-06-20 07:12:08 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-1 - Start completed.
2025-06-20 07:12:09 [restartedMain] INFO  liquibase.changelog - Reading from dbpersonel.databasechangelog
2025-06-20 07:12:09 [restartedMain] INFO  liquibase.ui - Database is up to date, no changesets to execute
2025-06-20 07:12:09 [restartedMain] INFO  liquibase.changelog - Reading from dbpersonel.databasechangelog
2025-06-20 07:12:09 [restartedMain] INFO  liquibase.util - UPDATE SUMMARY
2025-06-20 07:12:09 [restartedMain] INFO  liquibase.util - Run:                          0
2025-06-20 07:12:09 [restartedMain] INFO  liquibase.util - Previously run:              28
2025-06-20 07:12:09 [restartedMain] INFO  liquibase.util - Filtered out:                 0
2025-06-20 07:12:09 [restartedMain] INFO  liquibase.util - -------------------------------
2025-06-20 07:12:09 [restartedMain] INFO  liquibase.util - Total change sets:           28
2025-06-20 07:12:09 [restartedMain] INFO  liquibase.util - Update summary generated
2025-06-20 07:12:09 [restartedMain] INFO  liquibase.command - Command execution complete
2025-06-20 07:12:10 [restartedMain] WARN  org.hibernate.orm.deprecation - HHH90000025: PostgreSQLDialect does not need to be specified explicitly using 'hibernate.dialect' (remove the property setting and it will be selected by default)
2025-06-20 07:12:12 [restartedMain] WARN  o.s.s.c.a.a.c.InitializeUserDetailsBeanManagerConfigurer$InitializeUserDetailsManagerConfigurer - Global AuthenticationManager configured with an AuthenticationProvider bean. UserDetailsService beans will not be used for username/password login. Consider removing the AuthenticationProvider bean. Alternatively, consider using the UserDetailsService in a manually instantiated DaoAuthenticationProvider.
2025-06-20 07:12:13 [restartedMain] INFO  c.p.s.impl.PersonelTypeServiceImpl - Checking for default personnel types...
2025-06-20 07:12:13 [restartedMain] INFO  c.p.s.impl.PersonelTypeServiceImpl - Personnel types already exist in the database.
2025-06-20 07:12:14 [restartedMain] WARN  o.s.b.a.o.j.JpaBaseConfiguration$JpaWebConfiguration - spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
2025-06-20 07:12:15 [restartedMain] INFO  o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = personneltrackingsystem-admin-0
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-20 07:12:15 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 07:12:15 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 07:12:15 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750392735952
2025-06-20 07:12:16 [kafka-admin-client-thread | personneltrackingsystem-admin-0] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for personneltrackingsystem-admin-0 unregistered
2025-06-20 07:12:16 [kafka-admin-client-thread | personneltrackingsystem-admin-0] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-20 07:12:16 [kafka-admin-client-thread | personneltrackingsystem-admin-0] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-20 07:12:16 [kafka-admin-client-thread | personneltrackingsystem-admin-0] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-20 07:12:16 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-20 07:12:16 [restartedMain] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pts-group-reset-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pts-group-reset
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-06-20 07:12:16 [restartedMain] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 07:12:16 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 07:12:16 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 07:12:16 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750392736436
2025-06-20 07:12:16 [restartedMain] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Subscribed to topic(s): turnstile-request
2025-06-20 07:12:16 [restartedMain] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pts-group-reset-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pts-group-reset
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-06-20 07:12:16 [restartedMain] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 07:12:16 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 07:12:16 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2025-06-20 07:12:16 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 07:12:16 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 07:12:16 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750392736468
2025-06-20 07:12:16 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:12:16 [restartedMain] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Subscribed to topic(s): email-notification
2025-06-20 07:12:16 [restartedMain] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pts-group-reset-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pts-group-reset
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-06-20 07:12:16 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 07:12:16 [restartedMain] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 07:12:16 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2025-06-20 07:12:16 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:12:16 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Request joining group due to: need to re-join with the given member-id: consumer-pts-group-reset-2-b81faa68-912f-49be-a1f0-32414832e1b3
2025-06-20 07:12:16 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Request joining group due to: need to re-join with the given member-id: consumer-pts-group-reset-1-4f530311-2179-4191-8981-5fa000403cae
2025-06-20 07:12:16 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:12:16 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:12:16 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 07:12:16 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 07:12:16 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750392736494
2025-06-20 07:12:16 [restartedMain] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Subscribed to topic(s): turnstile-passage
2025-06-20 07:12:16 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 07:12:16 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2025-06-20 07:12:16 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:12:16 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Request joining group due to: need to re-join with the given member-id: consumer-pts-group-reset-3-c0434597-6ec3-43f2-833b-fabe3eb79465
2025-06-20 07:12:16 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:12:16 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - Started PersonelTrackingSystemApplication in 13.844 seconds (process running for 14.364)
2025-06-20 07:12:39 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=46, memberId='consumer-pts-group-reset-2-b81faa68-912f-49be-a1f0-32414832e1b3', protocol='range'}
2025-06-20 07:12:39 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=46, memberId='consumer-pts-group-reset-1-4f530311-2179-4191-8981-5fa000403cae', protocol='range'}
2025-06-20 07:12:39 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=46, memberId='consumer-pts-group-reset-3-c0434597-6ec3-43f2-833b-fabe3eb79465', protocol='range'}
2025-06-20 07:12:39 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Finished assignment for group at generation 46: {consumer-pts-group-reset-2-b81faa68-912f-49be-a1f0-32414832e1b3=Assignment(partitions=[email-notification-0]), consumer-pts-group-reset-3-c0434597-6ec3-43f2-833b-fabe3eb79465=Assignment(partitions=[turnstile-passage-0]), consumer-pts-group-reset-1-4f530311-2179-4191-8981-5fa000403cae=Assignment(partitions=[turnstile-request-0])}
2025-06-20 07:12:39 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=46, memberId='consumer-pts-group-reset-2-b81faa68-912f-49be-a1f0-32414832e1b3', protocol='range'}
2025-06-20 07:12:39 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=46, memberId='consumer-pts-group-reset-1-4f530311-2179-4191-8981-5fa000403cae', protocol='range'}
2025-06-20 07:12:39 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=46, memberId='consumer-pts-group-reset-3-c0434597-6ec3-43f2-833b-fabe3eb79465', protocol='range'}
2025-06-20 07:12:39 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[email-notification-0])
2025-06-20 07:12:39 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[turnstile-request-0])
2025-06-20 07:12:39 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[turnstile-passage-0])
2025-06-20 07:12:39 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Adding newly assigned partitions: email-notification-0
2025-06-20 07:12:39 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Adding newly assigned partitions: turnstile-passage-0
2025-06-20 07:12:39 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Adding newly assigned partitions: turnstile-request-0
2025-06-20 07:12:39 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition turnstile-request-0 to the committed offset FetchPosition{offset=12, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 07:12:39 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition email-notification-0 to the committed offset FetchPosition{offset=3, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 07:12:39 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition turnstile-passage-0 to the committed offset FetchPosition{offset=3, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 07:15:13 [http-nio-8080-exec-1] INFO  o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-06-20 07:15:14 [http-nio-8080-exec-1] INFO  c.p.c.impl.TurnstileControllerImpl - Turnstile passage request received: DtoTurnstilePassageFullRequest(wantedToEnterTurnstileId=7, personelId=1, operationType=IN, operationTimeStr=09:19:35)
2025-06-20 07:15:14 [http-nio-8080-exec-1] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - No previous operation found for personel 1 on turnstile 7
2025-06-20 07:15:14 [http-nio-8080-exec-1] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Turnstile passage validation - Personel: 1, Turnstile: 7, Requested Operation: IN, Last Operation: null
2025-06-20 07:15:14 [http-nio-8080-exec-1] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Operation validated successfully: IN (Personel: 1, Turnstile: 7)
2025-06-20 07:15:14 [http-nio-8080-exec-1] INFO  c.p.s.k.i.KafkaProducerServiceImpl - Sending turnstile request event to Kafka: TurnstileRequestEvent(wantedToEnterTurnstileId=7, personelId=1, operationType=IN, operationTimeStr=09:19:35)
2025-06-20 07:15:14 [http-nio-8080-exec-1] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = personneltrackingsystem-producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 3
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.springframework.kafka.support.serializer.JsonSerializer

2025-06-20 07:15:14 [http-nio-8080-exec-1] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 07:15:14 [http-nio-8080-exec-1] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=personneltrackingsystem-producer-1] Instantiated an idempotent producer.
2025-06-20 07:15:14 [http-nio-8080-exec-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 07:15:14 [http-nio-8080-exec-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 07:15:14 [http-nio-8080-exec-1] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750392914285
2025-06-20 07:15:14 [kafka-producer-network-thread | personneltrackingsystem-producer-1] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=personneltrackingsystem-producer-1] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 07:15:14 [kafka-producer-network-thread | personneltrackingsystem-producer-1] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=personneltrackingsystem-producer-1] ProducerId set to 8 with epoch 0
2025-06-20 07:15:14 [kafka-producer-network-thread | personneltrackingsystem-producer-1] INFO  c.p.s.k.i.KafkaProducerServiceImpl - Turnstile request event sent successfully to topic: turnstile-request, partition: 0, offset: 12
2025-06-20 07:15:14 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.k.i.TurnstileRequestConsumerServiceImpl - Received turnstile request event: TurnstileRequestEvent(wantedToEnterTurnstileId=7, personelId=1, operationType=IN, operationTimeStr=09:19:35)
2025-06-20 07:15:15 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.c.i.PersonelCacheServiceImpl - Personnel found in cache with ID: 1
2025-06-20 07:15:15 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Saved turnstile registration log: Personel 1 - Turnstile 7 - Operation IN
2025-06-20 07:15:15 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.k.i.TurnstileRequestConsumerServiceImpl - Turnstile request event processed successfully
2025-06-20 07:15:22 [http-nio-8080-exec-3] INFO  c.p.c.impl.TurnstileControllerImpl - Turnstile passage request received: DtoTurnstilePassageFullRequest(wantedToEnterTurnstileId=11, personelId=1, operationType=IN, operationTimeStr=09:19:35)
2025-06-20 07:15:22 [http-nio-8080-exec-3] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - No previous operation found for personel 1 on turnstile 11
2025-06-20 07:15:22 [http-nio-8080-exec-3] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Turnstile passage validation - Personel: 1, Turnstile: 11, Requested Operation: IN, Last Operation: null
2025-06-20 07:15:22 [http-nio-8080-exec-3] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Operation validated successfully: IN (Personel: 1, Turnstile: 11)
2025-06-20 07:15:22 [http-nio-8080-exec-3] INFO  c.p.s.k.i.KafkaProducerServiceImpl - Sending turnstile request event to Kafka: TurnstileRequestEvent(wantedToEnterTurnstileId=11, personelId=1, operationType=IN, operationTimeStr=09:19:35)
2025-06-20 07:15:22 [kafka-producer-network-thread | personneltrackingsystem-producer-1] INFO  c.p.s.k.i.KafkaProducerServiceImpl - Turnstile request event sent successfully to topic: turnstile-request, partition: 0, offset: 13
2025-06-20 07:15:22 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.k.i.TurnstileRequestConsumerServiceImpl - Received turnstile request event: TurnstileRequestEvent(wantedToEnterTurnstileId=11, personelId=1, operationType=IN, operationTimeStr=09:19:35)
2025-06-20 07:15:22 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] ERROR c.p.s.k.i.TurnstileRequestConsumerServiceImpl - Error processing turnstile request event: turnstile.not.found : 11
com.personneltrackingsystem.exception.BaseException: turnstile.not.found : 11
	at com.personneltrackingsystem.service.impl.TurnstileServiceImpl.lambda$2(TurnstileServiceImpl.java:164)
	at java.base/java.util.Optional.orElseThrow(Optional.java:403)
	at com.personneltrackingsystem.service.impl.TurnstileServiceImpl.passTurnstile(TurnstileServiceImpl.java:164)
	at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)
	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:355)
	at org.springframework.aop.framework.CglibAopProxy$DynamicAdvisedInterceptor.intercept(CglibAopProxy.java:716)
	at com.personneltrackingsystem.service.impl.TurnstileServiceImpl$$SpringCGLIB$$0.passTurnstile(<generated>)
	at com.personneltrackingsystem.service.kafka.impl.TurnstileRequestConsumerServiceImpl.consumeTurnstileRequestEvent(TurnstileRequestConsumerServiceImpl.java:34)
	at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)
	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:169)
	at org.springframework.kafka.listener.adapter.KotlinAwareInvocableHandlerMethod.doInvoke(KotlinAwareInvocableHandlerMethod.java:45)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:119)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:70)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:420)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701)
	at io.micrometer.observation.Observation.observe(Observation.java:565)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 07:15:26 [http-nio-8080-exec-5] INFO  c.p.c.impl.TurnstileControllerImpl - Turnstile passage request received: DtoTurnstilePassageFullRequest(wantedToEnterTurnstileId=11, personelId=1, operationType=IN, operationTimeStr=09:19:35)
2025-06-20 07:15:26 [http-nio-8080-exec-5] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - No previous operation found for personel 1 on turnstile 11
2025-06-20 07:15:26 [http-nio-8080-exec-5] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Turnstile passage validation - Personel: 1, Turnstile: 11, Requested Operation: IN, Last Operation: null
2025-06-20 07:15:26 [http-nio-8080-exec-5] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Operation validated successfully: IN (Personel: 1, Turnstile: 11)
2025-06-20 07:15:26 [http-nio-8080-exec-5] INFO  c.p.s.k.i.KafkaProducerServiceImpl - Sending turnstile request event to Kafka: TurnstileRequestEvent(wantedToEnterTurnstileId=11, personelId=1, operationType=IN, operationTimeStr=09:19:35)
2025-06-20 07:15:26 [kafka-producer-network-thread | personneltrackingsystem-producer-1] INFO  c.p.s.k.i.KafkaProducerServiceImpl - Turnstile request event sent successfully to topic: turnstile-request, partition: 0, offset: 14
2025-06-20 07:15:26 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.k.i.TurnstileRequestConsumerServiceImpl - Received turnstile request event: TurnstileRequestEvent(wantedToEnterTurnstileId=11, personelId=1, operationType=IN, operationTimeStr=09:19:35)
2025-06-20 07:15:26 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] ERROR c.p.s.k.i.TurnstileRequestConsumerServiceImpl - Error processing turnstile request event: turnstile.not.found : 11
com.personneltrackingsystem.exception.BaseException: turnstile.not.found : 11
	at com.personneltrackingsystem.service.impl.TurnstileServiceImpl.lambda$2(TurnstileServiceImpl.java:164)
	at java.base/java.util.Optional.orElseThrow(Optional.java:403)
	at com.personneltrackingsystem.service.impl.TurnstileServiceImpl.passTurnstile(TurnstileServiceImpl.java:164)
	at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)
	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:355)
	at org.springframework.aop.framework.CglibAopProxy$DynamicAdvisedInterceptor.intercept(CglibAopProxy.java:716)
	at com.personneltrackingsystem.service.impl.TurnstileServiceImpl$$SpringCGLIB$$0.passTurnstile(<generated>)
	at com.personneltrackingsystem.service.kafka.impl.TurnstileRequestConsumerServiceImpl.consumeTurnstileRequestEvent(TurnstileRequestConsumerServiceImpl.java:34)
	at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)
	at java.base/java.lang.reflect.Method.invoke(Method.java:580)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:169)
	at org.springframework.kafka.listener.adapter.KotlinAwareInvocableHandlerMethod.doInvoke(KotlinAwareInvocableHandlerMethod.java:45)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:119)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:70)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:420)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701)
	at io.micrometer.observation.Observation.observe(Observation.java:565)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
	at java.base/java.lang.Thread.run(Thread.java:1583)
2025-06-20 07:16:07 [http-nio-8080-exec-7] INFO  c.p.c.impl.TurnstileControllerImpl - Turnstile passage request received: DtoTurnstilePassageFullRequest(wantedToEnterTurnstileId=5, personelId=1, operationType=IN, operationTimeStr=09:19:35)
2025-06-20 07:16:07 [http-nio-8080-exec-7] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - No previous operation found for personel 1 on turnstile 5
2025-06-20 07:16:07 [http-nio-8080-exec-7] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Turnstile passage validation - Personel: 1, Turnstile: 5, Requested Operation: IN, Last Operation: null
2025-06-20 07:16:07 [http-nio-8080-exec-7] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Operation validated successfully: IN (Personel: 1, Turnstile: 5)
2025-06-20 07:16:07 [http-nio-8080-exec-7] INFO  c.p.s.k.i.KafkaProducerServiceImpl - Sending turnstile request event to Kafka: TurnstileRequestEvent(wantedToEnterTurnstileId=5, personelId=1, operationType=IN, operationTimeStr=09:19:35)
2025-06-20 07:16:07 [kafka-producer-network-thread | personneltrackingsystem-producer-1] INFO  c.p.s.k.i.KafkaProducerServiceImpl - Turnstile request event sent successfully to topic: turnstile-request, partition: 0, offset: 15
2025-06-20 07:16:07 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.k.i.TurnstileRequestConsumerServiceImpl - Received turnstile request event: TurnstileRequestEvent(wantedToEnterTurnstileId=5, personelId=1, operationType=IN, operationTimeStr=09:19:35)
2025-06-20 07:16:07 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.c.i.PersonelCacheServiceImpl - Personnel found in cache with ID: 1
2025-06-20 07:16:07 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Saved turnstile registration log: Personel 1 - Turnstile 5 - Operation IN
2025-06-20 07:16:07 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.k.i.TurnstileRequestConsumerServiceImpl - Turnstile request event processed successfully
2025-06-20 07:16:14 [http-nio-8080-exec-8] INFO  c.p.c.impl.TurnstileControllerImpl - Turnstile passage request received: DtoTurnstilePassageFullRequest(wantedToEnterTurnstileId=5, personelId=1, operationType=IN, operationTimeStr=09:19:35)
2025-06-20 07:16:14 [http-nio-8080-exec-8] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Last operation found: IN (Personel: 1, Turnstile: 5)
2025-06-20 07:16:14 [http-nio-8080-exec-8] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Turnstile passage validation - Personel: 1, Turnstile: 5, Requested Operation: IN, Last Operation: IN
2025-06-20 07:16:14 [http-nio-8080-exec-8] WARN  c.p.s.i.TurnstileRegistrationLogServiceImpl - Only OUT operation is allowed when the last operation is IN (Personel: 1, Turnstile: 5)
2025-06-20 07:16:14 [http-nio-8080-exec-8] WARN  o.s.w.s.m.m.a.ExceptionHandlerExceptionResolver - Resolved [com.personneltrackingsystem.exception.ValidationException: turnstile.entry.requires.prior.exit]
2025-06-20 07:16:52 [http-nio-8080-exec-9] INFO  c.p.c.impl.TurnstileControllerImpl - Turnstile passage request received: DtoTurnstilePassageFullRequest(wantedToEnterTurnstileId=5, personelId=1, operationType=OUT, operationTimeStr=18:21:35)
2025-06-20 07:16:52 [http-nio-8080-exec-9] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Last operation found: IN (Personel: 1, Turnstile: 5)
2025-06-20 07:16:52 [http-nio-8080-exec-9] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Turnstile passage validation - Personel: 1, Turnstile: 5, Requested Operation: OUT, Last Operation: IN
2025-06-20 07:16:52 [http-nio-8080-exec-9] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Operation validated successfully: OUT (Personel: 1, Turnstile: 5)
2025-06-20 07:16:52 [http-nio-8080-exec-9] INFO  c.p.s.k.i.KafkaProducerServiceImpl - Sending turnstile request event to Kafka: TurnstileRequestEvent(wantedToEnterTurnstileId=5, personelId=1, operationType=OUT, operationTimeStr=18:21:35)
2025-06-20 07:16:52 [kafka-producer-network-thread | personneltrackingsystem-producer-1] INFO  c.p.s.k.i.KafkaProducerServiceImpl - Turnstile request event sent successfully to topic: turnstile-request, partition: 0, offset: 16
2025-06-20 07:16:52 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.k.i.TurnstileRequestConsumerServiceImpl - Received turnstile request event: TurnstileRequestEvent(wantedToEnterTurnstileId=5, personelId=1, operationType=OUT, operationTimeStr=18:21:35)
2025-06-20 07:16:52 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.c.i.PersonelCacheServiceImpl - Personnel found in cache with ID: 1
2025-06-20 07:16:52 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Saved turnstile registration log: Personel 1 - Turnstile 5 - Operation OUT
2025-06-20 07:16:52 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.k.i.TurnstileRequestConsumerServiceImpl - Turnstile request event processed successfully
2025-06-20 07:17:14 [http-nio-8080-exec-2] INFO  c.p.c.impl.TurnstileControllerImpl - Turnstile passage request received: DtoTurnstilePassageFullRequest(wantedToEnterTurnstileId=7, personelId=1, operationType=OUT, operationTimeStr=18:11:35)
2025-06-20 07:17:14 [http-nio-8080-exec-2] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Last operation found: IN (Personel: 1, Turnstile: 7)
2025-06-20 07:17:14 [http-nio-8080-exec-2] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Turnstile passage validation - Personel: 1, Turnstile: 7, Requested Operation: OUT, Last Operation: IN
2025-06-20 07:17:14 [http-nio-8080-exec-2] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Operation validated successfully: OUT (Personel: 1, Turnstile: 7)
2025-06-20 07:17:14 [http-nio-8080-exec-2] INFO  c.p.s.k.i.KafkaProducerServiceImpl - Sending turnstile request event to Kafka: TurnstileRequestEvent(wantedToEnterTurnstileId=7, personelId=1, operationType=OUT, operationTimeStr=18:11:35)
2025-06-20 07:17:14 [kafka-producer-network-thread | personneltrackingsystem-producer-1] INFO  c.p.s.k.i.KafkaProducerServiceImpl - Turnstile request event sent successfully to topic: turnstile-request, partition: 0, offset: 17
2025-06-20 07:17:14 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.k.i.TurnstileRequestConsumerServiceImpl - Received turnstile request event: TurnstileRequestEvent(wantedToEnterTurnstileId=7, personelId=1, operationType=OUT, operationTimeStr=18:11:35)
2025-06-20 07:17:14 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.c.i.PersonelCacheServiceImpl - Personnel found in cache with ID: 1
2025-06-20 07:17:14 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Saved turnstile registration log: Personel 1 - Turnstile 7 - Operation OUT
2025-06-20 07:17:14 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.k.i.TurnstileRequestConsumerServiceImpl - Turnstile request event processed successfully
2025-06-20 07:29:23 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - Starting PersonelTrackingSystemApplication using Java 21.0.7 with PID 15664 (C:\Users\Lenovo\Desktop\personnel-tracking-system\target\classes started by Lenovo in C:\Users\Lenovo\Desktop\personnel-tracking-system)
2025-06-20 07:29:23 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - No active profile set, falling back to 1 default profile: "default"
2025-06-20 07:29:26 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8080"]
2025-06-20 07:29:26 [restartedMain] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
2025-06-20 07:29:26 [restartedMain] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.30]
2025-06-20 07:29:26 [restartedMain] INFO  o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-06-20 07:29:26 [restartedMain] WARN  c.h.i.impl.HazelcastInstanceFactory - Hazelcast is starting in a Java modular environment (Java 9 and newer) but without proper access to required Java packages. Use additional Java arguments to provide Hazelcast access to Java internal API. The internal API access is used to get the best performance results. Arguments to be used:
 --add-modules java.se --add-exports java.base/jdk.internal.ref=ALL-UNNAMED --add-opens java.base/java.lang=ALL-UNNAMED --add-opens java.base/sun.nio.ch=ALL-UNNAMED --add-opens java.management/sun.management=ALL-UNNAMED --add-opens jdk.management/com.sun.management.internal=ALL-UNNAMED
2025-06-20 07:29:26 [restartedMain] INFO  c.hazelcast.instance.AddressPicker - [LOCAL] [pts-cluster] [5.4.0] Interfaces is disabled, trying to pick one address from TCP-IP config addresses: [127.0.0.1]
2025-06-20 07:29:26 [restartedMain] INFO  com.hazelcast.system.logo - [127.0.0.1]:5701 [pts-cluster] [5.4.0] 
	o    o     o     o---o   o--o o      o---o     o     o----o o--o--o
	|    |    / \       /         |     /         / \    |         |   
	o----o       o     o   o----o |    o             o   o----o    |   
	|    |  *     \   /           |     \       *     \       |    |   
	o    o *       o o---o   o--o o----o o---o *       o o----o    o   
2025-06-20 07:29:26 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Copyright (c) 2008-2024, Hazelcast, Inc. All Rights Reserved.
2025-06-20 07:29:26 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Hazelcast Platform 5.4.0 (20240415) starting at [127.0.0.1]:5701
2025-06-20 07:29:26 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Cluster name: pts-cluster
2025-06-20 07:29:26 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Integrity Checker is disabled. Fail-fast on corrupted executables will not be performed. For more information, see the documentation for Integrity Checker.
2025-06-20 07:29:26 [restartedMain] INFO  com.hazelcast.system - [127.0.0.1]:5701 [pts-cluster] [5.4.0] The Jet engine is disabled.
To enable the Jet engine on the members, do one of the following:
  - Change member config using Java API: config.getJetConfig().setEnabled(true)
  - Change XML/YAML configuration property: Set hazelcast.jet.enabled to true
  - Add system property: -Dhz.jet.enabled=true (for Hazelcast embedded, works only when loading config via Config.load)
  - Add environment variable: HZ_JET_ENABLED=true (recommended when running container image. For Hazelcast embedded, works only when loading config via Config.load)
2025-06-20 07:29:27 [restartedMain] INFO  com.hazelcast.system.security - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Enable DEBUG/FINE log level for log category com.hazelcast.system.security  or use -Dhazelcast.security.recommendations system property to see security recommendations and the status of current config.
2025-06-20 07:29:27 [restartedMain] INFO  com.hazelcast.instance.impl.Node - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Using TCP/IP discovery
2025-06-20 07:29:27 [restartedMain] WARN  com.hazelcast.cp.CPSubsystem - [127.0.0.1]:5701 [pts-cluster] [5.4.0] CP Subsystem is not enabled. CP data structures will operate in UNSAFE mode! Please note that UNSAFE mode will not provide strong consistency guarantees.
2025-06-20 07:29:27 [restartedMain] INFO  c.h.internal.diagnostics.Diagnostics - [127.0.0.1]:5701 [pts-cluster] [5.4.0] Diagnostics disabled. To enable add -Dhazelcast.diagnostics.enabled=true to the JVM arguments.
2025-06-20 07:29:27 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is STARTING
2025-06-20 07:29:27 [hz.personnel-tracking-system.cached.thread-3] INFO  c.h.i.cluster.impl.TcpIpJoiner - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5702 is added to the blacklist.
2025-06-20 07:29:27 [hz.personnel-tracking-system.cached.thread-2] INFO  c.h.i.cluster.impl.TcpIpJoiner - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5703 is added to the blacklist.
2025-06-20 07:29:28 [restartedMain] INFO  c.h.internal.cluster.ClusterService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] 

Members {size:1, ver:1} [
	Member [127.0.0.1]:5701 - 47fb149c-ede7-49f3-b43a-01ecbd98dcb7 this
]

2025-06-20 07:29:28 [restartedMain] INFO  com.hazelcast.core.LifecycleService - [127.0.0.1]:5701 [pts-cluster] [5.4.0] [127.0.0.1]:5701 is STARTED
2025-06-20 07:29:28 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-1 - Starting...
2025-06-20 07:29:29 [restartedMain] INFO  com.zaxxer.hikari.pool.HikariPool - HikariPool-1 - Added connection org.postgresql.jdbc.PgConnection@73f7f289
2025-06-20 07:29:29 [restartedMain] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-1 - Start completed.
2025-06-20 07:29:29 [restartedMain] INFO  liquibase.changelog - Creating database history table with name: dbpersonel.databasechangelog
2025-06-20 07:29:29 [restartedMain] INFO  liquibase.changelog - Reading from dbpersonel.databasechangelog
2025-06-20 07:29:29 [restartedMain] INFO  liquibase.lockservice - Successfully acquired change log lock
2025-06-20 07:29:29 [restartedMain] INFO  liquibase.command - Using deploymentId: 0393769998
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Reading from dbpersonel.databasechangelog
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::1::liquibase
2025-06-20 07:29:30 [restartedMain] WARN  liquibase.executor - "dbpersonel" emas zaten mevcut, atlanyor
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::1::liquibase ran successfully in 14ms
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::2::liquibase
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::2::liquibase ran successfully in 107ms
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::3::liquibase
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Table personel_type created
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::3::liquibase ran successfully in 12ms
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::4::liquibase
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Table building created
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::4::liquibase ran successfully in 8ms
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::5::liquibase
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Table floor created
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::5::liquibase ran successfully in 16ms
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::6::liquibase
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Table personel created
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::6::liquibase ran successfully in 12ms
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::7::liquibase
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Table unit created
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::7::liquibase ran successfully in 14ms
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::8::liquibase
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Table personel_unit created
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::8::liquibase ran successfully in 13ms
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::9::liquibase
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Table gate created
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::9::liquibase ran successfully in 13ms
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::10::liquibase
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Table turnstile created
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::10::liquibase ran successfully in 13ms
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::11::liquibase
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Table turnstile_registration_log created
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::11::liquibase ran successfully in 14ms
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::12::liquibase
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Table working_hours created
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::12::liquibase ran successfully in 11ms
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::13::liquibase
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Table salary created
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::13::liquibase ran successfully in 16ms
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::14::liquibase
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Table user created
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::14::liquibase ran successfully in 19ms
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::15::liquibase
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Table permission created
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::15::liquibase ran successfully in 14ms
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::16::liquibase
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Table role_permission created
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::16::liquibase ran successfully in 9ms
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/001-create-schema-and-tables.yaml::17::liquibase
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Custom SQL executed
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/001-create-schema-and-tables.yaml::17::liquibase ran successfully in 55ms
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::1::liquibase
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - New row inserted into personel_type
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - New row inserted into personel_type
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - New row inserted into personel_type
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::1::liquibase ran successfully in 5ms
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::2::liquibase
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - New row inserted into building
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - New row inserted into building
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - New row inserted into building
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::2::liquibase ran successfully in 6ms
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::3::liquibase
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Data loaded from 'db/changelog/data/floor-data.csv' into table 'floor'
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::3::liquibase ran successfully in 51ms
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::4::liquibase
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - New row inserted into personel
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - New row inserted into personel
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - New row inserted into personel
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::4::liquibase ran successfully in 10ms
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::5::liquibase
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - New row inserted into unit
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - New row inserted into unit
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - New row inserted into unit
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::5::liquibase ran successfully in 10ms
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::6::liquibase
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - New row inserted into personel_unit
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - New row inserted into personel_unit
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - New row inserted into personel_unit
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::6::liquibase ran successfully in 10ms
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::7::liquibase
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Data loaded from 'db/changelog/data/gate-data.csv' into table 'gate'
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::7::liquibase ran successfully in 22ms
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::8::liquibase
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - Data loaded from 'db/changelog/data/turnstile-data.csv' into table 'turnstile'
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::8::liquibase ran successfully in 19ms
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::9::liquibase
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - New row inserted into working_hours
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - New row inserted into working_hours
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - New row inserted into working_hours
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::9::liquibase ran successfully in 15ms
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::10::liquibase
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - New row inserted into permission
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - New row inserted into permission
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - New row inserted into permission
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - New row inserted into permission
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - New row inserted into permission
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::10::liquibase ran successfully in 11ms
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.ui - Running Changeset: db/changelog/changes/002-insert-initial-data.yaml::11::liquibase
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - New row inserted into role_permission
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - New row inserted into role_permission
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - New row inserted into role_permission
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - New row inserted into role_permission
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - New row inserted into role_permission
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.changelog - ChangeSet db/changelog/changes/002-insert-initial-data.yaml::11::liquibase ran successfully in 7ms
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.util - UPDATE SUMMARY
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.util - Run:                         28
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.util - Previously run:               0
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.util - Filtered out:                 0
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.util - -------------------------------
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.util - Total change sets:           28
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.util - Update summary generated
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.command - Update command completed successfully.
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.ui - Liquibase: Update has been successful. Rows affected: 124
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.lockservice - Successfully released change log lock
2025-06-20 07:29:30 [restartedMain] INFO  liquibase.command - Command execution complete
2025-06-20 07:29:31 [restartedMain] WARN  org.hibernate.orm.deprecation - HHH90000025: PostgreSQLDialect does not need to be specified explicitly using 'hibernate.dialect' (remove the property setting and it will be selected by default)
2025-06-20 07:29:33 [restartedMain] WARN  o.s.s.c.a.a.c.InitializeUserDetailsBeanManagerConfigurer$InitializeUserDetailsManagerConfigurer - Global AuthenticationManager configured with an AuthenticationProvider bean. UserDetailsService beans will not be used for username/password login. Consider removing the AuthenticationProvider bean. Alternatively, consider using the UserDetailsService in a manually instantiated DaoAuthenticationProvider.
2025-06-20 07:29:34 [restartedMain] INFO  c.p.s.impl.PersonelTypeServiceImpl - Checking for default personnel types...
2025-06-20 07:29:34 [restartedMain] INFO  c.p.s.impl.PersonelTypeServiceImpl - Personnel types already exist in the database.
2025-06-20 07:29:34 [restartedMain] WARN  o.s.b.a.o.j.JpaBaseConfiguration$JpaWebConfiguration - spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
2025-06-20 07:29:35 [restartedMain] INFO  o.a.k.c.admin.AdminClientConfig - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = personneltrackingsystem-admin-0
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2025-06-20 07:29:35 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 07:29:35 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 07:29:35 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750393775801
2025-06-20 07:29:36 [kafka-admin-client-thread | personneltrackingsystem-admin-0] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.admin.client for personneltrackingsystem-admin-0 unregistered
2025-06-20 07:29:36 [kafka-admin-client-thread | personneltrackingsystem-admin-0] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
2025-06-20 07:29:36 [kafka-admin-client-thread | personneltrackingsystem-admin-0] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
2025-06-20 07:29:36 [kafka-admin-client-thread | personneltrackingsystem-admin-0] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
2025-06-20 07:29:36 [restartedMain] INFO  o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8080"]
2025-06-20 07:29:36 [restartedMain] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pts-group-reset-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pts-group-reset
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-06-20 07:29:36 [restartedMain] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 07:29:36 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 07:29:36 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 07:29:36 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750393776390
2025-06-20 07:29:36 [restartedMain] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Subscribed to topic(s): turnstile-request
2025-06-20 07:29:36 [restartedMain] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pts-group-reset-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pts-group-reset
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-06-20 07:29:36 [restartedMain] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 07:29:36 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 07:29:36 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 07:29:36 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 07:29:36 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750393776419
2025-06-20 07:29:36 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2025-06-20 07:29:36 [restartedMain] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Subscribed to topic(s): email-notification
2025-06-20 07:29:36 [restartedMain] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-pts-group-reset-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = pts-group-reset
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.springframework.kafka.support.serializer.JsonDeserializer

2025-06-20 07:29:36 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:29:36 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 07:29:36 [restartedMain] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 07:29:36 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2025-06-20 07:29:36 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:29:36 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 07:29:36 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 07:29:36 [restartedMain] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750393776447
2025-06-20 07:29:36 [restartedMain] INFO  o.a.k.c.c.i.LegacyKafkaConsumer - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Subscribed to topic(s): turnstile-passage
2025-06-20 07:29:36 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 07:29:36 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Discovered group coordinator host.docker.internal:9092 (id: 2147483647 rack: null)
2025-06-20 07:29:36 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:29:36 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Request joining group due to: need to re-join with the given member-id: consumer-pts-group-reset-2-39602b49-8aba-447d-8147-0b032f4866bf
2025-06-20 07:29:36 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Request joining group due to: need to re-join with the given member-id: consumer-pts-group-reset-3-e7ac6778-bbe7-494c-abd6-81882f93677b
2025-06-20 07:29:36 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:29:36 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Request joining group due to: need to re-join with the given member-id: consumer-pts-group-reset-1-dfd9e7e6-8d4f-4dfc-8c54-168e1ee88f9e
2025-06-20 07:29:36 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:29:36 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:29:36 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=48, memberId='consumer-pts-group-reset-3-e7ac6778-bbe7-494c-abd6-81882f93677b', protocol='range'}
2025-06-20 07:29:36 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=48, memberId='consumer-pts-group-reset-2-39602b49-8aba-447d-8147-0b032f4866bf', protocol='range'}
2025-06-20 07:29:36 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] SyncGroup failed: The group began another rebalance. Need to re-join the group. Sent generation was Generation{generationId=48, memberId='consumer-pts-group-reset-3-e7ac6778-bbe7-494c-abd6-81882f93677b', protocol='range'}
2025-06-20 07:29:36 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Request joining group due to: rebalance failed due to 'The group is rebalancing, so a rejoin is needed.' (RebalanceInProgressException)
2025-06-20 07:29:36 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:29:36 [restartedMain] INFO  c.p.PersonelTrackingSystemApplication - Started PersonelTrackingSystemApplication in 13.548 seconds (process running for 14.18)
2025-06-20 07:29:36 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Finished assignment for group at generation 48: {consumer-pts-group-reset-2-39602b49-8aba-447d-8147-0b032f4866bf=Assignment(partitions=[email-notification-0]), consumer-pts-group-reset-3-e7ac6778-bbe7-494c-abd6-81882f93677b=Assignment(partitions=[turnstile-passage-0])}
2025-06-20 07:29:36 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] SyncGroup failed: The group began another rebalance. Need to re-join the group. Sent generation was Generation{generationId=48, memberId='consumer-pts-group-reset-2-39602b49-8aba-447d-8147-0b032f4866bf', protocol='range'}
2025-06-20 07:29:36 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Request joining group due to: rebalance failed due to 'The group is rebalancing, so a rejoin is needed.' (RebalanceInProgressException)
2025-06-20 07:29:36 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] (Re-)joining group
2025-06-20 07:29:36 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=49, memberId='consumer-pts-group-reset-2-39602b49-8aba-447d-8147-0b032f4866bf', protocol='range'}
2025-06-20 07:29:36 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=49, memberId='consumer-pts-group-reset-3-e7ac6778-bbe7-494c-abd6-81882f93677b', protocol='range'}
2025-06-20 07:29:36 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Successfully joined group with generation Generation{generationId=49, memberId='consumer-pts-group-reset-1-dfd9e7e6-8d4f-4dfc-8c54-168e1ee88f9e', protocol='range'}
2025-06-20 07:29:36 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Finished assignment for group at generation 49: {consumer-pts-group-reset-2-39602b49-8aba-447d-8147-0b032f4866bf=Assignment(partitions=[email-notification-0]), consumer-pts-group-reset-1-dfd9e7e6-8d4f-4dfc-8c54-168e1ee88f9e=Assignment(partitions=[turnstile-request-0]), consumer-pts-group-reset-3-e7ac6778-bbe7-494c-abd6-81882f93677b=Assignment(partitions=[turnstile-passage-0])}
2025-06-20 07:29:36 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=49, memberId='consumer-pts-group-reset-2-39602b49-8aba-447d-8147-0b032f4866bf', protocol='range'}
2025-06-20 07:29:36 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=49, memberId='consumer-pts-group-reset-3-e7ac6778-bbe7-494c-abd6-81882f93677b', protocol='range'}
2025-06-20 07:29:36 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Successfully synced group in generation Generation{generationId=49, memberId='consumer-pts-group-reset-1-dfd9e7e6-8d4f-4dfc-8c54-168e1ee88f9e', protocol='range'}
2025-06-20 07:29:36 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[email-notification-0])
2025-06-20 07:29:36 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[turnstile-passage-0])
2025-06-20 07:29:36 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Notifying assignor about the new Assignment(partitions=[turnstile-request-0])
2025-06-20 07:29:36 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Adding newly assigned partitions: turnstile-request-0
2025-06-20 07:29:36 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Adding newly assigned partitions: turnstile-passage-0
2025-06-20 07:29:36 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerRebalanceListenerInvoker - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Adding newly assigned partitions: email-notification-0
2025-06-20 07:29:36 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition email-notification-0 to the committed offset FetchPosition{offset=3, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 07:29:36 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition turnstile-passage-0 to the committed offset FetchPosition{offset=3, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 07:29:36 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.a.k.c.c.internals.ConsumerUtils - Setting offset for partition turnstile-request-0 to the committed offset FetchPosition{offset=18, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[host.docker.internal:9092 (id: 0 rack: null)], epoch=0}}
2025-06-20 07:30:09 [http-nio-8080-exec-2] INFO  o.a.c.c.C.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-06-20 07:30:09 [http-nio-8080-exec-2] INFO  c.p.c.impl.TurnstileControllerImpl - Turnstile passage request received: DtoTurnstilePassageFullRequest(wantedToEnterTurnstileId=7, personelId=1, operationType=IN, operationTimeStr=09:19:35)
2025-06-20 07:30:09 [http-nio-8080-exec-2] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - No previous operation found for personel 1 on turnstile 7
2025-06-20 07:30:09 [http-nio-8080-exec-2] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Turnstile passage validation - Personel: 1, Turnstile: 7, Requested Operation: IN, Last Operation: null
2025-06-20 07:30:09 [http-nio-8080-exec-2] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Operation validated successfully: IN (Personel: 1, Turnstile: 7)
2025-06-20 07:30:09 [http-nio-8080-exec-2] INFO  c.p.s.k.i.KafkaProducerServiceImpl - Sending turnstile request event to Kafka: TurnstileRequestEvent(wantedToEnterTurnstileId=7, personelId=1, operationType=IN, operationTimeStr=09:19:35)
2025-06-20 07:30:09 [http-nio-8080-exec-2] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = personneltrackingsystem-producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 3
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.springframework.kafka.support.serializer.JsonSerializer

2025-06-20 07:30:09 [http-nio-8080-exec-2] INFO  o.a.k.c.t.i.KafkaMetricsCollector - initializing Kafka metrics collector
2025-06-20 07:30:09 [http-nio-8080-exec-2] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=personneltrackingsystem-producer-1] Instantiated an idempotent producer.
2025-06-20 07:30:09 [http-nio-8080-exec-2] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.7.1
2025-06-20 07:30:09 [http-nio-8080-exec-2] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: e2494e6ffb89f828
2025-06-20 07:30:09 [http-nio-8080-exec-2] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1750393809883
2025-06-20 07:30:09 [kafka-producer-network-thread | personneltrackingsystem-producer-1] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=personneltrackingsystem-producer-1] Cluster ID: vs1Qza1KSai1qLfalC1-qA
2025-06-20 07:30:09 [kafka-producer-network-thread | personneltrackingsystem-producer-1] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=personneltrackingsystem-producer-1] ProducerId set to 9 with epoch 0
2025-06-20 07:30:09 [kafka-producer-network-thread | personneltrackingsystem-producer-1] INFO  c.p.s.k.i.KafkaProducerServiceImpl - Turnstile request event sent successfully to topic: turnstile-request, partition: 0, offset: 18
2025-06-20 07:30:10 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.k.i.TurnstileRequestConsumerServiceImpl - Received turnstile request event: TurnstileRequestEvent(wantedToEnterTurnstileId=7, personelId=1, operationType=IN, operationTimeStr=09:19:35)
2025-06-20 07:30:10 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.c.i.PersonelCacheServiceImpl - Personnel not found in cache with ID: 1
2025-06-20 07:30:10 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.c.i.PersonelCacheServiceImpl - Personnel cached successfully with ID: 1
2025-06-20 07:30:10 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Saved turnstile registration log: Personel 1 - Turnstile 7 - Operation IN
2025-06-20 07:30:10 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.k.i.TurnstileRequestConsumerServiceImpl - Turnstile request event processed successfully
2025-06-20 07:30:24 [http-nio-8080-exec-1] INFO  c.p.c.impl.TurnstileControllerImpl - Turnstile passage request received: DtoTurnstilePassageFullRequest(wantedToEnterTurnstileId=7, personelId=1, operationType=OUT, operationTimeStr=18:04:35)
2025-06-20 07:30:24 [http-nio-8080-exec-1] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Last operation found: IN (Personel: 1, Turnstile: 7)
2025-06-20 07:30:24 [http-nio-8080-exec-1] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Turnstile passage validation - Personel: 1, Turnstile: 7, Requested Operation: OUT, Last Operation: IN
2025-06-20 07:30:24 [http-nio-8080-exec-1] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Operation validated successfully: OUT (Personel: 1, Turnstile: 7)
2025-06-20 07:30:24 [http-nio-8080-exec-1] INFO  c.p.s.k.i.KafkaProducerServiceImpl - Sending turnstile request event to Kafka: TurnstileRequestEvent(wantedToEnterTurnstileId=7, personelId=1, operationType=OUT, operationTimeStr=18:04:35)
2025-06-20 07:30:24 [kafka-producer-network-thread | personneltrackingsystem-producer-1] INFO  c.p.s.k.i.KafkaProducerServiceImpl - Turnstile request event sent successfully to topic: turnstile-request, partition: 0, offset: 19
2025-06-20 07:30:24 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.k.i.TurnstileRequestConsumerServiceImpl - Received turnstile request event: TurnstileRequestEvent(wantedToEnterTurnstileId=7, personelId=1, operationType=OUT, operationTimeStr=18:04:35)
2025-06-20 07:30:24 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.c.i.PersonelCacheServiceImpl - Personnel found in cache with ID: 1
2025-06-20 07:30:24 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.i.TurnstileRegistrationLogServiceImpl - Saved turnstile registration log: Personel 1 - Turnstile 7 - Operation OUT
2025-06-20 07:30:24 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  c.p.s.k.i.TurnstileRequestConsumerServiceImpl - Turnstile request event processed successfully
2025-06-20 07:38:36 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Node -1 disconnected.
2025-06-20 07:38:36 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Node -1 disconnected.
2025-06-20 07:38:36 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Node -1 disconnected.
2025-06-20 07:39:09 [kafka-producer-network-thread | personneltrackingsystem-producer-1] INFO  o.apache.kafka.clients.NetworkClient - [Producer clientId=personneltrackingsystem-producer-1] Node -1 disconnected.
2025-06-20 07:48:36 [org.springframework.kafka.KafkaListenerEndpointContainer#2-0-C-1] INFO  o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-pts-group-reset-1, groupId=pts-group-reset] Node -1 disconnected.
2025-06-20 07:48:36 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-pts-group-reset-2, groupId=pts-group-reset] Node -1 disconnected.
2025-06-20 07:48:36 [org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-pts-group-reset-3, groupId=pts-group-reset] Node -1 disconnected.
2025-06-20 07:49:09 [kafka-producer-network-thread | personneltrackingsystem-producer-1] INFO  o.apache.kafka.clients.NetworkClient - [Producer clientId=personneltrackingsystem-producer-1] Node -1 disconnected.
